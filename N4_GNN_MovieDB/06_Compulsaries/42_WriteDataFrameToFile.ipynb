{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d776f754-ef98-4cbd-a423-647c483341ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Here we demonstrate the following Dask DataFrame techniques;\n",
    "#\n",
    "#     .  How to create a Dask DataFrame from an array.\n",
    "#\n",
    "#     .  Given a source Dask DataFrame, create a second new DataFrame with just\n",
    "#        unique values, and the ability to link back to the source.\n",
    "#\n",
    "#        We might use this technique when given a flat file of data, that we derive\n",
    "#        nodes and edges from.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682150e3-6a50-4197-a239-80d1d2ea6edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "NUM_PARTITIONS  = 3\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0416c10d-a1cb-433d-ad5e-85498aaa1744",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "   #\n",
    "import dask.array as da\n",
    "import dask.dataframe as dd\n",
    "   #\n",
    "from dask.dataframe import from_pandas\n",
    "\n",
    "from tabulate import tabulate\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131298d0-5cb4-4b62-8413-5e5571ef4b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Create our source Dask DataFrame from an array\n",
    "\n",
    "np_airports = np.array([\n",
    "       #\n",
    "   [\"MKE\", \"Milwaukee\"     , \"WI\", \"Airport\"],\n",
    "   [\"ORD\", \"Chicago O-Hare\", \"IL\", \"Airport\"],\n",
    "   [\"SJC\", \"San Jose\"      , \"CA\", \"Airport\"],\n",
    "   [\"LAX\", \"Los Angeles\"   , \"CA\", \"Airport\"],\n",
    "   [\"DEN\", \"Denver\"        , \"CO\", \"Airport\"],\n",
    "       #\n",
    "   ], dtype=\"str\")\n",
    "\n",
    "pd_airports = pd.DataFrame(np_airports, columns = [\"airport_code\", \"airport_name\", \"state_code\", \"LABEL\"])\n",
    "   #\n",
    "dd_airports = from_pandas(pd_airports, npartitions = NUM_PARTITIONS)\n",
    "\n",
    "\n",
    "for l_each in dd_airports.iterrows():\n",
    "      #\n",
    "   l_airport_code   = l_each[1][0]\n",
    "   l_airport_name   = l_each[1][1]\n",
    "   l_state_code     = l_each[1][2]\n",
    "   l_LABEL          = l_each[1][3]\n",
    "      #\n",
    "   print(\"Airport:  %3s   %-18s   %-2s   %-10s\" % (l_airport_code, l_airport_name, l_state_code, l_LABEL))\n",
    "\n",
    "print(\"--\")\n",
    "\n",
    "#  Sample output,\n",
    "#\n",
    "#  Airport:  MKE   Milwaukee            WI   Airport   \n",
    "#  Airport:  ORD   Chicago O-Hare       IL   Airport   \n",
    "#  Airport:  SJC   San Jose             CA   Airport   \n",
    "#  Airport:  LAX   Los Angeles          CA   Airport   \n",
    "#  Airport:  DEN   Denver               CO   Airport  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d569d6-6547-4e2f-ba57-e78d198b7cac",
   "metadata": {},
   "source": [
    "#  Write DataFrame to local TXT file  (local to Jupyter Docker container)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b708e18-dcc4-4e8e-b508-de70687066f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Write DataFrame to an ASCII file that is local to Jupyter Docker container\n",
    "\n",
    "print(type(dd_airports))\n",
    "\n",
    "dd_airports2 = dd_airports.repartition(npartitions=1) \n",
    "\n",
    "dd_airports2.to_csv(\"42_parent_folder\", index=None, sep=\"|\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdeec52a-db86-4f05-9879-bcff5a07feed",
   "metadata": {},
   "source": [
    "#  Write DataFrame to GS  (easier download later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828d62e9-1cf8-4eee-9147-f351961b95f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Settings:\n",
    "#     Need a public or private accessible storage bucket on GCP or similar ..\n",
    "#\n",
    "#  From,\n",
    "#     https://stackoverflow.com/questions/36314797/write-a-pandas-dataframe-to-google-cloud-storage-or-bigquery\n",
    "#     https://stackoverflow.com/questions/29325458/dictionary-column-in-pandas-dataframe/29325954#29325954\n",
    "\n",
    "import os\n",
    "from google.cloud import storage\n",
    "\n",
    "\n",
    "#  Setup for all work below ..\n",
    "#\n",
    "#  Url for viewing,  https://console.cloud.google.com/storage/browser/farrell-bucket\n",
    "\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/home/jovyan/work/My_KG_NoteBooks/03_MyKeys.json\"\n",
    "\n",
    "l_bucket = \"farrell-bucket\"\n",
    "\n",
    "\n",
    "g_client = storage.Client()\n",
    "   #\n",
    "g_bucket = g_client.get_bucket(l_bucket)\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f8f0b2-4aa7-4822-b27e-3221b57aab89",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  The technique used below worked for Pandas DataFrames, not Dask DataFrames.\n",
    "#  So, we convert Dask to Pandas.\n",
    "\n",
    "dd_airports2      = dd_airports.repartition(npartitions=1) \n",
    "   #\n",
    "dd_airports2_AsPd = dd_airports2.compute()                    #  Convert to Pandas DataFrame\n",
    "\n",
    "#  print(type(dd_airports2_AsPd))\n",
    "\n",
    "\n",
    "l_file = \"42_write_test/node.txt\"\n",
    "   #\n",
    "g_bucket.blob(l_file).upload_from_string(dd_airports2_AsPd.to_csv(header=None, index=None, sep=\"|\"), \"text/plain\")\n",
    "\n",
    "\n",
    "#  This was working, then started throwing new error about \"path\"\n",
    "#\n",
    "#  l_file = \"42_write_test/node.parquet\"\n",
    "#     #\n",
    "#  g_bucket.blob(l_file).upload_from_string(dd_airports2.to_parquet(engine=\"pyarrow\", version=\"2.6\"), \"application/octet-stream\")\n",
    "\n",
    "\n",
    "print(\"--\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
