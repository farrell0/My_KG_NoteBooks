{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064cde52-a7e1-486e-a7c4-3ce3aaaa01da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  This NoteBook contains code to run classic ML routines against a \n",
    "#  number of familiar data sets ..\n",
    "#\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d433bb83-5f71-406e-ab1c-1b0ef02583f1",
   "metadata": {
    "tags": []
   },
   "source": [
    "#  Step 00: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "0ba7589c-6f1e-4f5c-a37b-21f8944c2499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Exception reporting mode: Minimal\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#  Largely code to control how print statements and related work\n",
    "#\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%xmode Minimal\n",
    "\n",
    "\n",
    "#  Setting display options \n",
    "#\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.width\", 640)\n",
    "   #\n",
    "import numpy as np\n",
    "np.set_printoptions(edgeitems = 30, linewidth = 100000, \n",
    "   formatter = dict(float = lambda x: \"%.3g\" % x))\n",
    "\n",
    "#  Sets horizontal scroll for wide outputs\n",
    "#\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"\"))\n",
    "\n",
    "   ###\n",
    "    \n",
    "from tabulate import tabulate\n",
    "#\n",
    "#  How to use tabulate-\n",
    "#\n",
    "#  l_result = [{ \"col1\": 20, \"col2\": 30}]\n",
    "#  #\n",
    "#  print(tabulate(l_result, headers='keys', tablefmt='psql', showindex=False))\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "78a747a0-4aec-4cd2-bf38-8849deee7ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#  Helper functions for what we want to do below-\n",
    "#\n",
    "#  **  You must run this cell to do much of anything in this NoteBook\n",
    "\n",
    "#  We use these objects to store the history of results; display only\n",
    "#\n",
    "class HistoryIterator:\n",
    "   def __init__(self, history):\n",
    "       self._history = history\n",
    "       self._index = 0\n",
    "\n",
    "   def __next__(self):\n",
    "       if (self._index < len(self._history._events)):\n",
    "           result = (self._history._events[self._index][\"event\"] , self._history._events[self._index][\"measure\"])\n",
    "           self._index +=1\n",
    "           return result\n",
    "       raise StopIteration\n",
    "\n",
    "class History:\n",
    "   def __init__(self):\n",
    "      self._events = list()\n",
    "\n",
    "   def clear(self):\n",
    "      self._events = list()\n",
    "    \n",
    "   def add(self, event, measure):\n",
    "      self._events.append({\"event\": event, \"measure\": measure})\n",
    "\n",
    "   def __iter__(self):\n",
    "      return HistoryIterator(self)\n",
    "\n",
    "\n",
    "l_history = History()\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e65a137-2d39-411c-9316-aff11a9b6108",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Sometimes we want to clear history-\n",
    "#\n",
    "\n",
    "l_history.clear()\n",
    "\n",
    "\n",
    "#  To add a blank line to history-\n",
    "#\n",
    "\n",
    "l_history.add(event = \"\", measure = \"\")\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8909c6ac-eef4-42ea-af33-cfda0b6d5740",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  All of our model libraries are imported below, but ..\n",
    "#\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317e2979-c711-419c-91b4-e035bafbe79c",
   "metadata": {
    "tags": []
   },
   "source": [
    "#  Step A1: Iris Data load, encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d1be95-f9df-4435-ab56-add6aa68b8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Read the Iris data into a Pandas Dataframe\n",
    "#\n",
    "#     Features\n",
    "#     1. sepal length in cm\n",
    "#     2. sepal width in cm\n",
    "#     3. petal length in cm\n",
    "#     4. petal width in cm\n",
    "#     5. class: \n",
    "#        Iris-setosa\n",
    "#        Iris-versicolour\n",
    "#        Iris-virginica\n",
    "#\n",
    "#  To convert class into a numeric, we use sklearn.preprocessing.LabelEncoder\n",
    "#  See,\n",
    "#     https://www.turing.com/kb/convert-categorical-data-in-pandas-and-scikit-learn\n",
    "#\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "my_le = LabelEncoder()\n",
    "   #\n",
    "l_folder = \"20_Data\"\n",
    "l_file   = \"11_iris.data.txt\"\n",
    "\n",
    "\n",
    "pd_iris  = pd.read_csv((l_folder + \"/\" + l_file), header = 0, sep = \",\",\n",
    "   names = [\"sl\", \"sw\", \"pl\", \"pw\", \"class\"],\n",
    "   dtype = {\"sl\": \"float\", \"sw\": \"float\", \"pl\": \"float\", \"pw\": \"float\", \"class\": \"string\"} )\n",
    "      #\n",
    "pd_iris[\"class_encoded\"]  =  my_le.fit_transform(pd_iris[\"class\"])\n",
    "   #\n",
    "pd_iris = pd_iris.drop([\"class\"], axis = 1)\n",
    "    \n",
    "    \n",
    "#  Pandas.Dataframe.sample() returns a randomized set of rows, versus\n",
    "#  say head(), which always returns the first n ..\n",
    "#\n",
    "print(tabulate(pd_iris.sample(5), headers='keys', tablefmt='psql', showindex=False))\n",
    "print(\"Number of rows: %d\" % (len(pd_iris)))\n",
    "\n",
    "print(\"--\")\n",
    "\n",
    "#  Sample output,\n",
    "#\n",
    "#     +------+------+------+------+-----------------+\n",
    "#     |   sl |   sw |   pl |   pw |   class_encoded |\n",
    "#     |------+------+------+------+-----------------|\n",
    "#     |  5.5 |  2.4 |  3.8 |  1.1 |               1 |\n",
    "#     |  6.4 |  3.2 |  4.5 |  1.5 |               1 |\n",
    "#     |  6.8 |  3.2 |  5.9 |  2.3 |               2 |\n",
    "#     |  6.7 |  3.3 |  5.7 |  2.1 |               2 |\n",
    "#     |  5.5 |  2.6 |  4.4 |  1.2 |               1 |\n",
    "#     +------+------+------+------+-----------------+\n",
    "#     Number of rows: 149\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd64b3f-30aa-4680-b983-462d5da51304",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Split data into training and test.\n",
    "#  Convert the data into numpy arrays, since the ml code we use later expect that.\n",
    "#  We only want two of the classes, 1 and 2.\n",
    "#\n",
    "#     Why this data ?  It was harder to predict; see the plot below.\n",
    "#\n",
    "\n",
    "\n",
    "#  l_dtype = {\"names\": [\"f1\", \"f2\", \"f3\", \"f4\", \"class_encoded\"],                     #  Wound up not needing this\n",
    "#     \"formats\": [float, float, float, float, float]}\n",
    "#        #\n",
    "#  np_iris[\"train\"].dtype = l_dtype\n",
    "#  np_iris[\"test\" ].dtype = l_dtype\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np_iris = {}\n",
    "   #\n",
    "np_iris[\"train\"], np_iris[\"test\"] = train_test_split(pd_iris.to_numpy(),              #  random_state calls to shuffle the data,\n",
    "   test_size = 0.20, random_state = 40)                                               #    which had arrived sorted\n",
    "\n",
    "\n",
    "np_iris[\"train_12\"] = np.copy(np_iris[\"train\"])                                       #  Copy, then filter\n",
    "np_iris[\"test_12\" ] = np.copy(np_iris[\"test\" ])\n",
    "   #\n",
    "np_iris[\"train_12\"] = np_iris[\"train\"][ ( np_iris[\"train\"][:, -1] > 0) & ( np_iris[\"train\"][:, -1] < 3) ]\n",
    "np_iris[\"test_12\" ] = np_iris[\"test\" ][ ( np_iris[\"test\" ][:, -1] > 0) & ( np_iris[\"test\" ][:, -1] < 3) ]\n",
    "\n",
    "\n",
    "\n",
    "#   a[ (-6<a[:,1]) & (a[:,1]<3) ]\n",
    "\n",
    "\n",
    "print(\"Number of total       rows... %d   Training rows: %d   Test rows: %d\" %        #  Outputs for confirmation\n",
    "  (len(pd_iris), len(np_iris[\"train\"]), len(np_iris[\"test\"])) )\n",
    "print()\n",
    "print(\"Number of total (1-2) rows... %d   Training rows: %d   Test rows: %d\" %\n",
    "  ( len(np_iris[\"train_12\"]) + len(np_iris[\"test_12\"]),\n",
    "    len(np_iris[\"train_12\"]), len(np_iris[\"test_12\"]) ) ) \n",
    "print()\n",
    "\n",
    "print(\"Train data:\")\n",
    "print(\"%s\" % (np_iris[\"train_12\"][0:5]))\n",
    "print()\n",
    "print(\"Test  data:\")\n",
    "print(\"%s\" % (np_iris[\"test_12\" ][0:5]))\n",
    "print()\n",
    "   #\n",
    "print(\"--\")\n",
    "\n",
    "#  Sample output\n",
    "#\n",
    "#     Number of total       rows... 149   Training rows: 119   Test rows: 30\n",
    "#     \n",
    "#     Number of total (1-2) rows... 100   Training rows: 78   Test rows: 22\n",
    "#     \n",
    "#     Train data:\n",
    "#     [[6.1 2.9 4.7 1.4 1]\n",
    "#      [7.2 3 5.8 1.6 2]\n",
    "#      [5.8 2.8 5.1 2.4 2]\n",
    "#      [6.3 2.3 4.4 1.3 1]\n",
    "#      [6.4 2.8 5.6 2.1 2]]\n",
    "#     \n",
    "#     Test  data:\n",
    "#     [[5.8 2.7 4.1 1 1]\n",
    "#      [6.5 3 5.5 1.8 2]\n",
    "#      [7.7 2.6 6.9 2.3 2]\n",
    "#      [5.5 2.6 4.4 1.2 1]\n",
    "#      [6.3 3.4 5.6 2.4 2]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "62191e70-57c7-49d1-ba47-cca3517b0d5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "#  Plot of just two features from the two class data set\n",
    "#\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.scatter(np_iris[\"train_12\"][np_iris[\"train_12\"][:, -1]==1][:, 2], np_iris[\"train_12\"][np_iris[\"train_12\"][:, -1]==1][:, 3], c = \"red\"       , label = \"Class 1\")\n",
    "plt.scatter(np_iris[\"train_12\"][np_iris[\"train_12\"][:, -1]==2][:, 2], np_iris[\"train_12\"][np_iris[\"train_12\"][:, -1]==2][:, 3], c = \"blue\"      , label = \"Class 2\")\n",
    "    \n",
    "plt.legend()\n",
    "   #\n",
    "plt.xlabel('Feature 2')\n",
    "plt.ylabel('Feature 3')\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eee2512-59dc-41ed-8cf2-96505468b1e7",
   "metadata": {},
   "source": [
    "<div> \n",
    "<img src=\"./01_Images/09_Results_Chart.png\" alt=\"Drawing\" style=\"width: 800px;\"/>  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93cb4fa2-1927-4534-8ea4-1dde87fceddd",
   "metadata": {},
   "source": [
    "#  Step A2: Iris Data load, encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59cef88-7f7a-4579-922c-fc7fa9960059",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0df4b7d-946c-4437-8965-d67a07a089c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c1202c-c630-4e27-aea8-4ed425b139ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#  Our numpy array has 5 columns, with the last column being the class.\n",
    "#  To review numpy array slicing,\n",
    "#\n",
    "#     To get the first 4 columns use,\n",
    "#        np_iris[\"train\"][:, :4]\n",
    "#     To get the last column use,\n",
    "#        np_iris[\"train\"][:, -1]\n",
    "#\n",
    "\n",
    "do_model(KNeighborsClassifier(n_neighbors = 3), np_iris[\"train\"][:, :4], np_iris[\"train\"][:, -1], np_iris[\"test\"][:, :4], np_iris[\"test\"][:, -1], \"Iris: kNN=3\" ) \n",
    "print()\n",
    "print(tabulate(l_history, headers=[\"Category\", \"Accuracy\"], tablefmt='psql', showindex=False))\n",
    "\n",
    "#  do_model(KNeighborsClassifier(n_neighbors = 3), np_iris[\"train_norm\"][:, :4], np_iris[\"train_norm\"][:, -1], np_iris[\"test_norm\"][:, :4], np_iris[\"test_norm\"][:, -1], \"Iris: kNN=3 Normalized\" ) \n",
    "#  print()\n",
    "#  print(tabulate(l_history, headers=[\"Category\", \"Accuracy\"], tablefmt='psql', showindex=False))\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c1740a-f656-4d6d-bdf3-cb2f053fef60",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "#  Naive Bayes, Gaussian\n",
    "#\n",
    "#     Gaussian usually does better than the Multinomial below because,\n",
    "#        Gaussian expects continuous values\n",
    "#        Multinomial expects discreet values\n",
    "#\n",
    "#     And our values are continuous\n",
    "#\n",
    "\n",
    "do_model(GaussianNB(), np_iris[\"train\"][:, :4], np_iris[\"train\"][:, -1], np_iris[\"test\"][:, :4], np_iris[\"test\"][:, -1], \"Iris: GaussianNB\" ) \n",
    "print()\n",
    "print(tabulate(l_history, headers=[\"Category\", \"Accuracy\"], tablefmt='psql', showindex=False))\n",
    "\n",
    "#  do_model(GaussianNB(), np_iris[\"train_norm\"][:, :4], np_iris[\"train_norm\"][:, -1], np_iris[\"test_norm\"][:, :4], np_iris[\"test_norm\"][:, -1], \"Iris: GaussianNB Normalized\" ) \n",
    "#  print()\n",
    "#  print(tabulate(l_history, headers=[\"Category\", \"Accuracy\"], tablefmt='psql', showindex=False))\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de942e0e-31d0-44c1-bdb3-0ee3c5ca3315",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "#  Naive Bayes, Multinomial\n",
    "#\n",
    "\n",
    "do_model(MultinomialNB(), np_iris[\"train\"][:, :4], np_iris[\"train\"][:, -1], np_iris[\"test\"][:, :4], np_iris[\"test\"][:, -1], \"Iris: MultinomialNB\" ) \n",
    "print()\n",
    "print(tabulate(l_history, headers=[\"Category\", \"Accuracy\"], tablefmt='psql', showindex=False))\n",
    "\n",
    "#  do_model(MultinomialNB(), np_iris[\"train_norm\"][:, :4], np_iris[\"train_norm\"][:, -1], np_iris[\"test_norm\"][:, :4], np_iris[\"test_norm\"][:, -1], \"Iris: MultinomialNB Normalized\" ) \n",
    "#  print()\n",
    "#  print(tabulate(l_history, headers=[\"Category\", \"Accuracy\"], tablefmt='psql', showindex=False))\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4adb7874-3d2c-4f27-83db-147b953228c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "#  Decision Tree\n",
    "#\n",
    "\n",
    "do_model(DecisionTreeClassifier(), np_iris[\"train\"][:, :4], np_iris[\"train\"][:, -1], np_iris[\"test\"][:, :4], np_iris[\"test\"][:, -1], \"Iris: DecisionTree\" ) \n",
    "print()\n",
    "print(tabulate(l_history, headers=[\"Category\", \"Accuracy\"], tablefmt='psql', showindex=False))\n",
    "\n",
    "#  do_model(DecisionTreeClassifier(), np_iris[\"train_norm\"][:, :4], np_iris[\"train_norm\"][:, -1], np_iris[\"test_norm\"][:, :4], np_iris[\"test_norm\"][:, -1], \"Iris: DecisionTree Normalized\" ) \n",
    "#  print()\n",
    "#  print(tabulate(l_history, headers=[\"Category\", \"Accuracy\"], tablefmt='psql', showindex=False))\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0a74fa-57c5-44ef-ade3-dd5438751064",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#  Random Forest\n",
    "#\n",
    "\n",
    "#  n_estimators, number of random trees created and trained\n",
    "#\n",
    "\n",
    "do_model(RandomForestClassifier(n_estimators = 5), np_iris[\"train\"][:, :4], np_iris[\"train\"][:, -1], np_iris[\"test\"][:, :4], np_iris[\"test\"][:, -1], \"Iris: RandomForest\" ) \n",
    "print()\n",
    "print(tabulate(l_history, headers=[\"Category\", \"Accuracy\"], tablefmt='psql', showindex=False))\n",
    "\n",
    "#  do_model(RandomForestClassifier(n_estimators = 5), np_iris[\"train_norm\"][:, :4], np_iris[\"train_norm\"][:, -1], np_iris[\"test_norm\"][:, :4], np_iris[\"test_norm\"][:, -1], \"Iris: RandomForest Normalized\" ) \n",
    "#  print()\n",
    "#  print(tabulate(l_history, headers=[\"Category\", \"Accuracy\"], tablefmt='psql', showindex=False))\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2dc0262-190c-455d-b2f9-99056c1bea18",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "#  Support Vector Machine\n",
    "#\n",
    "#  We run this one with a number of configurations ..\n",
    "#\n",
    "#     C      ==  margin constant\n",
    "#     gamma  ==  used by the Gaussian kernel\n",
    "#\n",
    "\n",
    "do_model(SVC(kernel = \"linear\", C = 1.0), np_iris[\"train\"][:, :4], np_iris[\"train\"][:, -1], np_iris[\"test\"][:, :4], np_iris[\"test\"][:, -1], \"Iris: SVC/Linear\" ) \n",
    "print()\n",
    "print(tabulate(l_history, headers=[\"Category\", \"Accuracy\"], tablefmt='psql', showindex=False))\n",
    "print()\n",
    "             \n",
    "#  do_model(SVC(kernel = \"linear\", C = 1.0), np_iris[\"train_norm\"][:, :4], np_iris[\"train_norm\"][:, -1], np_iris[\"test_norm\"][:, :4], np_iris[\"test_norm\"][:, -1], \"Iris: SVC/Linear Normalized\" ) \n",
    "#  print()\n",
    "#  print(tabulate(l_history, headers=[\"Category\", \"Accuracy\"], tablefmt='psql', showindex=False))\n",
    "#  print()\n",
    "\n",
    "             \n",
    "do_model(SVC(kernel = \"rbf\", C = 1.0, gamma = 0.25), np_iris[\"train\"][:, :4], np_iris[\"train\"][:, -1], np_iris[\"test\"][:, :4], np_iris[\"test\"][:, -1], \"Iris: RBF\" ) \n",
    "print()\n",
    "print(tabulate(l_history, headers=[\"Category\", \"Accuracy\"], tablefmt='psql', showindex=False))\n",
    "print()\n",
    "             \n",
    "#  do_model(SVC(kernel = \"rbf\", C = 1.0, gamma = 0.25), np_iris[\"train_norm\"][:, :4], np_iris[\"train_norm\"][:, -1], np_iris[\"test_norm\"][:, :4], np_iris[\"test_norm\"][:, -1], \"Iris: RBF Normalized\" ) \n",
    "#  print()\n",
    "#  print(tabulate(l_history, headers=[\"Category\", \"Accuracy\"], tablefmt='psql', showindex=False))\n",
    "#  print()\n",
    "\n",
    "             \n",
    "do_model(SVC(kernel = \"rbf\", C = 1.0, gamma = 0.001), np_iris[\"train\"][:, :4], np_iris[\"train\"][:, -1], np_iris[\"test\"][:, :4], np_iris[\"test\"][:, -1], \"Iris: RBF 2\" ) \n",
    "print()\n",
    "print(tabulate(l_history, headers=[\"Category\", \"Accuracy\"], tablefmt='psql', showindex=False))\n",
    "print()\n",
    "\n",
    "#  do_model(SVC(kernel = \"rbf\", C = 1.0, gamma = 0.001), np_iris[\"train_norm\"][:, :4], np_iris[\"train_norm\"][:, -1], np_iris[\"test_norm\"][:, :4], np_iris[\"test_norm\"][:, -1], \"Iris: RBF 2 Normalized\" ) \n",
    "#  print()\n",
    "#  print(tabulate(l_history, headers=[\"Category\", \"Accuracy\"], tablefmt='psql', showindex=False))\n",
    "#  print()\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafe262a-4fb5-4954-9d6f-c8cbea09d763",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Read the Breast Cancer data into a Pandas Dataframe\n",
    "#\n",
    "#     Features\n",
    "#     1)     ID number\n",
    "#     2)     Diagnosis (M = malignant, B = benign)\n",
    "#     3-32)\n",
    "#       Ten real-valued features are computed for each cell nucleus:\n",
    "#     \n",
    "#     \ta) radius (mean of distances from center to points on the perimeter)\n",
    "#     \tb) texture (standard deviation of gray-scale values)\n",
    "#     \tc) perimeter\n",
    "#     \td) area\n",
    "#     \te) smoothness (local variation in radius lengths)\n",
    "#     \tf) compactness (perimeter^2 / area - 1.0)\n",
    "#     \tg) concavity (severity of concave portions of the contour)\n",
    "#     \th) concave points (number of concave portions of the contour)\n",
    "#     \ti) symmetry \n",
    "#     \tj) fractal dimension (\"coastline approximation\" - 1)\n",
    "#\n",
    "#  Sample data line,\n",
    "#     842302,M,\n",
    "#     17.99,    10.38,    122.8,    1001,    0.1184,    0.2776,    0.3001,    0.1471,    0.2419,    0.07871,         #  10 count\n",
    "#     1.095,    0.9053,   8.589,    153.4,   0.006399,  0.04904,   0.05373,   0.01587,   0.03003,   0.006193,\n",
    "#     25.38,    17.33,    184.6,    2019,    0.1622,    0.6656,    0.7119,    0.2654,    0.4601     ,0.1189\n",
    "#\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "my_le = LabelEncoder()\n",
    "   #\n",
    "l_folder = \"20_Data\"\n",
    "l_file   = \"22_wdbc.data.txt\"\n",
    "\n",
    "\n",
    "pd_bc  = pd.read_csv((l_folder + \"/\" + l_file), header = 0, sep = \",\",\n",
    "   names = [\"id\", \"class\",\n",
    "            \"f01\", \"f02\", \"f03\", \"f04\", \"f05\", \"f06\", \"f07\", \"f08\", \"f09\", \"f10\", \n",
    "            \"f11\", \"f12\", \"f13\", \"f14\", \"f15\", \"f16\", \"f17\", \"f18\", \"f19\", \"f20\", \n",
    "            \"f21\", \"f22\", \"f23\", \"f24\", \"f25\", \"f26\", \"f27\", \"f28\", \"f29\", \"f30\", \n",
    "           ],\n",
    "   dtype = {\"id\": \"int\", \"class\": \"string\",\n",
    "            \"f01\": \"float\", \"f02\": \"float\", \"f03\": \"float\", \"f04\": \"float\", \"f05\": \"float\", \"f06\": \"float\", \"f07\": \"float\", \"f08\": \"float\", \"f09\": \"float\", \"f10\": \"float\", \n",
    "            \"f11\": \"float\", \"f12\": \"float\", \"f13\": \"float\", \"f14\": \"float\", \"f15\": \"float\", \"f16\": \"float\", \"f17\": \"float\", \"f18\": \"float\", \"f19\": \"float\", \"f20\": \"float\", \n",
    "            \"f21\": \"float\", \"f22\": \"float\", \"f23\": \"float\", \"f24\": \"float\", \"f25\": \"float\", \"f26\": \"float\", \"f27\": \"float\", \"f28\": \"float\", \"f29\": \"float\", \"f30\": \"float\", \n",
    "           } )\n",
    "      #\n",
    "pd_bc[\"class_encoded\"]  =  my_le.fit_transform(pd_bc[\"class\"])\n",
    "   #\n",
    "pd_bc = pd_bc.drop([\"class\", \"id\"], axis = 1)\n",
    "    \n",
    "    \n",
    "#  Pandas.Dataframe.sample() returns a randomized count of rows\n",
    "#\n",
    "print(tabulate(pd_bc.sample(5), headers='keys', tablefmt='psql', showindex=False))\n",
    "print(\"Number of rows: %d\" % (len(pd_bc)))\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe05409-32ac-4da0-bd05-6d97b35df9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Split data into training and test.\n",
    "#  Convert the data into numpy arrays, since the ml libraries we use later expect that.\n",
    "#\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np_bc = {}\n",
    "   #\n",
    "np_bc[\"train\"], np_bc[\"test\"] = train_test_split(pd_bc.to_numpy(),                    #  random_state calls to shuffle the data,\n",
    "   test_size = 0.20, random_state = 40)                                               #    which had arrived sorted\n",
    "                                                                                      #  10% yields way too high of an accuracy\n",
    "                                                                                      #    far below\n",
    "print(\"Number of total rows: %d   Training rows: %d   Test rows: %d\" %\n",
    "  (len(pd_bc), len(np_bc[\"train\"]), len(np_bc[\"test\"])) )\n",
    "\n",
    "print()\n",
    "print(\"Train data:\")\n",
    "print(\"%s\" % (np_bc[\"train\"][0:5]))\n",
    "print()\n",
    "print(\"Test  data:\")\n",
    "print(\"%s\" % (np_bc[\"test\" ][0:5]))\n",
    "print()\n",
    "   #\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f885ac6-db00-4fd0-9ef3-c7b5bd130542",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Normalizing the data\n",
    "#\n",
    "\n",
    "def my_normalize(X, x_min, x_max):\n",
    "   nom = (X-X.min(axis=0))*(x_max-x_min)\n",
    "   denom = X.max(axis=0) - X.min(axis=0)\n",
    "   denom[denom==0] = 1\n",
    "   return x_min + nom/denom \n",
    "\n",
    "\n",
    "print(\"Number of columns in matrix: %d\" % (np_bc[\"train\"].shape[1]))\n",
    "      \n",
    "#  If we normalize the \"class\" column, we lose the categorical nature\n",
    "#  of that data. So, create a deep copy, then just normalize the non-\n",
    "#  class columns.\n",
    "#\n",
    "np_bc[\"train_norm\"] = np.copy(np_bc[\"train\"])\n",
    "np_bc[\"test_norm\" ] = np.copy(np_bc[\"test\" ])\n",
    "   #\n",
    "np_bc[\"train_norm\"][:, :30] = my_normalize(np_bc[\"train_norm\"][:, :30], 0, 1)\n",
    "np_bc[\"test_norm\" ][:, :30] = my_normalize(np_bc[\"test_norm\" ][:, :30], 0, 1)\n",
    "\n",
    "plt.boxplot(np_bc[\"train\"     ])\n",
    "plt.show()\n",
    "plt.boxplot(np_bc[\"train_norm\"])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf36b4ec-23da-4af1-8cb4-fe5669cc5cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  All of our model libraries were imported above, but ..\n",
    "#\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "#  To add a blank line to history-\n",
    "#\n",
    "l_history.add(event = \"\", measure = \"\")\n",
    "\n",
    "\n",
    "#  Our numpy array has many columns, with the last column being the class.\n",
    "#\n",
    "#  To review numpy array slicing,\n",
    "#\n",
    "#     To get the first 30 columns use,\n",
    "#        np_iris[\"train\"][:, :30]\n",
    "#     To get the last column use,\n",
    "#        np_iris[\"train\"][:, -1]\n",
    "#\n",
    "\n",
    "do_model(NearestCentroid(), np_bc[\"train\"][:, :30], np_bc[\"train\"][:, -1], np_bc[\"test\"][:, :30], np_bc[\"test\"][:, -1], \"Breast Cancer: Centroid\") \n",
    "#  do_model(NearestCentroid(), np_bc[\"train_norm\"][:, :30], np_bc[\"train_norm\"][:, -1], np_bc[\"test_norm\"][:, :30], np_bc[\"test_norm\"][:, -1], \"Breast Cancer: Centroid Normalized\") \n",
    "print()\n",
    "\n",
    "do_model(KNeighborsClassifier(n_neighbors = 3), np_bc[\"train\"][:, :30], np_bc[\"train\"][:, -1], np_bc[\"test\"][:, :30], np_bc[\"test\"][:, -1], \"Breast Cancer: kNN=3\") \n",
    "#  do_model(KNeighborsClassifier(n_neighbors = 3), np_bc[\"train_norm\"][:, :30], np_bc[\"train_norm\"][:, -1], np_bc[\"test_norm\"][:, :30], np_bc[\"test_norm\"][:, -1], \"Breast Cancer: kNN=3 Normalized\") \n",
    "print()\n",
    "\n",
    "do_model(GaussianNB(), np_bc[\"train\"][:, :30], np_bc[\"train\"][:, -1], np_bc[\"test\"][:, :30], np_bc[\"test\"][:, -1], \"Breast Cancer: GaussianNB\") \n",
    "#  do_model(GaussianNB(), np_bc[\"train_norm\"][:, :30], np_bc[\"train_norm\"][:, -1], np_bc[\"test_norm\"][:, :30], np_bc[\"test_norm\"][:, -1], \"Breast Cancer: GaussianNB Normalized\") \n",
    "print()\n",
    "\n",
    "do_model(MultinomialNB(), np_bc[\"train\"][:, :30], np_bc[\"train\"][:, -1], np_bc[\"test\"][:, :30], np_bc[\"test\"][:, -1], \"Breast Cancer: MultinomialNB\") \n",
    "#  do_model(MultinomialNB(), np_bc[\"train_norm\"][:, :30], np_bc[\"train_norm\"][:, -1], np_bc[\"test_norm\"][:, :30], np_bc[\"test_norm\"][:, -1], \"Breast Cancer: MultinomialNB Normalized\") \n",
    "print()\n",
    "\n",
    "do_model(DecisionTreeClassifier(), np_bc[\"train\"][:, :30], np_bc[\"train\"][:, -1], np_bc[\"test\"][:, :30], np_bc[\"test\"][:, -1], \"Breast Cancer: DecisionTree\") \n",
    "#  do_model(DecisionTreeClassifier(), np_bc[\"train_norm\"][:, :30], np_bc[\"train_norm\"][:, -1], np_bc[\"test_norm\"][:, :30], np_bc[\"test_norm\"][:, -1], \"Breast Cancer: DecisionTree Normalized\") \n",
    "print()\n",
    "\n",
    "#  n_estimators, number of random trees created and trained\n",
    "#\n",
    "do_model(RandomForestClassifier(n_estimators = 5), np_bc[\"train\"][:, :30], np_bc[\"train\"][:, -1], np_bc[\"test\"][:, :30], np_bc[\"test\"][:, -1], \"Breast Cancer: Random Forest = 5\") \n",
    "#  do_model(RandomForestClassifier(n_estimators = 5), np_bc[\"train_norm\"][:, :30], np_bc[\"train_norm\"][:, -1], np_bc[\"test_norm\"][:, :30], np_bc[\"test_norm\"][:, -1], \"Breast Cancer: Random Forest = 5 Normalized\") \n",
    "print()\n",
    "\n",
    "   ###\n",
    "\n",
    "#  Support Vector Machine\n",
    "#\n",
    "#  We run this one with a number of configurations ..\n",
    "#\n",
    "#     C      ==  margin constant\n",
    "#     gamma  ==  used by the Gaussian kernel\n",
    "#\n",
    "\n",
    "do_model(SVC(kernel = \"linear\", C = 1.0), np_bc[\"train\"][:, :30], np_bc[\"train\"][:, -1], np_bc[\"test\"][:, :30], np_bc[\"test\"][:, -1], \"Breast Cancer: SVC/Linear\") \n",
    "#  do_model(SVC(kernel = \"linear\", C = 1.0), np_bc[\"train_norm\"][:, :30], np_bc[\"train_norm\"][:, -1], np_bc[\"test_norm\"][:, :30], np_bc[\"test_norm\"][:, -1], \"Breast Cancer: SVC/Linear Normalized\") \n",
    "print()\n",
    "\n",
    "do_model(SVC(kernel = \"rbf\", C = 1.0, gamma = 1.0), np_bc[\"train\"][:, :30], np_bc[\"train\"][:, -1], np_bc[\"test\"][:, :30], np_bc[\"test\"][:, -1], \"Breast Cancer: RBF\") \n",
    "#  do_model(SVC(kernel = \"rbf\", C = 1.0, gamma = 1.0), np_bc[\"train_norm\"][:, :30], np_bc[\"train_norm\"][:, -1], np_bc[\"test_norm\"][:, :30], np_bc[\"test_norm\"][:, -1], \"Breast Cancer: RBF Normalized\") \n",
    "print()\n",
    "\n",
    "do_model(SVC(kernel = \"rbf\", C = 1.0, gamma = 0.001), np_bc[\"train\"][:, :30], np_bc[\"train\"][:, -1], np_bc[\"test\"][:, :30], np_bc[\"test\"][:, -1], \"Breast Cancer: RBF 2\") \n",
    "#  do_model(SVC(kernel = \"rbf\", C = 1.0, gamma = 0.001), np_bc[\"train_norm\"][:, :30], np_bc[\"train_norm\"][:, -1], np_bc[\"test_norm\"][:, :30], np_bc[\"test_norm\"][:, -1], \"Breast Cancer: RBF 2 Normalized\") \n",
    "print()\n",
    "\n",
    "\n",
    "print()\n",
    "print(tabulate(l_history, headers=[\"Category\", \"Accuracy\"], tablefmt='psql', showindex=False))\n",
    "print()\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09eb083d-60ba-4c42-b65c-f8f7c0864b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  We will be using Keras, so pip install it inside the Jupyter NoteBook container ..\n",
    "#\n",
    "\n",
    "l_package1 = \"keras\"\n",
    "l_package2 = \"tensorflow\"\n",
    "    \n",
    "def my_func(arg1):\n",
    "    \n",
    "   import sys\n",
    "   import subprocess\n",
    "    \n",
    "   subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", arg1 ])\n",
    "      #\n",
    "   print(\"From each host ..\")\n",
    "      #\n",
    "   return\n",
    "\n",
    "\n",
    "   ##########################################\n",
    "    \n",
    "\n",
    "print(\"Install Python Pip packages on Jupyter container ..\")\n",
    "   #\n",
    "my_return = my_func(l_package1)\n",
    "my_return = my_func(l_package2)\n",
    "print()\n",
    "    \n",
    "\n",
    "#  Use this if installing o nthe KGIP worker nodes ..\n",
    "#\n",
    "#  print(\"Install Python Pip packages on KGIP worker node containers ..\")\n",
    "#     # \n",
    "#  my_return = my_graph.run(lambda g: my_func(l_package))\n",
    "#  print()\n",
    "    \n",
    "print(\"--\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f9a359-02b1-4bb0-aced-216eb7cad1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Intead of loading MNist from disk, we load it from the Keras library ..\n",
    "#\n",
    "\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "\n",
    "\n",
    "np_mnist = {}\n",
    "   #\n",
    "(np_mnist[\"train\"], np_mnist[\"train_label\"]), (np_mnist[\"test\"], np_mnist[\"test_label\"]) = mnist.load_data()\n",
    "\n",
    "\n",
    "#  train and test ccome in as an array [(n), 28, 28] where n == 60000 for train,\n",
    "#  and 100000 for test\n",
    "#\n",
    "#  We need that 28*28 as a vector, so ..\n",
    "#\n",
    "np_mnist[\"train_v\"] = np_mnist[\"train\"].reshape(-1, 28*28)\n",
    "np_mnist[\"test_v\"]  = np_mnist[\"test\" ].reshape(-1, 28*28)\n",
    "\n",
    "\n",
    "print(\"Train shape ................ %s\" % (str(np_mnist[\"train\"].shape)))\n",
    "print(\"Train label shape .......... %s\" % (str(np_mnist[\"train_label\"].shape)))\n",
    "   #\n",
    "print(\"Test  shape ................ %s\" % (str(np_mnist[\"test\"].shape)))\n",
    "print(\"Test  label shape .......... %s\" % (str(np_mnist[\"test_label\"].shape)))\n",
    "   #\n",
    "print(\"Train vector shape ......... %s\" % (str(np_mnist[\"train_v\"].shape)))\n",
    "print(\"Test  vector shape ......... %s\" % (str(np_mnist[\"test_v\" ].shape)))\n",
    "   #\n",
    "print()\n",
    "\n",
    "\n",
    "#  tabulate() displays poorly with this wide data. Straight up print() works well.\n",
    "#\n",
    "# print(tabulate(np_mnist[\"train\"][0:2], headers='keys', tablefmt='psql', showindex=False))\n",
    "print(np_mnist[\"train\"][0:2])\n",
    "print(\"Number of rows: %d\" % (len(np_mnist[\"train\"])))\n",
    "   #\n",
    "print()\n",
    "\n",
    "print(np_mnist[\"train_label\"][0:2])\n",
    "print(\"Number of rows: %d\" % (len(np_mnist[\"train_label\"])))\n",
    "   #\n",
    "print()\n",
    "\n",
    "\n",
    "print(np_mnist[\"test\"][0:2])\n",
    "print(\"Number of rows: %d\" % (len(np_mnist[\"test\"])))\n",
    "   #\n",
    "print()\n",
    "\n",
    "print(np_mnist[\"test_label\"][0:2])\n",
    "print(\"Number of rows: %d\" % (len(np_mnist[\"test_label\"])))\n",
    "   #\n",
    "print()\n",
    "\n",
    "    \n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c425b03-8d9c-4208-af6b-9f6115104b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Sample results\n",
    "#\n",
    "#     Train shape ................ (60000, 28, 28)\n",
    "#     Train label shape .......... (60000,)\n",
    "#     Test  shape ................ (10000, 28, 28)\n",
    "#     Test  label shape .......... (10000,)\n",
    "#     Train vector shape ......... (60000, 784)\n",
    "#     Test  vector shape ......... (10000, 784)\n",
    "#     \n",
    "#     [[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   3  18  18  18 126 136 175  26 166 255 247 127   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0  30  36  94 154 170 253 253 253 253 253 225 172 253 242 195  64   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0  49 238 253 253 253 253 253 253 253 253 251  93  82  82  56  39   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0  18 219 253 253 253 253 253 198 182 247 241   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0  80 156 107 253 253 205  11   0  43 154   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0  14   1 154 253  90   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0 139 253 190   2   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0  11 190 253  70   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0  35 241 225 160 108   1   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0  81 240 253 253 119  25   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  45 186 253 253 150  27   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  16  93 252 253 187   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 249 253 249  64   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  46 130 183 253 253 207   2   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0  39 148 229 253 253 253 250 182   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0  24 114 221 253 253 253 253 201  78   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0  23  66 213 253 253 253 253 198  81   2   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0  18 171 219 253 253 253 253 195  80   9   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0  55 172 226 253 253 253 253 244 133  11   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0 136 253 253 253 212 135 132  16   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]]\n",
    "#     \n",
    "#      [[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  51 159 253 159  50   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  48 238 252 252 252 237   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0  54 227 253 252 239 233 252  57   6   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0  10  60 224 252 253 252 202  84 252 253 122   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0 163 252 252 252 253 252 252  96 189 253 167   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0  51 238 253 253 190 114 253 228  47  79 255 168   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0  48 238 252 252 179  12  75 121  21   0   0 253 243  50   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0  38 165 253 233 208  84   0   0   0   0   0   0 253 252 165   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   7 178 252 240  71  19  28   0   0   0   0   0   0 253 252 195   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0  57 252 252  63   0   0   0   0   0   0   0   0   0 253 252 195   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0 198 253 190   0   0   0   0   0   0   0   0   0   0 255 253 196   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0  76 246 252 112   0   0   0   0   0   0   0   0   0   0 253 252 148   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0  85 252 230  25   0   0   0   0   0   0   0   0   7 135 253 186  12   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0  85 252 223   0   0   0   0   0   0   0   0   7 131 252 225  71   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0  85 252 145   0   0   0   0   0   0   0  48 165 252 173   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0  86 253 225   0   0   0   0   0   0 114 238 253 162   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0  85 252 249 146  48  29  85 178 225 253 223 167  56   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0  85 252 252 252 229 215 252 252 252 196 130   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0  28 199 252 252 253 252 252 233 145   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0  25 128 252 253 252 141  37   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]]]\n",
    "#     Number of rows: 60000\n",
    "#     \n",
    "#     [5 0]\n",
    "#     Number of rows: 60000\n",
    "#     \n",
    "#     [[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0  84 185 159 151  60  36   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0 222 254 254 254 254 241 198 198 198 198 198 198 198 198 170  52   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0  67 114  72 114 163 227 254 225 254 254 254 250 229 254 254 140   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0  17  66  14  67  67  67  59  21 236 254 106   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  83 253 209  18   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  22 233 255  83   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 129 254 238  44   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  59 249 254  62   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 133 254 187   5   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   9 205 248  58   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 126 254 182   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  75 251 240  57   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0  19 221 254 166   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   3 203 254 219  35   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0  38 254 254  77   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0  31 224 254 115   1   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0 133 254 254  52   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0  61 242 254 254  52   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0 121 254 254 219  40   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0 121 254 207  18   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]]\n",
    "#     \n",
    "#      [[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0 116 125 171 255 255 150  93   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0 169 253 253 253 253 253 253 218  30   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0 169 253 253 253 213 142 176 253 253 122   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0  52 250 253 210  32  12   0   6 206 253 140   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0  77 251 210  25   0   0   0 122 248 253  65   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0  31  18   0   0   0   0 209 253 253  65   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0 117 247 253 198  10   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0  76 247 253 231  63   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0 128 253 253 144   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0 176 246 253 159  12   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0  25 234 253 233  35   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0 198 253 253 141   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0  78 248 253 189  12   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0  19 200 253 253 141   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0 134 253 253 173  12   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0 248 253 253  25   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0 248 253 253  43  20  20  20  20   5   0   5  20  20  37 150 150 150 147  10   0]\n",
    "#       [  0   0   0   0   0   0   0   0 248 253 253 253 253 253 253 253 168 143 166 253 253 253 253 253 253 253 123   0]\n",
    "#       [  0   0   0   0   0   0   0   0 174 253 253 253 253 253 253 253 253 253 253 253 249 247 247 169 117 117  57   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0 118 123 123 123 166 253 253 253 155 123 123  41   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]]]\n",
    "#     Number of rows: 10000\n",
    "#     \n",
    "#     [7 2]\n",
    "#     Number of rows: 10000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6bcf95-6f46-448e-a618-4bf0fb15e098",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Sometimes we want to clear history-\n",
    "#\n",
    "\n",
    "l_history.clear()\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97564456-56d7-4f55-b4d8-94ec6ddde2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  All of our model libraries were imported above, but ..\n",
    "#\n",
    "\n",
    "#  Here we run given ML routines against the MNist data set\n",
    "#\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "#  Adding these to the above\n",
    "#\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import decomposition\n",
    "\n",
    "import time\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e883ec4-3c77-43cd-8d71-588f58f12503",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  To add a blank line to history-\n",
    "#\n",
    "l_history.add(event = \"\", measure = \"\")\n",
    "\n",
    "\n",
    "do_model(NearestCentroid(), np_mnist[\"train_v\"], np_mnist[\"train_label\"], np_mnist[\"test_v\"], np_mnist[\"test_label\"], \"MNist: Centroid\") \n",
    "print()\n",
    "\n",
    "do_model(KNeighborsClassifier(n_neighbors =  3), np_mnist[\"train_v\"], np_mnist[\"train_label\"], np_mnist[\"test_v\"], np_mnist[\"test_label\"], \"MNist: kNN=3\" ) \n",
    "do_model(KNeighborsClassifier(n_neighbors =  7), np_mnist[\"train_v\"], np_mnist[\"train_label\"], np_mnist[\"test_v\"], np_mnist[\"test_label\"], \"MNist: kNN=7\") \n",
    "print()\n",
    "\n",
    "do_model(GaussianNB(), np_mnist[\"train_v\"], np_mnist[\"train_label\"], np_mnist[\"test_v\"], np_mnist[\"test_label\"], \"MNist: GaussianNB\") \n",
    "print()\n",
    "\n",
    "do_model(MultinomialNB(), np_mnist[\"train_v\"], np_mnist[\"train_label\"], np_mnist[\"test_v\"], np_mnist[\"test_label\"], \"MNist: MultinomialNB\") \n",
    "print()\n",
    "\n",
    "do_model(DecisionTreeClassifier(), np_mnist[\"train_v\"], np_mnist[\"train_label\"], np_mnist[\"test_v\"], np_mnist[\"test_label\"], \"MNist: DecisionTree\") \n",
    "print()\n",
    "\n",
    "#  n_estimators, number of random trees created and trained\n",
    "#\n",
    "do_model(RandomForestClassifier(n_estimators = 5   ), np_mnist[\"train_v\"], np_mnist[\"train_label\"], np_mnist[\"test_v\"], np_mnist[\"test_label\"], \"MNist: Random Forest = 5   \") \n",
    "do_model(RandomForestClassifier(n_estimators = 50  ), np_mnist[\"train_v\"], np_mnist[\"train_label\"], np_mnist[\"test_v\"], np_mnist[\"test_label\"], \"MNist: Random Forest = 50  \") \n",
    "do_model(RandomForestClassifier(n_estimators = 500 ), np_mnist[\"train_v\"], np_mnist[\"train_label\"], np_mnist[\"test_v\"], np_mnist[\"test_label\"], \"MNist: Random Forest = 500 \") \n",
    "do_model(RandomForestClassifier(n_estimators = 5000), np_mnist[\"train_v\"], np_mnist[\"train_label\"], np_mnist[\"test_v\"], np_mnist[\"test_label\"], \"MNist: Random Forest = 5000\") \n",
    "\n",
    "print()\n",
    "\n",
    "   ###\n",
    "\n",
    "#  Support Vector Machine\n",
    "#\n",
    "#  We run this one with a number of configurations ..\n",
    "#\n",
    "#     C      ==  margin constant\n",
    "#     gamma  ==  used by the Gaussian kernel\n",
    "#\n",
    "\n",
    "#  As configured, these throw warnings, never settle ..\n",
    "#\n",
    "\n",
    "#  do_model(LinearSVC(C = 0.01), np_mnist[\"train_v\"], np_mnist[\"train_label\"], np_mnist[\"test_v\"], np_mnist[\"test_label\"], \"MNist: LinearSVC c=0.01   \") \n",
    "#  print()\n",
    "#  do_model(LinearSVC(C = 0.1 ), np_mnist[\"train_v\"], np_mnist[\"train_label\"], np_mnist[\"test_v\"], np_mnist[\"test_label\"], \"MNist: LinearSVC c=0.1    \") \n",
    "#  print()\n",
    "#  do_model(LinearSVC(C = 1.0 ), np_mnist[\"train_v\"], np_mnist[\"train_label\"], np_mnist[\"test_v\"], np_mnist[\"test_label\"], \"MNist: LinearSVC c=1.0    \") \n",
    "#  print()\n",
    "#  do_model(LinearSVC(C = 10.0), np_mnist[\"train_v\"], np_mnist[\"train_label\"], np_mnist[\"test_v\"], np_mnist[\"test_label\"], \"MNist: LinearSVC c=10.0   \") \n",
    "#  print()\n",
    "\n",
    "print()\n",
    "print(tabulate(l_history, headers=[\"Category\", \"Accuracy\"], tablefmt='psql', showindex=False))\n",
    "print()\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1b1c44-a03b-4965-9f81-c838e23136ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Sample results\n",
    "#\n",
    "#     MNist: Centroid ...\n",
    "#        Actual    labels from test......... [7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4 9 6 6 5 4 0 7 4 0 1 ... 5 2 4 9 4 3 6 4 1 7 2 6 5 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6]\n",
    "#        Predicted labels from test......... [7 2 1 0 4 1 4 9 2 9 0 2 9 0 1 5 9 7 3 4 7 6 4 5 4 0 7 4 0 1 ... 3 2 4 9 4 2 6 4 1 7 0 6 6 0 1 8 8 4 5 6 7 8 4 0 1 2 3 4 5 6]\n",
    "#        ###\n",
    "#        Accuracy: 82.0300 %\n",
    "#     \n",
    "#     MNist: kNN=3 ...\n",
    "#        Actual    labels from test......... [7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4 9 6 6 5 4 0 7 4 0 1 ... 5 2 4 9 4 3 6 4 1 7 2 6 5 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6]\n",
    "#        Predicted labels from test......... [7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4 9 6 6 5 4 0 7 4 0 1 ... 5 2 4 9 4 3 6 4 1 7 2 6 3 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6]\n",
    "#        ###\n",
    "#        Accuracy: 97.0500 %\n",
    "#     MNist: kNN=7 ...\n",
    "#        Actual    labels from test......... [7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4 9 6 6 5 4 0 7 4 0 1 ... 5 2 4 9 4 3 6 4 1 7 2 6 5 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6]\n",
    "#        Predicted labels from test......... [7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4 9 6 6 5 4 0 7 4 0 1 ... 5 2 4 9 4 3 6 4 1 7 2 6 6 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6]\n",
    "#        ###\n",
    "#        Accuracy: 96.9400 %\n",
    "#     \n",
    "#     MNist: GaussianNB ...\n",
    "#        Actual    labels from test......... [7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4 9 6 6 5 4 0 7 4 0 1 ... 5 2 4 9 4 3 6 4 1 7 2 6 5 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6]\n",
    "#        Predicted labels from test......... [9 2 1 0 9 1 8 9 4 9 0 6 9 0 1 0 9 7 2 9 9 6 6 8 9 0 7 9 0 1 ... 6 0 8 9 8 8 6 9 1 9 3 6 6 0 1 9 8 9 8 6 9 8 9 0 1 8 8 9 8 6]\n",
    "#        ###\n",
    "#        Accuracy: 55.5800 %\n",
    "#     \n",
    "#     MNist: MultinomialNB ...\n",
    "#        Actual    labels from test......... [7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4 9 6 6 5 4 0 7 4 0 1 ... 5 2 4 9 4 3 6 4 1 7 2 6 5 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6]\n",
    "#        Predicted labels from test......... [7 2 1 0 4 1 4 9 4 9 0 6 9 0 1 3 9 7 3 4 9 6 6 5 4 0 7 4 0 1 ... 2 2 4 9 4 2 6 4 1 7 2 6 6 0 1 8 8 4 5 6 7 8 9 0 1 2 3 9 8 6]\n",
    "#        ###\n",
    "#        Accuracy: 83.6500 %\n",
    "#     \n",
    "#     MNist: DecisionTree ...\n",
    "#        Actual    labels from test......... [7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4 9 6 6 5 4 0 7 4 0 1 ... 5 2 4 9 4 3 6 4 1 7 2 6 5 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6]\n",
    "#        Predicted labels from test......... [7 2 1 0 4 1 4 7 6 9 0 6 9 0 1 5 9 7 6 4 9 6 6 5 4 0 7 4 0 1 ... 5 2 4 9 4 3 8 4 1 7 5 6 8 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6]\n",
    "#        ###\n",
    "#        Accuracy: 87.6700 %\n",
    "#     \n",
    "#     MNist: Random Forest = 5    ...\n",
    "#        Actual    labels from test......... [7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4 9 6 6 5 4 0 7 4 0 1 ... 5 2 4 9 4 3 6 4 1 7 2 6 5 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6]\n",
    "#        Predicted labels from test......... [7 2 1 0 4 1 4 9 4 9 0 6 9 0 1 5 9 7 3 4 9 6 6 5 4 0 7 4 0 1 ... 5 2 4 9 4 2 6 4 1 7 8 6 6 0 1 2 3 4 5 6 7 3 9 0 1 2 3 4 5 6]\n",
    "#        ###\n",
    "#        Accuracy: 91.9100 %\n",
    "#     MNist: Random Forest = 50   ...\n",
    "#        Actual    labels from test......... [7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4 9 6 6 5 4 0 7 4 0 1 ... 5 2 4 9 4 3 6 4 1 7 2 6 5 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6]\n",
    "#        Predicted labels from test......... [7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4 9 6 6 5 4 0 7 4 0 1 ... 5 2 4 9 4 2 6 4 1 7 3 6 3 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6]\n",
    "#        ###\n",
    "#        Accuracy: 96.7000 %\n",
    "#     MNist: Random Forest = 500  ...\n",
    "#        Actual    labels from test......... [7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4 9 6 6 5 4 0 7 4 0 1 ... 5 2 4 9 4 3 6 4 1 7 2 6 5 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6]\n",
    "#        Predicted labels from test......... [7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4 9 6 6 5 4 0 7 4 0 1 ... 5 2 4 9 4 2 6 4 1 7 2 6 6 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6]\n",
    "#        ###\n",
    "#        Accuracy: 97.0400 %\n",
    "#     MNist: Random Forest = 5000 ...\n",
    "#        Actual    labels from test......... [7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4 9 6 6 5 4 0 7 4 0 1 ... 5 2 4 9 4 3 6 4 1 7 2 6 5 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6]\n",
    "#        Predicted labels from test......... [7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4 9 6 6 5 4 0 7 4 0 1 ... 3 2 4 9 4 2 6 4 1 7 2 6 6 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6]\n",
    "#        ###\n",
    "#        Accuracy: 97.1800 %\n",
    "#     \n",
    "#     /opt/conda/lib/python3.8/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
    "#       warnings.warn(\n",
    "#     \n",
    "#     MNist: LinearSVC c=0.01    ...\n",
    "#        Actual    labels from test......... [7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4 9 6 6 5 4 0 7 4 0 1 ... 5 2 4 9 4 3 6 4 1 7 2 6 5 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6]\n",
    "#        Predicted labels from test......... [7 2 1 0 4 1 4 9 6 9 0 6 9 0 1 5 9 7 2 4 9 6 6 5 4 0 7 4 0 1 ... 3 2 4 9 4 3 6 4 1 7 3 6 6 0 1 2 3 4 5 6 7 3 9 0 1 2 3 4 5 6]\n",
    "#        ###\n",
    "#        Accuracy: 87.1200 %\n",
    "#     \n",
    "#     /opt/conda/lib/python3.8/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
    "#       warnings.warn(\n",
    "#     \n",
    "#     MNist: LinearSVC c=0.1     ...\n",
    "#        Actual    labels from test......... [7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4 9 6 6 5 4 0 7 4 0 1 ... 5 2 4 9 4 3 6 4 1 7 2 6 5 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6]\n",
    "#        Predicted labels from test......... [7 2 1 0 4 1 4 1 6 4 0 6 9 0 1 5 9 7 2 4 9 6 6 5 4 0 7 4 0 1 ... 3 2 4 4 4 2 6 4 1 7 3 6 6 0 1 2 3 4 5 6 7 3 4 0 1 2 3 4 5 6]\n",
    "#        ###\n",
    "#        Accuracy: 86.4700 %\n",
    "#     \n",
    "#     /opt/conda/lib/python3.8/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
    "#       warnings.warn(\n",
    "#     \n",
    "#     MNist: LinearSVC c=1.0     ...\n",
    "#        Actual    labels from test......... [7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4 9 6 6 5 4 0 7 4 0 1 ... 5 2 4 9 4 3 6 4 1 7 2 6 5 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6]\n",
    "#        Predicted labels from test......... [7 2 1 0 4 1 4 9 4 9 0 6 9 0 1 5 9 7 3 4 9 6 6 5 4 0 7 4 0 1 ... 3 2 4 9 4 3 6 4 1 7 3 6 2 0 1 2 3 4 5 6 7 3 9 0 1 2 3 4 5 6]\n",
    "#        ###\n",
    "#        Accuracy: 83.9900 %\n",
    "#     \n",
    "#     MNist: LinearSVC c=10.0    ...\n",
    "#        Actual    labels from test......... [7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4 9 6 6 5 4 0 7 4 0 1 ... 5 2 4 9 4 3 6 4 1 7 2 6 5 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6]\n",
    "#        Predicted labels from test......... [7 2 1 0 4 1 8 9 6 9 0 6 9 0 1 5 9 7 3 4 9 6 6 5 4 0 7 4 0 1 ... 3 2 4 9 4 3 6 4 1 7 3 6 6 0 1 8 8 4 5 6 7 8 9 0 1 8 3 5 5 6]\n",
    "#        ###\n",
    "#        Accuracy: 83.6900 %\n",
    "#     \n",
    "#     \n",
    "#     +-----------------------------+------------+\n",
    "#     | Category                    | Accuracy   |\n",
    "#     |-----------------------------+------------|\n",
    "#     |                             |            |\n",
    "#     | MNist: Centroid             | 82.03      |\n",
    "#     | MNist: kNN=3                | 97.05      |\n",
    "#     | MNist: kNN=7                | 96.94      |\n",
    "#     | MNist: GaussianNB           | 55.58      |\n",
    "#     | MNist: MultinomialNB        | 83.65      |\n",
    "#     | MNist: DecisionTree         | 87.67      |\n",
    "#     | MNist: Random Forest = 5    | 91.91      |\n",
    "#     | MNist: Random Forest = 50   | 96.7       |\n",
    "#     | MNist: Random Forest = 500  | 97.04      |\n",
    "#     | MNist: Random Forest = 5000 | 97.18      |\n",
    "#     | MNist: LinearSVC c=0.01     | 87.12      |\n",
    "#     | MNist: LinearSVC c=0.1      | 86.47      |\n",
    "#     | MNist: LinearSVC c=1.0      | 83.99      |\n",
    "#     | MNist: LinearSVC c=10.0     | 83.69      |\n",
    "#     +-----------------------------+------------+\n",
    "#     \n",
    "#     --\n",
    "#     \n",
    "#     /opt/conda/lib/python3.8/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
    "#       warnings.warn(\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8612d30f-3774-4620-a843-7c04c7a6095f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Regarding this,\n",
    "#\n",
    "#     /opt/conda/lib/python3.8/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning:\n",
    "#        Liblinear failed to converge, increase the number of iterations.\n",
    "#        warnings.warn(\n",
    "#\n",
    "#  From,\n",
    "#     https://stackoverflow.com/questions/52670012/convergencewarning-liblinear-failed-to-converge-increase-the-number-of-iterati\n",
    "#\n",
    "#      Normally when an optimization algorithm does not converge, it is usually because the problem is not well-conditioned,\n",
    "#      perhaps due to a poor scaling of the decision variables. There are a few things you can try.\n",
    "#      \n",
    "#          Normalize your training data so that the problem hopefully becomes more well conditioned, which in turn can speed up\n",
    "#          convergence. One possibility is to scale your data to 0 mean, unit standard deviation using Scikit-Learn's StandardScaler\n",
    "#          for an example.\n",
    "#\n",
    "#          Note that you have to apply the StandardScaler fitted on the training data to the test data. Also, if you have discrete\n",
    "#          features, make sure they are transformed properly so that scaling them makes sense.\n",
    "#\n",
    "#          Related to 1), make sure the other arguments such as regularization weight, C, is set appropriately. C has to be > 0.\n",
    "#          Typically one would try various values of C in a logarithmic scale (1e-5, 1e-4, 1e-3, ..., 1, 10, 100, ...) before\n",
    "#          finetuning it at finer granularity within a particular interval. These days, it probably make more sense to tune\n",
    "#          parameters using, for e.g., Bayesian Optimization using a package such as Scikit-Optimize.\n",
    "#\n",
    "#          Set max_iter to a larger value. The default is 1000. This should be your last resort. If the optimization process does\n",
    "#          not converge within the first 1000 iterations, having it converge by setting a larger max_iter typically masks other\n",
    "#          problems such as those described in 1) and 2). It might even indicate that you have some in appropriate features or\n",
    "#          strong correlations in the features. Debug those first before taking this easy way out.\n",
    "#\n",
    "#          Set dual = True if number of features > number of examples and vice versa. This solves the SVM optimization problem using\n",
    "#          the dual formulation. Thanks @Nino van Hooff for pointing this out, and @JamesKo for spotting my mistake.\n",
    "#\n",
    "#          Use a different solver, for e.g., the L-BFGS solver if you are using Logistic Regression. See @5ervant's answer.\n",
    "#      \n",
    "#      Note: One should not ignore this warning.\n",
    "#      \n",
    "#      This warning came about because\n",
    "#      \n",
    "#          Solving the linear SVM is just solving a quadratic optimization problem. The solver is typically an iterative algorithm\n",
    "#          that keeps a running estimate of the solution (i.e., the weight and bias for the SVM). It stops running when the solution\n",
    "#          corresponds to an objective value that is optimal for this convex optimization problem, or when it hits the maximum number\n",
    "#          of iterations set.\n",
    "#      \n",
    "#          If the algorithm does not converge, then the current estimate of the SVM's parameters are not guaranteed to be any good, \n",
    "#          hence the predictions can also be complete garbage.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff57e71-9dc7-4e7c-9e6c-eb447e07867a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#  Effect of randomness, moving the bits around inside each image.\n",
    "#\n",
    "#  Note; each row is randomzied by its own unique pattern.\n",
    "#\n",
    "\n",
    "#  The following variables are in scope ..\n",
    "#\n",
    "#     np_mnist[\"train\"] \n",
    "#     np_mnist[\"train_label\"]\n",
    "#     np_mnist[\"test\"]\n",
    "#     np_mnist[\"test_label\"]\n",
    "#     np_mnist[\"train_v\"]           #  vectors of the two data sets above\n",
    "#     np_mnist[\"test_v\"] \n",
    "#\n",
    "\n",
    "#  Here we want to copy the two \"v\" arrays and randomize them\n",
    "#\n",
    "np_mnist[\"train_v_s\"] = np.copy(np_mnist[\"train_v\"])\n",
    "np_mnist[\"test_v_s\" ] = np.copy(np_mnist[\"test_v\" ])\n",
    "   #\n",
    "for i in range(np_mnist[\"train_v_s\"].shape[0]):\n",
    "   np.random.shuffle(np_mnist[\"train_v_s\"][i, :])\n",
    "for i in range(np_mnist[\"test_v_s\" ].shape[0]):\n",
    "   np.random.shuffle(np_mnist[\"test_v_s\" ][i, :])\n",
    "\n",
    "\n",
    "#  Looking at the non-scrambled, and yes-scrambled data\n",
    "#\n",
    "#  Currently the data lives as a vector. To look at it, copy\n",
    "#  it back to a 28*28 numpy array. We only need this for two\n",
    "#  rows we wish to view, and we choose to use test.\n",
    "#\n",
    "np_mnist[\"test_s\"] = np.zeros((2, np_mnist[\"test\"].shape[1], np_mnist[\"test\"].shape[2]), dtype = int)\n",
    "   #\n",
    "for i in range(np_mnist[\"test_s\"].shape[0]):\n",
    "   np_mnist[\"test_s\"][i,:,:] = np_mnist[\"test_v_s\"][i].reshape(28, 28)\n",
    "\n",
    "#  And the actual print\n",
    "#\n",
    "#  Non-randomized bits\n",
    "#\n",
    "print(np_mnist[\"test_label\"][0:2])\n",
    "print(np_mnist[\"test\"][0:2])\n",
    "print(\"Number of rows: %d\" % (len(np_mnist[\"test\"])))\n",
    "   #\n",
    "print()\n",
    "\n",
    "#  Problems with print formatting. These lines help\n",
    "#\n",
    "np.set_printoptions()\n",
    "np.set_printoptions(edgeitems = 30, linewidth = 100000,  formatter = dict(int = lambda x: \"%3i\" % x))\n",
    "\n",
    "#  Randomized bits\n",
    "#\n",
    "print(np_mnist[\"test_label\"][0:2])\n",
    "print(np_mnist[\"test_s\"][0:2])\n",
    "print(\"Number of rows: %d\" % (len(np_mnist[\"test_s\"])))\n",
    "   #\n",
    "print()\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1983d12-36a3-42fd-83dc-3fb41fe464d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Sample results\n",
    "#\n",
    "#     [  7   2]\n",
    "#     [[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0  84 185 159 151  60  36   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0 222 254 254 254 254 241 198 198 198 198 198 198 198 198 170  52   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0  67 114  72 114 163 227 254 225 254 254 254 250 229 254 254 140   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0  17  66  14  67  67  67  59  21 236 254 106   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  83 253 209  18   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  22 233 255  83   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 129 254 238  44   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  59 249 254  62   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 133 254 187   5   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   9 205 248  58   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 126 254 182   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  75 251 240  57   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0  19 221 254 166   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   3 203 254 219  35   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0  38 254 254  77   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0  31 224 254 115   1   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0 133 254 254  52   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0  61 242 254 254  52   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0 121 254 254 219  40   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0 121 254 207  18   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]]\n",
    "#     \n",
    "#      [[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0 116 125 171 255 255 150  93   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0 169 253 253 253 253 253 253 218  30   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0 169 253 253 253 213 142 176 253 253 122   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0  52 250 253 210  32  12   0   6 206 253 140   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0  77 251 210  25   0   0   0 122 248 253  65   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0  31  18   0   0   0   0 209 253 253  65   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0 117 247 253 198  10   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0  76 247 253 231  63   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0 128 253 253 144   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0 176 246 253 159  12   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0  25 234 253 233  35   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0 198 253 253 141   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0  78 248 253 189  12   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0  19 200 253 253 141   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0 134 253 253 173  12   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0 248 253 253  25   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0 248 253 253  43  20  20  20  20   5   0   5  20  20  37 150 150 150 147  10   0]\n",
    "#       [  0   0   0   0   0   0   0   0 248 253 253 253 253 253 253 253 168 143 166 253 253 253 253 253 253 253 123   0]\n",
    "#       [  0   0   0   0   0   0   0   0 174 253 253 253 253 253 253 253 253 253 253 253 249 247 247 169 117 117  57   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0 118 123 123 123 166 253 253 253 155 123 123  41   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]]]\n",
    "#     Number of rows: 10000\n",
    "#     \n",
    "#     [  7   2]\n",
    "#     [[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [ 61   0  38   0 254   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   9   0   0   0   0   0   0   0   0   0   0   0   0   0   0 254   0   0   0 198 241   0   0   0]\n",
    "#       [  0   0   0   0   0   0 170   0 121   0 233 254   0   0   0 115   0 185 198   0   0   0   0   0 129   0   0   0]\n",
    "#       [  0   0 225   0   0   0   0   0   0   0   0   0   0 205   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [159   0   0  19   0   0   0   0   0  18   0   0   0   0  58   0  21 254   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0 229   0   0   0   0 121   0   0   0   0   0   0   0   0   0   0   0   0   0   0 254   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 254   0  60   0   0 238  67   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0 151   0   0   0  59   0 254   0   0   0   0   0   0   0   0 207   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0 209   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0  67   0   0   0   0   0 254   0   0   0   0   0   0   0   0  52]\n",
    "#       [  0  57 163  77   0   0   0   0   0   0  84   0   0   0   0   0   0   0   0   0   0   0  52   0   0   0   0 248]\n",
    "#       [  0   0   0   0   0   0   0   0 254   0   0   0 236   0   0   0 249   0   0   0   0   0   0   0  59   0   0   0]\n",
    "#       [  0   0   0   1   0   0   0   0 221   0   0   0   0   0   0   0   0   0   0   0   0 254   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0 133   0   0   0   0   0   0 240   0   0   0 219   0   0   0   0]\n",
    "#       [  0 251   0   0   0   0   0  36   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  17]\n",
    "#       [  0   0   0  31   0   0   0 254   0 254   0   0   0   0   0   0   0 219   0  66   0   0   0   0   0   0   0   0]\n",
    "#       [166   0   0   0   0 254   0   0 254   0   0   0   0   0   0   0   0   0   0   0   0   0  62   0   0   3   0   0]\n",
    "#       [  0   0   0   0   0  18   0   0   0   0   0   0 254   0   0   0   0 254   0   0  83   0   0   0   0  40   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0 250  75   0   0 198   0   0   0   0   0   0   0 203   0   0   0  67 114   0   0]\n",
    "#       [  0   0   0 140   0   0   0   0   0   0   0 254   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0 106   0 227   0  52   0   0   0   0   0   0   0   0   0   0  14   0   0   0   0   0   0   0 222]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 254   0 182]\n",
    "#       [126   0   0   0   0   0   0   0   0   0   0 254   0   0   0   0   0   0   0   0 242   0   0  44   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0 254 198 254   0   0   0 114   0   0   0   0 133   0   0 198   0   0   0   0]\n",
    "#       [  0   0   0   0   0 254 253   5   0   0   0 187   0   0   0   0  67   0   0   0  72 198   0  83   0   0   0   0]\n",
    "#       [  0   0 254 198   0 254 254   0   0   0   0   0 254   0   0   0   0   0   0   0   0   0   0  35   0   0  22   0]\n",
    "#       [  0   0   0 255   0   0   0   0 254   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 198   0 224]]\n",
    "#     \n",
    "#      [[  0   0 247   0   0 253  12 118  31   0 248   0   0   0   0   0 176 166   0   0   0   0   0  25   0   0   0   0]\n",
    "#       [  0   0 209   0  93   0   0   0   0   0   0   0   0   0   0 123   0   0   0   0 200   0  35  10   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0 141   0 253 253   0   0   0   0   0   0   0   0 253   0   0   0 253   0   0 253]\n",
    "#       [  0   0   0   0   0   0   0   0   0 117   0   0 206 122 253  10   0 150   0   0   0   0 253 253   0   0 253 253]\n",
    "#       [ 43   0   0   0   0   0   0   0   0   0   0   0 253   0   0   0 253   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0  32   0   0 253   0   0   0   0 253 155   0   0   0   0   0   0   0   0 198 140   0   0   0   0   0]\n",
    "#       [  0   0   0  12   0   0   0 141   0 255   0   0   0  30   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [249   0   0   0   0 123 166   0   0   0   0   0   0 253   0   0   0   0 210 248   0   0   0  20   0   0   0   0]\n",
    "#       [  0   0 123  18   0   0   0   0   0   0   0   0  20 253   0   0   0   0   0   0 198   0 253   0   0   0   0   0]\n",
    "#       [  0 255   0   0 253   0   0   0   0   0   0   0 253   0   0   0   0   0   0   0   0   0   0  63   0   0 253   0]\n",
    "#       [  0   0   0   0 116   6   0   0   0   0  20   0   0   0 248   0   0  12 142   0   0   0   0   0 253   0 253   0]\n",
    "#       [  0   0   0   0 253   0   0   0   0   0 150   0   0   0   0 253 234   0   0 253   0   0   0   0  78   0   0   0]\n",
    "#       [  0   0   0 253   0   0   0   0   0   0   0 169   0   0 218   0   0   0   0   0 253   0   0 253   0   0   5   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0 213   0   0   0   0   0   0 253   0   0 253   0   0 231   0   0   0 253]\n",
    "#       [  0   0 253   0   0   0   0   0   0 134   0   0   0   0   0   0 253   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [174   0   0   0   0   0   0   0  19   0 123   0   0   0   0   0   0   0   0 171 168   0   0   0   0   0   0 128]\n",
    "#       [  0   0  20  41 253   0 253 144   0   0  76   0   0   0   0 253   0   0   0 176   0 159   0 253   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0 248   0  20   0   0   0   0   0   0   0 253   0   0   0   0   0   0   0   0   0]\n",
    "#       [246   0   0   0   0   0   0   0   0   0   0   0  25   0   0   0   0 253   0 253   0   0 253   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0 253 253   0   0   0   0   0   0   0   0 123   0   0   0 117   0   0   0   0   0 125   5]\n",
    "#       [  0   0   0   0   0   0 253   0   0   0   0   0   0   0 250   0 253   0   0  52   0 122 123   0   0   0   0   0]\n",
    "#       [  0 147   0 253   0   0 169   0   0   0   0   0 253   0 253   0   0   0   0   0   0 143   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0 253   0   0   0   0 247   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 150   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0 253   0 253   0   0   0   0  77   0 253   0   0   0   0]\n",
    "#       [248  37   0   0   0   0   0  65   0   0   0   0 173 233   0   0 253   0   0 253   0   0   0   0   0   0   0   0]\n",
    "#       [150   0 169 247   0   0   0   0   0  12   0   0   0   0   0   0  25   0   0   0   0  65   0   0   0   0 189   0]\n",
    "#       [  0 253   0 251   0   0   0   0   0   0   0   0 253 253   0   0   0   0   0 253   0   0   0   0   0   0   0   0]\n",
    "#       [117   0 247   0   0   0   0   0   0   0   0   0   0   0   0   0   0 253  20 210 253   0  57   0   0   0   0   0]]]\n",
    "#     Number of rows: 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b208e4-2d65-4efa-9ed1-f4f51be6bf35",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Creating a bar chart; Are these the same values ?\n",
    "#\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "l_hs = np.hstack(np_mnist[\"test_v\"][0])\n",
    "_ = plt.hist(l_hs, bins='auto') \n",
    "   #\n",
    "plt.title(\"784 possible values, range 0-256: Image pre-randomization\")\n",
    "plt.xlabel('RGB Value')\n",
    "plt.ylabel('Count of Said Value')\n",
    "   #\n",
    "plt.show()\n",
    "\n",
    "l_hs = np.hstack(np_mnist[\"test_v_s\"][0])\n",
    "_ = plt.hist(l_hs, bins='auto') \n",
    "   #\n",
    "plt.title(\"784 possible values, range 0-256: Image post-randomization\")\n",
    "plt.xlabel('RGB Value')\n",
    "plt.ylabel('Count of Said Value')\n",
    "   #\n",
    "plt.show()\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266e165c-4d92-48cb-8ee2-85782fc4119d",
   "metadata": {},
   "source": [
    "<div> \n",
    "<img src=\"./01_Images/07_Results_BarChart.png\" alt=\"Drawing\" style=\"width: 800px;\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6316f92-89a4-440c-9861-090b6f6e8fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Rerun ML routines now on the scrambled images\n",
    "#\n",
    "\n",
    "#  To add a blank line to history-\n",
    "#\n",
    "l_history.add(event = \"\", measure = \"\")\n",
    "\n",
    "\n",
    "do_model(NearestCentroid(), np_mnist[\"train_v_s\"], np_mnist[\"train_label\"], np_mnist[\"test_v_s\"], np_mnist[\"test_label\"], \"MNist: Centroid, Scramble\") \n",
    "print()\n",
    "\n",
    "do_model(KNeighborsClassifier(n_neighbors =  3), np_mnist[\"train_v_s\"], np_mnist[\"train_label\"], np_mnist[\"test_v_s\"], np_mnist[\"test_label\"], \"MNist: kNN=3, Scramble\" ) \n",
    "do_model(KNeighborsClassifier(n_neighbors =  7), np_mnist[\"train_v_s\"], np_mnist[\"train_label\"], np_mnist[\"test_v_s\"], np_mnist[\"test_label\"], \"MNist: kNN=7, Scramble\") \n",
    "print()\n",
    "\n",
    "do_model(GaussianNB(), np_mnist[\"train_v_s\"], np_mnist[\"train_label\"], np_mnist[\"test_v_s\"], np_mnist[\"test_label\"], \"MNist: GaussianNB, Scramble\") \n",
    "print()\n",
    "\n",
    "do_model(MultinomialNB(), np_mnist[\"train_v_s\"], np_mnist[\"train_label\"], np_mnist[\"test_v_s\"], np_mnist[\"test_label\"], \"MNist: MultinomialNB, Scramble\") \n",
    "print()\n",
    "\n",
    "do_model(DecisionTreeClassifier(), np_mnist[\"train_v_s\"], np_mnist[\"train_label\"], np_mnist[\"test_v_s\"], np_mnist[\"test_label\"], \"MNist: DecisionTree, Scramble\") \n",
    "print()\n",
    "\n",
    "#  n_estimators, number of random trees created and trained\n",
    "#\n",
    "#  do_model(RandomForestClassifier(n_estimators = 5   ), np_mnist[\"train_scramble_v\"], np_mnist[\"train_label\"], np_mnist[\"test_scramble_v\"], np_mnist[\"test_label\"], \"MNist: Random Forest = 5   , Scramble\") \n",
    "#  do_model(RandomForestClassifier(n_estimators = 50  ), np_mnist[\"train_scramble_v\"], np_mnist[\"train_label\"], np_mnist[\"test_scramble_v\"], np_mnist[\"test_label\"], \"MNist: Random Forest = 50  , Scramble\") \n",
    "#  do_model(RandomForestClassifier(n_estimators = 500 ), np_mnist[\"train_scramble_v\"], np_mnist[\"train_label\"], np_mnist[\"test_scramble_v\"], np_mnist[\"test_label\"], \"MNist: Random Forest = 500 , Scramble\") \n",
    "#  do_model(RandomForestClassifier(n_estimators = 5000), np_mnist[\"train_scramble_v\"], np_mnist[\"train_label\"], np_mnist[\"test_scramble_v\"], np_mnist[\"test_label\"], \"MNist: Random Forest = 5000, Scramble\") \n",
    "\n",
    "print()\n",
    "\n",
    "   ###\n",
    "\n",
    "print()\n",
    "print(tabulate(l_history, headers=[\"Category\", \"Accuracy\"], tablefmt='psql', showindex=False))\n",
    "print()\n",
    "\n",
    "print(\"--\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf646e34-43c1-48a0-b369-7aa59aed6fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  The results below were on images that were not scrambled.\n",
    "#\n",
    "#     +-----------------------------+-------------------+\n",
    "#     | Category                    | Accuracy          |\n",
    "#     |-----------------------------+-------------------|\n",
    "#     | MNist: Centroid             | 82.03             |\n",
    "#     | MNist: kNN=3                | 97.05             |\n",
    "#     | MNist: kNN=7                | 96.94             |\n",
    "#     | MNist: GaussianNB           | 55.58             |\n",
    "#     | MNist: MultinomialNB        | 83.65             |\n",
    "#     | MNist: DecisionTree         | 87.72             |\n",
    "#     | MNist: Random Forest = 5    | 92.36999999999999 |\n",
    "#     | MNist: Random Forest = 50   | 96.67999999999999 |\n",
    "#     | MNist: Random Forest = 500  | 97.15             |\n",
    "#     | MNist: Random Forest = 5000 | 97.17             |\n",
    "#     |                             |                   |\n",
    "#     +-----------------------------+-------------------+\n",
    "#\n",
    "#  The results below on images when data is entirely randomized row by row ..\n",
    "#\n",
    "#     +--------------------------------+--------------------+\n",
    "#     | Category                       | Accuracy           |\n",
    "#     |--------------------------------+--------------------|\n",
    "#     | MNist: Centroid, Scramble      | 22.03              |\n",
    "#     | MNist: kNN=3, Scramble         | 11.540000000000001 |\n",
    "#     | MNist: kNN=7, Scramble         | 11.379999999999999 |\n",
    "#     | MNist: GaussianNB, Scramble    | 21.37              |\n",
    "#     | MNist: MultinomialNB, Scramble | 9.93               |\n",
    "#     | MNist: DecisionTree, Scramble  | 12.889999999999999 |\n",
    "#     |                                |                    |\n",
    "#     +--------------------------------+--------------------+\n",
    "#\n",
    "#  Takeaway,\n",
    "#\n",
    "#     .  Our data is 256 value bits * 2-dim array of (28 * 28).\n",
    "#        28 * 28 == 784\n",
    "#        Per each 28*28 image (784 bits), 650-700 of those bits are blank/zero.\n",
    "#\n",
    "#       So, to randomize the location of order bits itself is not bad, but it\n",
    "#       does not leave us with enough training data.\n",
    "#\n",
    "#       Meaning; a totally random \"7\" [ might ] be unique enough from a \"2\" or\n",
    "#       any other number, but we'd need a lot more data.\n",
    "#\n",
    "#     .  The approach now is to randomize the bits, but on a single consistent\n",
    "#        pattern applied to each row.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbebe7e5-79b4-4e75-8ab9-8eb30b370e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Effect of randomness, moving the bits around inside each image.\n",
    "#\n",
    "#  Note; each row is randomized by one shared pattern.\n",
    "#\n",
    "\n",
    "#  The following variables are in scope ..\n",
    "#\n",
    "#     np_mnist[\"train\"] \n",
    "#     np_mnist[\"train_label\"]\n",
    "#     np_mnist[\"test\"]\n",
    "#     np_mnist[\"test_label\"]\n",
    "#     np_mnist[\"train_v\"]           #  vectors of the two data sets above\n",
    "#     np_mnist[\"test_v\"] \n",
    "#\n",
    "\n",
    "#  Here we want to copy the two \"v\" arrays \n",
    "#\n",
    "np_mnist[\"train_v_s2\"] = np.copy(np_mnist[\"train_v\"])\n",
    "np_mnist[\"test_v_s2\" ] = np.copy(np_mnist[\"test_v\" ])\n",
    "\n",
    "\n",
    "#  Make a new vector of values 0-n, randomize that, uses this as a map to\n",
    "#  consistently 'randomize' all remaining data\n",
    "#\n",
    "np_random = np.arange(0, np_mnist[\"train_v_s2\"].shape[1] -1, 1, dtype = int)\n",
    "np.random.shuffle(np_random)\n",
    "\n",
    "\n",
    "#  Apply the actual 'randomization'\n",
    "#\n",
    "for i in range(np_mnist[\"train_v_s2\"].shape[0]):\n",
    "   l_tmp = np.zeros(np_mnist[\"train_v_s2\"][i].shape[0], dtype = int)\n",
    "   for j in range(np_mnist[\"train_v_s2\"][i].shape[0] -1):\n",
    "      l_tmp[j] = np_mnist[\"train_v_s2\"][i][ np_random[j] ]\n",
    "   np_mnist[\"train_v_s2\"][i, :] = l_tmp[:]\n",
    "\n",
    "\n",
    "       #\n",
    "for i in range(np_mnist[\"test_v_s2\"].shape[0]):\n",
    "   l_tmp = np.zeros(np_mnist[\"test_v_s2\"][i].shape[0], dtype = int)\n",
    "   for j in range(np_mnist[\"test_v_s2\"][i].shape[0] - 1):\n",
    "      l_tmp[j] = np_mnist[\"test_v_s2\"][i][ np_random[j] ]\n",
    "   np_mnist[\"test_v_s2\"][i, :] = l_tmp[:]\n",
    "\n",
    "\n",
    "#  Looking at the yes-scrambled data\n",
    "#\n",
    "#  Currently the data lives as a vector. To look at it, copy\n",
    "#  it back to a 28*28 numpy array. We only need this for two\n",
    "#  rows we wish to view, and we choose to use test.\n",
    "#\n",
    "np_mnist[\"test_s\"] = np.zeros((2, np_mnist[\"test\"].shape[1], np_mnist[\"test\"].shape[2]), dtype = int)\n",
    "   #\n",
    "for i in range(np_mnist[\"test_s\"].shape[0]):\n",
    "   np_mnist[\"test_s\"][i,:,:] = np_mnist[\"test_v_s2\"][i].reshape(np_mnist[\"test\"].shape[1], np_mnist[\"test\"].shape[2])\n",
    "\n",
    "\n",
    "#  Problems with print formatting. These lines help\n",
    "#\n",
    "np.set_printoptions()\n",
    "np.set_printoptions(edgeitems = 30, linewidth = 100000,  formatter = dict(int = lambda x: \"%3i\" % x))\n",
    "\n",
    "#  Randomized bits\n",
    "#\n",
    "print(np_mnist[\"test_label\"][0:2])\n",
    "print(np_mnist[\"test_s\"][0:2])\n",
    "print(\"Number of rows: %d\" % (len(np_mnist[\"test_s\"])))\n",
    "   #\n",
    "print()\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a02b6db-d527-4398-a4c9-782dc693ba79",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Sample output\n",
    "#\n",
    "#     [  7   2]\n",
    "#     [[[  0   0 133 205   0   0   0   0   0   0   0   0  18 219   0   0 254   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0 198   0   0 198   0   0   0 254   0   0   0   0   0   0  57   0   0   0   0 198   0   0   0   0]\n",
    "#       [  0 151   0   0   0   0 254   0   0   0   0   0   0   0   0  75   0   0   0   0   0  17   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0 254   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 198   0   0   0   0   0]\n",
    "#       [  0  21   0   0   0   0   0   0   0   0   0   0   0   0   0   0 236   0   0   0   0 254   0   0 121   0   0   0]\n",
    "#       [  0   0 241   0   0   0   0   0   0   0   0   0   0   0   0   0   0 254   0   0   0 255   0 254   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0 254   0   0   0   0   0 254   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   3   0   0   0 187   0  22   0 254   0 207   0   0   0  44   0   0   0  36   0   0   0  60   0 249   0]\n",
    "#       [  0   0   0   0   0 240  38   0 159   0   0   0   0 209   0 133   0   0   0   0   0   0   0  61 254   0   0   0]\n",
    "#       [  0   0   0 253   0   0 238   0 224   0   0   0   0   0   0  83   0  52   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0 254   0   0   0 229   0   0   0   0   0 254   0   0   0   0   0 114   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0 129   0   0   0   0   0 198   0   0   0   0   0 115   0   0   0   0  58   0   0   0 221]\n",
    "#       [  0 251   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 163   0   0   0]\n",
    "#       [  0  59   9   0   0  67   0   0   0   0   0   0   0 182   0   0   0   0 198   0   0   0   0   0   0   0  35   0]\n",
    "#       [  0   0 250   0   0  19   0 254   0   0   0   0   0   0   0   0   0   0   0   0   0  52 254   0   0   5   0   0]\n",
    "#       [  0   0  66 198   0   0   0   0   0   0 126   0  52   0   0   0 140   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0 254   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  14]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0  67   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 254]\n",
    "#       [  0   0   0   0   0   0   0 254   0 254   0   0   0 185   0   0  67   0   0   0   0   0   0   0  40 121   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0 166   0   0   0   0   0   0  31   0   0   0   0 222 203   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0 254   0 254   0   0   0   0   0   0   0   0 227   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 254   0 225   0  72   0   0   0  84   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0  18   0   0   0  59   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0 219   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 170   0   0   0 254   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0  77   0 106   0   0   0   0   0   0   0   0   0  67   0 233   0 198]\n",
    "#       [  0  62   0   0   0   0   0   0  83 248   0 254   0   0   0   0   0   0   0 254   0   0   0   0   0   0   0 242]\n",
    "#       [  0   0   0   0   0 114   0   0   0   0   0   0   0   0   1   0   0   0   0   0 254   0   0   0 254   0   0   0]]\n",
    "#     \n",
    "#      [[ 43   0   0   0   0   0 255   0   0   0   0   0   0   0   0   0  20   0   0   0   0   0   0   0  65   0  12   0]\n",
    "#       [  0   0   0   0 253   0 147 253 128   0   0 253   0   0   0  10   0   0   0   0   0   0   0   0   0 253   0   0]\n",
    "#       [  0 210   0   0 210  12   0 253   0 253   0 141 248 249   0   0 253   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0 144   0   0   0 253   0   0   0   0   0   0   0 253   0   0 142   0   0   0   0   0   0   0   0   0 253]\n",
    "#       [  0   0   0 253   0   0   0   0   0 246   0   0   0   0   0   0   0   0 253   0   0   0   0   0   0   0   0   0]\n",
    "#       [169   0   0   0   0   0   0   5   0   0   0   0   0   0   0 253   0 198 248   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0 253   0 247   0 253   0 253   0 166   0 250 253   0 143   0   0]\n",
    "#       [  0   0 253   0   0   0   0   0   0   0   0   0  12   0   0   0  25 150   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [253   0 253  20 253   0   0 253   0 176   0   0   0   0   0 125   0   0 253   0   0 169   0   0  25   0   0   0]\n",
    "#       [118   0   0   0 140   0 253   0 251 174   0  93   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 206]\n",
    "#       [  0   0   0   0   0 234   0   0 123   0   0   0   0   0   0   0   0   0   0   0   0  35   0   0   0   0   0   0]\n",
    "#       [  0 253   0 150  30   0   0   0 176   0   0 253   0   0   0   0   0   0   0 166   0   0   0   0   0   0 248   0]\n",
    "#       [  0   0   0   0 122   0   0   0   0   0   0   0   0   0   0   0   0   0 253   0 253   0   0   0  57   0  20  20]\n",
    "#       [  0   0   0   0 123   0   0   0   0   0   0   0   0   0  41 116   0   0   0   0   0 253 141 253   0   0   0 189]\n",
    "#       [  0   0   0   0   0  63   0   0   0   0   0   0 247   0   0   0   0 253 209   0   0 123   0   0   0   0 168 253]\n",
    "#       [ 25   0  10 150   0  20   0   0   0   0   0   0   0   0   0   0   0 253   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0  76   0   0   0 122   0   0 253   0 253   0   0   0   0   0   0 134   0   0   0   0   0   0   0   0   0]\n",
    "#       [ 37 213 253   0   0  18   0   0   0   0 253   0 253   0   0   0   0 253 253   0   0   0   0 117   0 248   0 247]\n",
    "#       [  0 255   0   0   0   0   0   0   0 253   0   0   0   0   0   0   0   0 253   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0 173   0   0   0   0   0  31   0   0   0   0   0  77  32   0 253   0 117 253   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0 169   0   0   0   0   0   5   0   0   0   0   0   0 123   0   0   0   0   0 253 198   0   0   0]\n",
    "#       [  0 253   0   0   0   0   0 253   0   0 253   0  20   0   0   0   0 123   0   0   0   0   0 123   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0 155   0   0  78   0   0   0 253   0 117   0   0   0 200   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 253   0]\n",
    "#       [  0   0 253   0   0 253   0 159   0   0   0   0   0   0 247   0   0   0   0   0   0   0   0   0 233   0   0   0]\n",
    "#       [  0   0 248 218   0   0 171   0   0   0   0 253   0   0   0   0 150   0   0 253   0 253   0 231  52   0   0  65]\n",
    "#       [  0   0   0   0   0   0   0  12   0   0   0   0   0   0 253   0   0   0   0   0   0   0   0   0   0   0  19   0]\n",
    "#       [253   0   0   0   6   0   0   0 253   0   0   0   0   0 253   0   0   0 253   0   0   0   0   0 253   0   0   0]]]\n",
    "#     Number of rows: 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346226c5-d1d2-4ee9-aa9b-192e950e22c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Rerun ML routines now on the scrambled images, those randomized by one shared pattern\n",
    "#\n",
    "\n",
    "#  To add a blank line to history-\n",
    "#\n",
    "l_history.add(event = \"\", measure = \"\")\n",
    "\n",
    "\n",
    "do_model(NearestCentroid(), np_mnist[\"train_v_s2\"], np_mnist[\"train_label\"], np_mnist[\"test_v_s2\"], np_mnist[\"test_label\"], \"MNist: Centroid, Scramble-2\") \n",
    "print()\n",
    "\n",
    "do_model(KNeighborsClassifier(n_neighbors =  3), np_mnist[\"train_v_s2\"], np_mnist[\"train_label\"], np_mnist[\"test_v_s2\"], np_mnist[\"test_label\"], \"MNist: kNN=3, Scramble-2\" ) \n",
    "do_model(KNeighborsClassifier(n_neighbors =  7), np_mnist[\"train_v_s2\"], np_mnist[\"train_label\"], np_mnist[\"test_v_s2\"], np_mnist[\"test_label\"], \"MNist: kNN=7, Scramble-2\") \n",
    "print()\n",
    "\n",
    "do_model(GaussianNB(), np_mnist[\"train_v_s2\"], np_mnist[\"train_label\"], np_mnist[\"test_v_s2\"], np_mnist[\"test_label\"], \"MNist: GaussianNB, Scramble-2\") \n",
    "print()\n",
    "\n",
    "do_model(MultinomialNB(), np_mnist[\"train_v_s2\"], np_mnist[\"train_label\"], np_mnist[\"test_v_s2\"], np_mnist[\"test_label\"], \"MNist: MultinomialNB, Scramble-2\") \n",
    "print()\n",
    "\n",
    "do_model(DecisionTreeClassifier(), np_mnist[\"train_v_s2\"], np_mnist[\"train_label\"], np_mnist[\"test_v_s2\"], np_mnist[\"test_label\"], \"MNist: DecisionTree, Scramble-2\") \n",
    "print()\n",
    "\n",
    "#  n_estimators, number of random trees created and trained\n",
    "#\n",
    "#  do_model(RandomForestClassifier(n_estimators = 5   ), np_mnist[\"train_scramble_v\"], np_mnist[\"train_label\"], np_mnist[\"test_scramble_v\"], np_mnist[\"test_label\"], \"MNist: Random Forest = 5   , Scramble\") \n",
    "#  do_model(RandomForestClassifier(n_estimators = 50  ), np_mnist[\"train_scramble_v\"], np_mnist[\"train_label\"], np_mnist[\"test_scramble_v\"], np_mnist[\"test_label\"], \"MNist: Random Forest = 50  , Scramble\") \n",
    "#  do_model(RandomForestClassifier(n_estimators = 500 ), np_mnist[\"train_scramble_v\"], np_mnist[\"train_label\"], np_mnist[\"test_scramble_v\"], np_mnist[\"test_label\"], \"MNist: Random Forest = 500 , Scramble\") \n",
    "#  do_model(RandomForestClassifier(n_estimators = 5000), np_mnist[\"train_scramble_v\"], np_mnist[\"train_label\"], np_mnist[\"test_scramble_v\"], np_mnist[\"test_label\"], \"MNist: Random Forest = 5000, Scramble\") \n",
    "\n",
    "print()\n",
    "\n",
    "   ###\n",
    "\n",
    "print()\n",
    "print(tabulate(l_history, headers=[\"Category\", \"Accuracy\"], tablefmt='psql', showindex=False))\n",
    "print()\n",
    "\n",
    "print(\"--\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420ad074-e31f-44ff-844a-2c77217929f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Sample output,\n",
    "#\n",
    "#  The results below were on images that were not scrambled.\n",
    "#\n",
    "#     +-----------------------------+-------------------+\n",
    "#     | Category                    | Accuracy          |\n",
    "#     |-----------------------------+-------------------|\n",
    "#     | MNist: Centroid             | 82.03             |\n",
    "#     | MNist: kNN=3                | 97.05             |\n",
    "#     | MNist: kNN=7                | 96.94             |\n",
    "#     | MNist: GaussianNB           | 55.58             |\n",
    "#     | MNist: MultinomialNB        | 83.65             |\n",
    "#     | MNist: DecisionTree         | 87.72             |\n",
    "#     | MNist: Random Forest = 5    | 92.36999999999999 |\n",
    "#     | MNist: Random Forest = 50   | 96.67999999999999 |\n",
    "#     | MNist: Random Forest = 500  | 97.15             |\n",
    "#     | MNist: Random Forest = 5000 | 97.17             |\n",
    "#     |                             |                   |\n",
    "#     +-----------------------------+-------------------+\n",
    "#\n",
    "#  The results below on images when data is randomized by one shared pattern ..\n",
    "#\n",
    "#     +----------------------------------+------------+\n",
    "#     | Category                         | Accuracy   |\n",
    "#     |----------------------------------+------------|\n",
    "#     |                                  |            |\n",
    "#     | MNist: Centroid, Scramble-2      | 82.03      |\n",
    "#     | MNist: kNN=3, Scramble-2         | 97.05      |\n",
    "#     | MNist: kNN=7, Scramble-2         | 96.94      |\n",
    "#     | MNist: GaussianNB, Scramble-2    | 55.58      |\n",
    "#     | MNist: MultinomialNB, Scramble-2 | 83.65      |\n",
    "#     | MNist: DecisionTree, Scramble-2  | 87.7       |\n",
    "#     +----------------------------------+------------+\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3976e37-8702-4f53-bf97-e28a958675f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a397bde-64f0-4ec9-a728-d37e4c51ecea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
