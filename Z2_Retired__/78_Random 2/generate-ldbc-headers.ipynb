{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46ff4746-b357-4439-8c0d-6362bbf3cb5f",
   "metadata": {},
   "source": [
    "# Create schema mapping files for LDBC initial snapshot and batch inserts (CSV)\n",
    "This notebook crawls through the LDBC csv directory and auto-discovers dataframes and creates header files based on file paths and column types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5969853-ac98-4f43-9e50-c8504f8bede0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "\n",
    "base_path = Path('/Users/andrew/datasets')\n",
    "dataset_path = base_path / 'sf-0.003/csv/bi/composite-projected-fk'\n",
    "headers_path = dataset_path / 'headers-v2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1d60b4a-6777-4d65-b16c-e8ff7ab07a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TYPE:\n",
    "    df: dd.DataFrame\n",
    "    files: list\n",
    "    \n",
    "def csv_to_df(path:Path):\n",
    "    df = dd.read_csv(path / '*.csv',\n",
    "                       delimiter='|',\n",
    "                       assume_missing=True,\n",
    "                       dtype={\n",
    "                           'id': int,\n",
    "                           'length': np.dtype('long'),\n",
    "                           'content': np.dtype('O'),\n",
    "                           'imageFile': np.dtype('O'),\n",
    "                           'classYear': np.dtype('long'),\n",
    "                           'workFrom': np.dtype('long'),\n",
    "                           'language': np.dtype('O'),\n",
    "                           'email': np.dtype('O')\n",
    "                       },\n",
    "                       converters={\n",
    "                           'creationDate': lambda x: pd.to_datetime(x, unit='ns'),\n",
    "                           'deletionDate': lambda x: pd.to_datetime(x, unit='ns'),\n",
    "                           'birthday': lambda x: pd.to_datetime(x),\n",
    "                           #'language': lambda x: x.split(';'),\n",
    "                           #'email': lambda x: x.split(';')\n",
    "                       })\n",
    "    type_name = path.name\n",
    "    # df['type'] = type_name\n",
    "    # df['typed_id'] = df.apply(lambda row: type_name + ':' + str(row['id']), axis=1, meta=('typed_id', 'str'))\n",
    "    return df\n",
    "\n",
    "def csv_files_in_dir(path, directory):\n",
    "    return [str((path / _).relative_to(directory)) for _ in path.iterdir() if _.match('*.csv')]\n",
    "\n",
    "def dfs_from_directory(directory:Path):\n",
    "    dfs = {}\n",
    "    for root, dirs, _ in os.walk(directory):\n",
    "        for name in dirs:\n",
    "            path = Path(os.path.join(root, name))\n",
    "            if any(_.match('*.csv') for _ in path.iterdir()):\n",
    "                dfs[str(path.relative_to(directory))] = TYPE(df=csv_to_df(path),\n",
    "                                                             files=csv_files_in_dir(path, dataset_path))\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6128d654-f469-4fbd-9072-f284ed23da2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = dfs_from_directory(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "521ea0e8-81b8-42aa-8447-f6b6258b9fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "examples, deletes/dynamic/Post/batch_id=2012-12-02, deletes/dynamic/Post/batch_id=2012-12-11, deletes/dynamic/Post/batch_id=2012-12-06, deletes/dynamic/Post/batch_id=2012-12-24, deletes/dynamic/Forum_hasMember_Person/batch_id=2012-10-13, deletes/dynamic/Forum_hasMember_Person/batch_id=2012-11-25, deletes/dynamic/Forum_hasMember_Person/batch_id=2012-12-02, deletes/dynamic/Forum_hasMember_Person/batch_id=2012-12-28, deletes/dynamic/Forum_hasMember_Person/batch_id=2012-12-19, deletes/dynamic/Forum_...\n"
     ]
    }
   ],
   "source": [
    "print(\", \".join(dfs)[:500] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5276c829-d9a2-4c47-9c74-9243f5b78122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_snapshot/dynamic/Post, initial_snapshot/dynamic/Forum_hasMember_Person, initial_snapshot/dynamic/Post_hasTag_Tag, initial_snapshot/dynamic/Comment_hasTag_Tag, initial_snapshot/dynamic/Person, initial_snapshot/dynamic/Comment_replyOf_Post, initial_snapshot/dynamic/Comment, initial_snapshot/dynamic/Comment_replyOf_Comment, initial_snapshot/dynamic/Comment_hasCreator_Person, initial_snapshot/dynamic/Post_isLocatedIn_Place, initial_snapshot/dynamic/Person_hasInterest_Tag, initial_snapshot/dy...\n"
     ]
    }
   ],
   "source": [
    "initial_snapshot_dfs = dict(filter(lambda x: x[0].startswith('initial_snapshot'), dfs.items()))\n",
    "print(\", \".join(initial_snapshot_dfs)[:500] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b6bb454-79ef-4624-a137-498b732bf77d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inserts/dynamic/Post/batch_id=2012-10-24, inserts/dynamic/Post/batch_id=2012-10-23, inserts/dynamic/Post/batch_id=2012-10-15, inserts/dynamic/Post/batch_id=2012-10-13, inserts/dynamic/Post/batch_id=2012-10-14, inserts/dynamic/Post/batch_id=2012-10-25, inserts/dynamic/Post/batch_id=2012-11-13, inserts/dynamic/Post/batch_id=2012-09-20, inserts/dynamic/Post/batch_id=2012-09-29, inserts/dynamic/Post/batch_id=2012-12-05, inserts/dynamic/Post/batch_id=2012-09-16, inserts/dynamic/Post/batch_id=2012-11-...\n"
     ]
    }
   ],
   "source": [
    "inserts_dfs = dict(filter(lambda x: x[0].startswith('inserts'), dfs.items()))\n",
    "print(\", \".join(inserts_dfs)[:500] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f09a317-5f43-4236-89a9-985c6addeaa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deletes/dynamic/Post/batch_id=2012-12-02, deletes/dynamic/Post/batch_id=2012-12-11, deletes/dynamic/Post/batch_id=2012-12-06, deletes/dynamic/Post/batch_id=2012-12-24, deletes/dynamic/Forum_hasMember_Person/batch_id=2012-10-13, deletes/dynamic/Forum_hasMember_Person/batch_id=2012-11-25, deletes/dynamic/Forum_hasMember_Person/batch_id=2012-12-02, deletes/dynamic/Forum_hasMember_Person/batch_id=2012-12-28, deletes/dynamic/Forum_hasMember_Person/batch_id=2012-12-19, deletes/dynamic/Forum_hasMember_...\n"
     ]
    }
   ],
   "source": [
    "deletes_dfs = dict(filter(lambda x: x[0].startswith('deletes'), dfs.items()))\n",
    "print(\", \".join(deletes_dfs)[:500] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c56f7f0e-baa7-4b28-a4be-e59ff2a18495",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "fk_id_re = re.compile(r'(?P<label>[^\\d\\W]+)(?P<number>\\d*)\\.id')\n",
    "edge_type_re = re.compile(r'(?P<start>\\w+)_(?P<type>\\w+)_(?P<end>\\w+)')\n",
    "\n",
    "dtype_to_katana_type = {np.dtype('int64'): 'LONG',\n",
    "                        np.dtype('O'): 'STRING',\n",
    "                        np.dtype('float64'): 'FLOAT',\n",
    "                        pd.DatetimeTZDtype(tz='UTC'): 'DATETIME',\n",
    "                        np.dtype('<M8[ns]'): 'DATE'}\n",
    "default_label = {\n",
    "    'Post' : 'Message',\n",
    "    'Comment' : 'Message',\n",
    "}\n",
    "\n",
    "def get_default_label(c:str):\n",
    "    if label_match := edge_type_re.search(c):\n",
    "        label = label_match.group('type')\n",
    "        return re.sub(r'(?<!^)(?=[A-Z])', '_', label).upper()\n",
    "    if c in default_label:\n",
    "        return default_label[c]\n",
    "    return c\n",
    "\n",
    "def get_type(c:str, dtype, label: str, include_fk=False):\n",
    "    # print('c: ' + c + ', label: ' + label)\n",
    "    if c == 'id':\n",
    "        return c + ':ID(' + label + ')'\n",
    "    elif label_match := edge_type_re.search(label):\n",
    "        # parse edge type\n",
    "        # print(label + ': ' + label_match.group('type'))\n",
    "        if fk_label_match := fk_id_re.search(c):\n",
    "            if not include_fk:\n",
    "                c = \"\"\n",
    "            fk_label = fk_label_match.group('label')\n",
    "            number = fk_label_match.group('number')\n",
    "            start = label_match.group('start')\n",
    "            end = label_match.group('end')\n",
    "            if fk_label == start and fk_label == end:\n",
    "                if number == '1':\n",
    "                    return c + ':START_ID(' + fk_label + ')'\n",
    "                return c + ':END_ID(' + fk_label + ')'\n",
    "            if fk_label == start:\n",
    "                return c + ':START_ID(' + fk_label + ')'\n",
    "            if fk_label == end:\n",
    "                return c + ':END_ID(' + fk_label + ')'\n",
    "    # parse node type\n",
    "    if fk_id_re.search(c):\n",
    "        return c + ':IGNORE'\n",
    "    if c == 'birthday':\n",
    "        return c + ':DATE'\n",
    "    return c + ':' + dtype_to_katana_type[dtype]\n",
    "\n",
    "def get_header(df:dd.DataFrame, label:str):\n",
    "    return [get_type(c, df[c].dtype, label, include_fk=True) for c in df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "047b1da2-e74d-4dba-83da-6d403b2727b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Person\n",
      "start: Person\n",
      "end: Person\n",
      "type: knows\n"
     ]
    }
   ],
   "source": [
    "s = 'Person1.id'\n",
    "m = fk_id_re.search(s)\n",
    "if m:\n",
    "    print(m.group('label'))\n",
    "\n",
    "s = 'Person_knows_Person'\n",
    "m = edge_type_re.search(s)\n",
    "if m:\n",
    "    print('start: ' + m.group('start'))\n",
    "    print('end: ' + m.group('end'))\n",
    "    print('type: ' + m.group('type'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d59af8b-5161-4568-80d0-6eeaa296ec99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_header_files(batches, directory:Path):\n",
    "    if not directory.exists():\n",
    "        directory.mkdir() \n",
    "        \n",
    "    for b in batches:\n",
    "        batch = batches[b]\n",
    "        nodefile = ''\n",
    "        edgefile = ''\n",
    "        for t in batch:\n",
    "            label = Path(t).name\n",
    "            if '_' in label:\n",
    "                # edge type\n",
    "                #print('edge: ' + t)\n",
    "                edgefile += 'KATANA_DEFAULT_LABEL=' + get_default_label(label) + '\\n'\n",
    "                edgefile += '|'.join(get_header(batch[t].df, label)) + '\\n'\n",
    "                edgefile += '\\n'.join(batch[t].files) + '\\n\\n'\n",
    "            else:\n",
    "                # node type\n",
    "                #print('node: ' + t)\n",
    "                nodefile += 'KATANA_DEFAULT_LABEL=' + get_default_label(label) + '\\n'\n",
    "                nodefile += '|'.join(get_header(batch[t].df, label)) + '\\n'\n",
    "                nodefile += '\\n'.join(batch[t].files) + '\\n\\n'\n",
    "        # print('processing batch file: ' + b)\n",
    "        if nodefile:\n",
    "            batch_node_file = open(directory / (b + '-node_headers.txt'), 'w')\n",
    "            batch_node_file.write(nodefile)\n",
    "            batch_node_file.close()\n",
    "        if edgefile:\n",
    "            batch_edge_file = open(directory / (b + '-edge_headers.txt'), 'w')\n",
    "            batch_edge_file.write(edgefile)\n",
    "            batch_edge_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418cd66c-f17f-46ee-b21e-60505172116d",
   "metadata": {},
   "source": [
    "## Generate Initial Snapshot Header file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1efd04b1-e698-4aa8-ab18-380d62adc4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_snapshot_batches = {}\n",
    "batch = 'initial_snapshot'\n",
    "for key in initial_snapshot_dfs.keys():\n",
    "    file = Path(key).name\n",
    "    #print(file)\n",
    "    if batch not in initial_snapshot_batches:\n",
    "        initial_snapshot_batches[batch] = {}\n",
    "    initial_snapshot_batches['initial_snapshot'][str(file)] = initial_snapshot_dfs[key]\n",
    "\n",
    "output_path = headers_path / 'initial_snapshot'\n",
    "output_path.mkdir(parents = True, exist_ok = True)\n",
    "generate_header_files(initial_snapshot_batches, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89d32b8-d0b9-4163-a8a3-d164ce114bf7",
   "metadata": {},
   "source": [
    "## Generate Inserts Header file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5052e659-5785-4f57-bf12-cba91c338488",
   "metadata": {},
   "outputs": [],
   "source": [
    "insert_batches = {}\n",
    "for key in inserts_dfs.keys():\n",
    "    file = Path(key).parent\n",
    "    batch = Path(key).name\n",
    "    if batch not in insert_batches:\n",
    "        insert_batches[batch] = {}\n",
    "    insert_batches[batch][str(file)] = inserts_dfs[key]\n",
    "\n",
    "output_path = headers_path / 'inserts'\n",
    "output_path.mkdir(parents = True, exist_ok = True)\n",
    "generate_header_files(insert_batches, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5fb1ab-478c-4d90-b067-599c7ebb12a7",
   "metadata": {},
   "source": [
    "## Generate Deletes Header file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "91c03f5b-3405-4eb9-ad95-aa1b985707fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_batches = {}\n",
    "for key in deletes_dfs.keys():\n",
    "    file = Path(key).parent\n",
    "    batch = Path(key).name\n",
    "    if batch not in delete_batches:\n",
    "        delete_batches[batch] = {}\n",
    "    delete_batches[batch][str(file)] = deletes_dfs[key]\n",
    "\n",
    "output_path = headers_path / 'deletes'\n",
    "output_path.mkdir(parents = True, exist_ok = True)\n",
    "generate_header_files(delete_batches, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377393d8-ec1a-4c4d-ac9c-cd38c8fbf324",
   "metadata": {},
   "source": [
    "## Collect and write out combined insert and delete batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "365071df-d7e5-4a51-86ce-23b6f5f1d093",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "batches_list = list(set(insert_batches.keys()) | set(delete_batches.keys()))\n",
    "batches_list.sort(key = lambda date: datetime.strptime(date, 'batch_id=%Y-%m-%d'))\n",
    "\n",
    "output = []\n",
    "\n",
    "for b in batches_list:\n",
    "    insert_edge_path = headers_path / 'inserts' / (b + '-edge_headers.txt')\n",
    "    insert_node_path = headers_path / 'inserts' / (b + '-node_headers.txt')\n",
    "    delete_edge_path = headers_path / 'deletes' / (b + '-edge_headers.txt')\n",
    "    delete_node_path = headers_path / 'deletes' / (b + '-node_headers.txt')\n",
    "    batch = {}\n",
    "    if insert_edge_path.exists():\n",
    "        batch['insert_edge_list'] = str(insert_edge_path.relative_to(dataset_path))\n",
    "    if insert_node_path.exists():\n",
    "        batch['insert_node_list'] = str(insert_node_path.relative_to(dataset_path))\n",
    "    if delete_edge_path.exists():\n",
    "        batch['delete_edge_list'] = str(delete_edge_path.relative_to(dataset_path))\n",
    "    if delete_node_path.exists():\n",
    "        batch['delete_node_list'] = str(delete_node_path.relative_to(dataset_path))\n",
    "    output.append(batch)\n",
    "batches_file_name = 'batches.json'\n",
    "batches_file = open(headers_path / batches_file_name, 'w')\n",
    "batches_file.write(json.dumps(output, indent=1))\n",
    "batches_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
