{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ab7ce90-67cf-462c-94a6-3c8c9d0754b3",
   "metadata": {},
   "source": [
    "# Setup: Connectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e87b75ba-c072-4111-8cca-4b4f394a0fce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<katana_enterprise.remote.sync_wrappers.Client object at 0x7f1436107e20>\n",
      "<_Graph my_graph, 9XsFVxEtTWkJCWAKvVVGUuAzf2J6xyG22QMsWB1WYB9a, 4>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from katana import remote\n",
    "   #\n",
    "from libraries.SupportFor34 import f_init\n",
    "\n",
    "    \n",
    "my_client, my_graph = f_init()\n",
    "\n",
    "print(my_client)\n",
    "print(my_graph)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71f4148-8670-4f8b-884b-e1d2d790eadb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4ec1c889-3b18-44f7-adcf-b9b5eca13dd5",
   "metadata": {},
   "source": [
    "#  Step 02: Enrich the above from a Google Web service .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6491d824-e497-4548-8ace-659c3657662a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Google has a Web service to convert text into usable UMLS codes. See,\n",
    "#        https://cloud.google.com/healthcare-api/docs/how-tos/nlp\n",
    "#\n",
    "#  In this cell, we begin to invoke this service on the text from the\n",
    "#  cell above.\n",
    "\n",
    "#  See also,\n",
    "#     https://stackoverflow.com/questions/53472429/how-to-get-a-gcp-bearer-token-programmatically-with-python\n",
    "\n",
    "#  Google:\n",
    "#\n",
    "#     .  We had to create an Auth Token, which produced a JSON file.\n",
    "#        (Instruction in Url above.)\n",
    "#\n",
    "#     .  Our JSON file is at,\n",
    "#              export GOOGLE_APPLICATION_CREDENTIALS=\"/mnt/hgfs/My.20/MyShare_1/46 Topics 2022/91 KG, All Prospects/13 KG, DataBricks, Google/10_Data/05_katana-clusters-beta-d8605ac248e7.json\"\n",
    "#              export GOOGLE_APPLICATION_CREDENTIALS=\"/home/jovyan/work/My_KG_NoteBooks/P1_Prospects/13_DataBricks_Google/10_Data/05_katana-clusters-beta-d8605ac248e7.json\"\n",
    "#\n",
    "#     .  To extract the Auth Token, set the above, then run\n",
    "#           gcloud auth application-default print-access-token\n",
    "\n",
    "\n",
    "import google.auth\n",
    "import google.auth.transport.requests\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "\n",
    "#  This token times out often; you must rerun this block from time to time\n",
    "#\n",
    "def f_get_token():\n",
    "   l_credentials = service_account.Credentials.from_service_account_file(\n",
    "      \"/home/jovyan/work/My_KG_NoteBooks/P1_Prospects/13_DataBricks_Google/10_Data/05_katana-clusters-beta-d8605ac248e7.json\",\n",
    "      scopes=['https://www.googleapis.com/auth/cloud-platform'])\n",
    "   l_auth_req = google.auth.transport.requests.Request()\n",
    "   l_credentials.refresh(l_auth_req)\n",
    "      #\n",
    "   return l_credentials.token\n",
    "     \n",
    "    \n",
    "l_token = f_get_token()\n",
    "\n",
    "    \n",
    "print(\"\")\n",
    "print(\"Token: \" + l_token[0:120] + \" ...\")\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "#  Sample output,\n",
    "#\n",
    "#     Token: ya29.c.b0Aa9VdylvtWgGXBZyFkW5mADfcFiyBVitZsvkoKbHpCuXU7zGkgANRcho_ax5_SWWbiXfQj6cprlobWUlHnPkYEoKBCRw6   ...\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c91100-2973-4f3f-9f3e-a28b53a1da20",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Sample data from above,\n",
    "#\n",
    "#     200\n",
    "#     \n",
    "#     {\n",
    "#        \"entityMentions\": [\n",
    "#           {\n",
    "#              \"mentionId\": \"1\",\n",
    "#              \"type\": \"MEDICINE\",\n",
    "#              \"text\": {\n",
    "#                 \"content\": \"Insulin regimen\",\n",
    "#                 \"beginOffset\": 0\n",
    "#              },\n",
    "#              \"linkedEntities\": [\n",
    "#                 {\n",
    "#                    \"entityId\": \"UMLS/C0021641\"\n",
    "#                 },\n",
    "#                 {\n",
    "#                    \"entityId\": \"UMLS/C0795635\"\n",
    "#                 },\n",
    "#                 {\n",
    "#                    \"entityId\": \"UMLS/C1533581\"\n",
    "#                 },\n",
    "#                 {\n",
    "#                    \"entityId\": \"UMLS/C3537244\"\n",
    "#                 },\n",
    "#                 {\n",
    "#                    \"entityId\": \"UMLS/C3714501\"\n",
    "#                 }\n",
    "#              ],\n",
    "#              \"temporalAssessment\": {\n",
    "#                 \"value\": \"CURRENT\",\n",
    "#                 \"confidence\": 0.8573660850524902\n",
    "#              },\n",
    "#              \"certaintyAssessment\": {\n",
    "#                 \"value\": \"LIKELY\",\n",
    "#                 \"confidence\": 0.9751282930374146\n",
    "#              },\n",
    "#              \"subject\": {\n",
    "#                 \"value\": \"PATIENT\",\n",
    "#                 \"confidence\": 0.9995787739753723\n",
    "#              },\n",
    "#              \"confidence\": 0.6379408836364746\n",
    "#           },\n",
    "#           {\n",
    "#              \"mentionId\": \"2\",\n",
    "#              \"type\": \"MED_DOSE\",\n",
    "#              \"text\": {\n",
    "#                 \"content\": \"5 units\",\n",
    "#                 \"beginOffset\": 22\n",
    "#              },\n",
    "#              \"confidence\": 0.7443782091140747\n",
    "#           },\n",
    "#           {\n",
    "#              \"mentionId\": \"3\",\n",
    "#              \"type\": \"MED_ROUTE\",\n",
    "#              \"text\": {\n",
    "#                 \"content\": \"IV\",\n",
    "#                 \"beginOffset\": 30\n",
    "#              },\n",
    "#              \"linkedEntities\": [\n",
    "#                 {\n",
    "#                    \"entityId\": \"UMLS/C0348016\"\n",
    "#                 }\n",
    "#              ],\n",
    "#              \"confidence\": 0.779011607170105\n",
    "#           }\n",
    "#        ],\n",
    "#        \"entities\": [\n",
    "#           {\n",
    "#              \"entityId\": \"UMLS/C0021641\",\n",
    "#              \"preferredTerm\": \"Insulin\",\n",
    "#              \"vocabularyCodes\": [\n",
    "#                 \"FMA/83365\",\n",
    "#                 \"LNC/LA15805-7\",\n",
    "#                 \"LNC/LP14676-8\",\n",
    "#                 \"LNC/LP16325-0\",\n",
    "#                 \"LNC/LP32542-0\",\n",
    "#                 \"LNC/LP70329-5\",\n",
    "#                 \"LNC/MTHU002108\",\n",
    "#                 \"LNC/MTHU019392\",\n",
    "#                 \"MSH/D007328\",\n",
    "#                 \"MTH/NOCODE\"\n",
    "#              ]\n",
    "#           },\n",
    "#           {\n",
    "#              \"entityId\": \"UMLS/C0348016\",\n",
    "#              \"preferredTerm\": \"Intravenous\",\n",
    "#              \"vocabularyCodes\": [\n",
    "#                 \"LNC/LA9437-0\",\n",
    "#                 \"LNC/LP32453-0\",\n",
    "#                 \"MTH/NOCODE\",\n",
    "#                 \"NCI/C13346\"\n",
    "#              ]\n",
    "#           },\n",
    "#           {\n",
    "#              \"entityId\": \"UMLS/C0795635\",\n",
    "#              \"preferredTerm\": \"insulin, regular, human\",\n",
    "#              \"vocabularyCodes\": [\n",
    "#                 \"LNC/LP17001-6\",\n",
    "#                 \"MSH/D061386\",\n",
    "#                 \"MTH/NOCODE\",\n",
    "#                 \"NCI/C29125\",\n",
    "#                 \"RXNORM/253182\",\n",
    "#                 \"VANDF/4017559\",\n",
    "#                 \"VANDF/4017569\",\n",
    "#                 \"VANDF/4019786\"\n",
    "#              ]\n",
    "#           },\n",
    "#           {\n",
    "#              \"entityId\": \"UMLS/C1533581\",\n",
    "#              \"preferredTerm\": \"Therapeutic Insulin\",\n",
    "#              \"vocabularyCodes\": [\n",
    "#                 \"MTH/NOCODE\",\n",
    "#                 \"NCI/C581\"\n",
    "#              ]\n",
    "#           },\n",
    "#           {\n",
    "#              \"entityId\": \"UMLS/C3537244\",\n",
    "#              \"preferredTerm\": \"Insulins\",\n",
    "#              \"vocabularyCodes\": [\n",
    "#                 \"MSH/D061385\",\n",
    "#                 \"MTH/NOCODE\"\n",
    "#              ]\n",
    "#           },\n",
    "#           {\n",
    "#              \"entityId\": \"UMLS/C3714501\",\n",
    "#              \"preferredTerm\": \"Insulin Drug Class\",\n",
    "#              \"vocabularyCodes\": [\n",
    "#                 \"MTH/NOCODE\",\n",
    "#                 \"VANDF/4021631\"\n",
    "#              ]\n",
    "#           }\n",
    "#        ],\n",
    "#        \"relationships\": [\n",
    "#           {\n",
    "#              \"subjectId\": \"1\",\n",
    "#              \"objectId\": \"2\",\n",
    "#              \"confidence\": 0.9996469616889954\n",
    "#           },\n",
    "#           {\n",
    "#              \"subjectId\": \"1\",\n",
    "#              \"objectId\": \"3\",\n",
    "#              \"confidence\": 0.9995671510696411\n",
    "#           }\n",
    "#        ]\n",
    "#     }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07bdf14-0a1b-48db-a0f8-9d9604c16a31",
   "metadata": {},
   "source": [
    "#  Step 02:  (continued)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11d557b-990a-4e72-bcb1-3cbcfdcbf6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#\n",
    "#  **  This cell takes minutes to complete ..\n",
    "#\n",
    "\n",
    "#  Run the above Web service against our first DataFrame\n",
    "#\n",
    "#  Recall df_PatientVisitNodes contains,\n",
    "#\n",
    "#     id|description|sample_name|transcription\n",
    "#\n",
    "\n",
    "df_enriched = pd.DataFrame(columns = [\"id\", \"enriched_from_Google\"])\n",
    "   #\n",
    "print(\"\")\n",
    "print(\"Enrich %d Patient Visit rows ..\" % (len(df_PatientVisitNodes)))\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "l_token   = f_get_token()\n",
    "l_cntr    = 0\n",
    "   #\n",
    "#   for l_each in df_PatientVisitNodes.head(10).itertuples():\n",
    "for l_each in df_PatientVisitNodes.itertuples():\n",
    "\n",
    "   l_cntr += 1\n",
    "      #\n",
    "   if (l_cntr % 1000 == 0):\n",
    "      print(\"\")\n",
    "      print(\"Processed so far: %d\" % (l_cntr))\n",
    "      #\n",
    "      #  Because this times out often ..\n",
    "      #\n",
    "      l_token   = f_get_token()\n",
    "   else:\n",
    "      print(\".\", end = \"\")\n",
    "\n",
    "   try:\n",
    "      l_response    = f_enrich(l_each.transcription)\n",
    "      l_data_asjson = json.loads(l_response.content) \n",
    "         #\n",
    "      df_enriched = df_enriched.append({\"id\" : l_each.id,\n",
    "         \"enriched_from_Google\" : l_data_asjson}, ignore_index = True)\n",
    "   except:\n",
    "      pass\n",
    "    \n",
    "\n",
    "\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "   #\n",
    "for l_each in df_enriched.head(5).itertuples():\n",
    "   print(\"Record number: %s   Short text: %s\" % (l_each.id,\n",
    "      str(l_each.enriched_from_Google)[0:80]))\n",
    "        \n",
    "print(\"\")\n",
    "\n",
    "\n",
    "#  Sample output,\n",
    "#\n",
    "#     Enrich 4999 Patient Visit rows ..\n",
    "#     \n",
    "#     ..................................................................................................................................................................................\n",
    "#     Processed so far: 1000\n",
    "#     ........................................................................................................................................................................\n",
    "#     Processed so far: 2000\n",
    "#     ..............................................................................................................................................................................\n",
    "#     Processed so far: 3000\n",
    "#     .................................................................................................................................................................................\n",
    "#     Processed so far: 4000\n",
    "#     ...........................................................................................................................................................................\n",
    "#     \n",
    "#     Record number: PV-4000   Short text: {'entityMentions': [{'mentionId': '1', 'type': 'PROBLEM', 'text': {'content': 'a\n",
    "#     Record number: PV-4001   Short text: {'entityMentions': [{'mentionId': '1', 'type': 'PROBLEM', 'text': {'content': 'D\n",
    "#     Record number: PV-4002   Short text: {'entityMentions': [{'mentionId': '1', 'type': 'BODY_MEASUREMENT', 'text': {'con\n",
    "#     Record number: PV-4003   Short text: {'entityMentions': [{'mentionId': '1', 'type': 'PROBLEM', 'text': {'content': 'a\n",
    "#     Record number: PV-4004   Short text: {'entityMentions': [{'mentionId': '1', 'type': 'ANATOMICAL_STRUCTURE', 'text': {\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4ad011-1bf5-4242-b93a-7480745e6cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Write this out to a file\n",
    "#\n",
    "#  We use GitHub which has a 25MB file size limit; 400-500 (NUM_ROWS) lines fits that approximate size\n",
    "#\n",
    "\n",
    "NUM_ROWS = 400\n",
    "   #\n",
    "l_cntr1  = 0\n",
    "l_cntr2  = 0\n",
    "   #\n",
    "while (l_cntr2 < len(df_enriched)):\n",
    "   l_cntr1 += 1\n",
    "      #\n",
    "   l_file = \"./10_Data/34_33_PlusGoogle.\" + str(l_cntr1).zfill(4) + \".txt\"\n",
    "      #\n",
    "   df_enriched.loc[l_cntr2:l_cntr2 + NUM_ROWS - 1].to_csv(l_file, sep = \"|\", encoding = \"utf-8\", index = False)\n",
    "      #\n",
    "   l_cntr2 += NUM_ROWS\n",
    "\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824e4efe-569f-4b16-82aa-a4ada5bfb099",
   "metadata": {},
   "source": [
    "# Step 03:  Extract actual field data from the JSON Google gave us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56197a1c-7c76-44f6-a9fc-d20a3a896ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#  Since the enrichment from Google takes a long time, let's not run that code over and over.\n",
    "#\n",
    "#  Load/re-load the resultant DataFrame from CSV.\n",
    "#\n",
    "\n",
    "import glob, os\n",
    "\n",
    "\n",
    "l_folder   = \"./10_Data\"\n",
    "l_files    = glob.glob(os.path.join(l_folder, \"34_33_PlusGoogle.*.txt\"))\n",
    "   #\n",
    "l_enriched = []\n",
    "\n",
    "\n",
    "for l_each in l_files:\n",
    "   l_dataframe = pd.read_csv(l_each, header = \"infer\", delimiter = \"|\")\n",
    "   l_enriched.append(l_dataframe)\n",
    "    \n",
    "    \n",
    "df_enriched = pd.concat(l_enriched, ignore_index = True)\n",
    "\n",
    "\n",
    "print(\"Number of rows: %d\" % (len(df_enriched)))\n",
    "   #\n",
    "for l_each in df_enriched.head(5).itertuples():\n",
    "   print(\"Id: %s   Text: %s\" % (l_each.id, l_each.enriched_from_Google[0:180]))\n",
    "\n",
    "print(\"--\")\n",
    "\n",
    "#  Sample output\n",
    "#\n",
    "#     Number of rows: 4905\n",
    "#     Id: PV-4000   Text: {'entityMentions': [{'mentionId': '1', 'type': 'PROBLEM', 'text': {'content': 'allergies', 'beginOffset': 71}, 'linkedEntities': [{'entityId': 'UMLS/C0020517'}, {'entityId': 'UMLS/\n",
    "#     Id: PV-4001   Text: {'entityMentions': [{'mentionId': '1', 'type': 'PROBLEM', 'text': {'content': 'Difficulty with snoring', 'beginOffset': 285}, 'linkedEntities': [{'entityId': 'UMLS/C0037384'}], 'te\n",
    "#     Id: PV-4002   Text: {'entityMentions': [{'mentionId': '1', 'type': 'BODY_MEASUREMENT', 'text': {'content': 'BMI', 'beginOffset': 142}, 'linkedEntities': [{'entityId': 'UMLS/C0578022'}, {'entityId': 'U\n",
    "#     Id: PV-4003   Text: {'entityMentions': [{'mentionId': '1', 'type': 'PROBLEM', 'text': {'content': 'atrial enlargement', 'beginOffset': 24}, 'linkedEntities': [{'entityId': 'UMLS/C0741276'}], 'temporal\n",
    "#     Id: PV-4004   Text: {'entityMentions': [{'mentionId': '1', 'type': 'ANATOMICAL_STRUCTURE', 'text': {'content': 'left ventricular cavity', 'beginOffset': 8}, 'linkedEntities': [{'entityId': 'UMLS/C0503\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad88ae4-fa71-4dcb-bc50-f8f663ecc15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#  The model for the df_enriched DataFrame is,\n",
    "#     id|enriched_from_Google\n",
    "#\n",
    "#     with the second column being a JSON encoded string with its own model.\n",
    "#\n",
    "#  Here we loop thru said DataFrme, and make our resultant Nodes and Edges\n",
    "#  for our graph.\n",
    "#\n",
    "\n",
    "import ast                                                  #  Best way to read in dictionaries, forget json.loads()\n",
    "\n",
    "\n",
    "l_NumOfBadJson                = 0\n",
    "l_NumOfRootKeyNotFound        = 0\n",
    "   #\n",
    "l_UmlsEntityNodes             = []\n",
    "l_UmlsVocabularyNodes         = []\n",
    "   #\n",
    "l_PatientVisitToEntityEdge_N  = []\n",
    "l_PatientVisitToEntityEdge_S  = []\n",
    "l_EntityToVocabularyEdge_N    = []\n",
    "l_EntityToVocabularyEdge_S    = []\n",
    "\n",
    "\n",
    "l_cntr    = 0\n",
    "   #\n",
    "for l_each in df_enriched.itertuples():\n",
    "    \n",
    "   #  Because we read from file, this value came is as a string when we \n",
    "   #  need it as a dictionary\n",
    "   #\n",
    "   l_each_asdict = ast.literal_eval(l_each.enriched_from_Google)\n",
    "    \n",
    "   #  Before we cleaned up all of the JSON (random single quotes for possesive nouns,\n",
    "   #  other), we would get errors similar to,\n",
    "   #\n",
    "   #      4001|\"{'error': {'code': 400, 'message': \"\"Invalid JSON payload received. Expected , or } after key:value pair.\\naced\n",
    "   #         on the patient's skin or on the imm\\n                    ^\"\", 'status': 'INVALID_ARGUMENT'}}\"\n",
    "   #\n",
    "   #  Upon receipt of this condition, a root level key with the title \"error\" would be\n",
    "   #  present. If those return, filter those out-\n",
    "   #\n",
    "   if (\"error\" in l_each_asdict):\n",
    "      l_NumOfBadJson += 1\n",
    "    \n",
    "   else:\n",
    "      #\n",
    "      #  \"entities\" should be a root level key to this dictionary\n",
    "      #\n",
    "      if (\"entities\" in l_each_asdict):\n",
    "         #\n",
    "         #  Loop thru these\n",
    "         #\n",
    "         for l_entity in l_each_asdict[\"entities\"]:\n",
    "            \n",
    "            l_cntr += 1\n",
    "               #\n",
    "            if (l_cntr % 100000 == 0):\n",
    "               print(\"\")\n",
    "               print(\"Processed so far: %d\" % (l_cntr))\n",
    "            else:\n",
    "               if (l_cntr % 1000 == 0):\n",
    "                  print(\".\", end = \"\")\n",
    "            \n",
    "            if (\"entityId\" in l_entity):\n",
    "               #\n",
    "               #  Build a dictionary that we will append to an array\n",
    "               #\n",
    "               l_recd1 = { \"id\": l_entity[\"entityId\"], \"entity_id\" : l_entity[\"entityId\"], \"LABEL\": \"UmlsEntity\" }\n",
    "               #\n",
    "               #  If this key is present, add it to the dictionary\n",
    "               #\n",
    "               if (\"preferredTerm\" in l_entity):\n",
    "                  #\n",
    "                  #  We have an additional key, add to the record and add to our array\n",
    "                  #\n",
    "                  l_recd1.update( {\"preferred_term\": l_entity[\"preferredTerm\"]} )\n",
    "                     #\n",
    "               l_UmlsEntityNodes.append(l_recd1)\n",
    "               #\n",
    "               #  Above was our list of Nodes of LABEL \"UmlsEntity\"\n",
    "               #  \n",
    "               #  Here we make our Edge list from;  PatientVisit --> UmlsEntity\n",
    "               #\n",
    "               #  We make all Edges to be bi-directional. As a heterogeneous relationship,\n",
    "               #  we need two arrays.\n",
    "               #\n",
    "               l_recd2a = { \"start_id\": str(l_each.id)           , \"end_id\":   str(l_entity[\"entityId\"]), \"TYPE\": \"VISIT_CONTAINS\" }\n",
    "               l_recd2b = { \"start_id\": str(l_entity[\"entityId\"]), \"end_id\":   str(l_each.id)           , \"TYPE\": \"VISIT_CONTAINS\" }\n",
    "                  #\n",
    "               l_PatientVisitToEntityEdge_N.append(l_recd2a)\n",
    "               l_PatientVisitToEntityEdge_S.append(l_recd2b)\n",
    "               #\n",
    "               #  We are done with UmlsEntity and its Edge to PatientVisit\n",
    "               #\n",
    "               #  Also in \"entities\" is another array, \"vocabularyCodes\"\n",
    "               #\n",
    "               if (\"vocabularyCodes\" in l_entity):\n",
    "                  for l_vocab in l_entity[\"vocabularyCodes\"]:\n",
    "                     #\n",
    "                     #  Add to our set of Vocabulary Nodes\n",
    "                     #\n",
    "                     l_recd3 = { \"id\": l_vocab, \"vocabularyCode\": l_vocab, \"LABEL\": \"UmlsVocabulary\" }\n",
    "                        #\n",
    "                     l_UmlsVocabularyNodes.append(l_recd3)\n",
    "                     #\n",
    "                     #  And create the Edge from UmlsEntity --> UmlsVocabulary\n",
    "                     #\n",
    "                     l_recd4a = { \"start_id\": str(l_entity[\"entityId\"]), \"end_id\": str(l_vocab             ), \"TYPE\": \"ALSO_CODED_AS\" }\n",
    "                     l_recd4b = { \"start_id\": str(l_vocab             ), \"end_id\": str(l_entity[\"entityId\"]), \"TYPE\": \"ALSO_CODED_AS\" }\n",
    "                        #\n",
    "                     l_EntityToVocabularyEdge_N.append(l_recd4a)\n",
    "                     l_EntityToVocabularyEdge_S.append(l_recd4b)\n",
    "            else:\n",
    "               #\n",
    "               #  No \"entityId\" in our record. This has never happened.\n",
    "               #  We wont report, just pass.\n",
    "               #\n",
    "               pass\n",
    "            \n",
    "      else:\n",
    "         l_NumOfRootKeyNotFound += 1\n",
    "            \n",
    "            \n",
    "   ###\n",
    "\n",
    "\n",
    "print(\"\")\n",
    "print(\"Number of 'Error' input records: %d   Number of 'No Root Key' input records: %d\" % ( l_NumOfBadJson, l_NumOfRootKeyNotFound) )\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "#   l_UmlsEntityNodes, l_UmlsVocabularyNodes, l_PatientVisitToEntityEdge, and l_EntityToVocabularyEdge\n",
    "#     are currently arrays, and have duplicate records.\n",
    "#\n",
    "#  Arguably; these should have been sets(), [ then ] made into arrays or DataFrames.\n",
    "#     (Memory versus CPU. And .. .. two processing loops versus one.)\n",
    "#\n",
    "#  Convert these to DataFrames and remove duplicates\n",
    "#\n",
    " \n",
    "df_UmlsEntityNodes             = pd.DataFrame.from_records(l_UmlsEntityNodes           ).drop_duplicates()\n",
    "df_UmlsVocabularyNodes         = pd.DataFrame.from_records(l_UmlsVocabularyNodes       ).drop_duplicates()\n",
    "   #\n",
    "df_PatientVisitToEntityEdge_N  = pd.DataFrame.from_records(l_PatientVisitToEntityEdge_N).drop_duplicates()\n",
    "df_PatientVisitToEntityEdge_S  = pd.DataFrame.from_records(l_PatientVisitToEntityEdge_S).drop_duplicates()\n",
    "df_EntityToVocabularyEdge_N    = pd.DataFrame.from_records(l_EntityToVocabularyEdge_N  ).drop_duplicates()\n",
    "df_EntityToVocabularyEdge_S    = pd.DataFrame.from_records(l_EntityToVocabularyEdge_S  ).drop_duplicates()\n",
    "\n",
    "\n",
    "print(\"Number of PatientVisit nodes: %d   UmlsEntity nodes: %d   UmlsVocabulary nodes: %d\" % ( len(df_PatientVisitNodes), len(df_UmlsEntityNodes), len(df_UmlsVocabularyNodes) ))\n",
    "print(\"\")\n",
    "print(\"Edges PatientVisit --> UmlsEntity: %d   %s   UmlsEntity --> UmlsVocabulary: %d   %s\" % (\n",
    "   len(df_PatientVisitToEntityEdge_N), len(df_PatientVisitToEntityEdge_S),\n",
    "   len(df_EntityToVocabularyEdge_N  ), len(df_EntityToVocabularyEdge_S  ) ))\n",
    "print(\"\")\n",
    "\n",
    "print(\"--\")    \n",
    "\n",
    "\n",
    "#  Sample output,\n",
    "#\n",
    "\n",
    "#  The original 90 documents yielded,\n",
    "#\n",
    "#     Number of 'Error' input records: 0   Number of 'No Root Key' input records: 0\n",
    "#     Number of PatientVisit nodes: 90   UmlsEntity nodes: 3115   UmlsVocabulary nodes: 8860\n",
    "#     Edges PatientVisit --> UmlsEntity: 8479   8479   UmlsEntity --> UmlsVocabulary: 10976   10976\n",
    "\n",
    "#  The current 5000 documents yielded,\n",
    "#\n",
    "#     ...................................................................................................\n",
    "#     Processed so far: 100000\n",
    "#     ...................................................................................................\n",
    "#     Processed so far: 200000\n",
    "#          ...\n",
    "#     Processed so far: 400000\n",
    "#     ..............................................................\n",
    "#     \n",
    "#     Number of 'Error' input records: 0   Number of 'No Root Key' input records: 54\n",
    "#     \n",
    "#     Number of PatientVisit nodes: 4999   UmlsEntity nodes: 20073   UmlsVocabulary nodes: 40148\n",
    "#     \n",
    "#     Edges PatientVisit --> UmlsEntity: 462876   462876   UmlsEntity --> UmlsVocabulary: 51875   51875\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd3cde2-9a0c-42f0-a9cd-4a5a6235733b",
   "metadata": {},
   "source": [
    "#  Step 00:  Save Our Work .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7024c4dd-335e-4d04-8c6b-3ba2634f35f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  We've created several sets of Nodes, and some bi-directional edges.\n",
    "#  Save these to file.\n",
    "#\n",
    "\n",
    "\n",
    "#  Umls Entity Nodes\n",
    "#\n",
    "l_file = \"./10_Data/N_04_00_UmlsEntity.txt\"\n",
    "   #\n",
    "df_UmlsEntityNodes.to_csv(l_file, sep = \"|\", encoding = \"utf-8\", index = False)\n",
    "\n",
    "\n",
    "#  Umls Vocabulary Nodes\n",
    "#\n",
    "l_file = \"./10_Data/N_05_00_UmlsVocabulary.txt\"\n",
    "   #\n",
    "df_UmlsVocabularyNodes.to_csv(l_file, sep = \"|\", encoding = \"utf-8\", index = False)\n",
    "\n",
    "\n",
    "   ###\n",
    "    \n",
    "\n",
    "#  Our Edges\n",
    "#\n",
    "l_file = \"./10_Data/E_03_00_N_PatientVistToEntity.txt\"\n",
    "   #\n",
    "df_PatientVisitToEntityEdge_N.to_csv(l_file, sep = \"|\", encoding = \"utf-8\", index = False)\n",
    "   #\n",
    "l_file = \"./10_Data/E_03_00_S_PatientVistToEntity.txt\"\n",
    "   #\n",
    "df_PatientVisitToEntityEdge_S.to_csv(l_file, sep = \"|\", encoding = \"utf-8\", index = False)\n",
    "\n",
    "\n",
    "l_file = \"./10_Data/E_04_00_N_PatientVistToVocabulary.txt\"\n",
    "   #\n",
    "df_EntityToVocabularyEdge_N.to_csv(l_file, sep = \"|\", encoding = \"utf-8\", index = False)\n",
    "   #\n",
    "l_file = \"./10_Data/E_04_00_S_PatientVistToVocabulary.txt\"\n",
    "   #\n",
    "df_EntityToVocabularyEdge_S.to_csv(l_file, sep = \"|\", encoding = \"utf-8\", index = False)\n",
    "\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d033566d-939b-42d4-a930-565894e6c293",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69cd6f3-c52c-434f-a74a-ee489796d2aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
