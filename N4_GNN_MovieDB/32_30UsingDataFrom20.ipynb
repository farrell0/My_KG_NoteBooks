{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a947ffc8-26ff-4ee7-bd4a-7a2cbc4d096d",
   "metadata": {},
   "source": [
    "# Part 00: Notebook overview .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072792a8-3286-43e6-a8c6-e20383d23232",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  This Notebook is part of a set that demonstrate GNN using a movie dataset.\n",
    "#  About this Notebook,\n",
    "#\n",
    "#  .  In NoteBook 20*, we built a Movie graph; movies, keywords\n",
    "#     In NoteBook 30* we delivered a primer on CountVectorizers, and Classifiers using non-graph\n",
    "#\n",
    "#  .  Here we'll apply the non-graph Classifier to our Movie nodes from our graph.\n",
    "#\n",
    "#     We'll apply it to just Movie.title, then Movie.tagline, then Movie.overview.\n",
    "#     And then we'll apply it to all 3 at one time.\n",
    "#\n",
    "#     And we'll compare the results to known Movie.genres_primary.\n",
    "#\n",
    "#        Recall Movies actually had an array of genres, and we derived genres_primary\n",
    "#        as the genres in the first position inside the array.\n",
    "#\n",
    "#  .  Our graph, our source of data, was prepared/loaded in NoteBook 20*.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487303ea-82e4-4c07-ae54-047a62595a13",
   "metadata": {},
   "source": [
    "#  Part 01: Graph setup, and initial read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd57fe7-db01-49e8-8bff-9719a2d00f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  The KatanaGraph remote API is expected to run from a node external to\n",
    "#  the Katana Graph cluster itself.\n",
    "#\n",
    "#  This differs from the distributed API, which is meant to run primitives\n",
    "#  on the Katana Graph worker nodes.\n",
    "#\n",
    "\n",
    "from katana import remote\n",
    "from katana.remote import import_data\n",
    "\n",
    "my_client = remote.Client()\n",
    "\n",
    "print(my_client)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0493b77a-8b25-405f-95f3-f003a5a2ab78",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "NUM_PARTITIONS  = 3\n",
    "   #\n",
    "DB_NAME         = \"my_db\"\n",
    "GRAPH_NAME      = \"my_graph1\" \n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bc42ab-d539-4374-bb04-fef7de5d4325",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "my_graph, *_ = my_client.get_database(name=DB_NAME).find_graphs_by_name(GRAPH_NAME)\n",
    "\n",
    "print(my_graph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5c1346d7-40cb-46cf-a4a9-e39b9b693fff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49f825549fd0412ba9e0dc5267ce99ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "          0/? [?op/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Number of Graph Nodes: 64857'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "502094b7aaf44407af9a3da032d9cd1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "          0/? [?op/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Number of Graph Edges: 330988'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "#  Sanity check \n",
    "#\n",
    "display(\"Number of Graph Nodes: %d\" % (my_graph.num_nodes()))\n",
    "display(\"Number of Graph Edges: %s\" % (my_graph.num_edges()))\n",
    "\n",
    "\n",
    "#  Sample output,\n",
    "#\n",
    "#     'Number of Graph Nodes: 64857'\n",
    "#     'Number of Graph Edges: 330988'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b4519f9d-53dc-4ac0-908b-a317fb77aff7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a77674d455ea469fad7902e1167de281",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "          0/? [?op/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12abe02aae474123ad8b5df795a1ebbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "          0/? [?op/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19a71711941041889a6d4691eac484a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='\\n                <style>\\n                #jp-main-content-panel .widget-containerâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "#  Looking at the graph\n",
    "#\n",
    "l_result = my_graph.query(\"\"\"\n",
    "\n",
    "   MATCH (n) -[r]-> (m)\n",
    "   RETURN n, r, m\n",
    "   LIMIT 1000                        //  Limit is 25,000 for visualization, smaller is better\n",
    "   \n",
    "   \"\"\",\n",
    "   contextualize=True)\n",
    "\n",
    "l_result.view()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8888e38-f1e3-4ef0-b96e-6a4b6480990a",
   "metadata": {},
   "source": [
    "\n",
    "<div> \n",
    "<img src=\"./01_Images/10-Movie-Query-1.png\" alt=\"Drawing\" style=\"width: 1600px;\"/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd60626-5ee8-416a-ab7b-94e4cea7cbea",
   "metadata": {},
   "source": [
    "#  Part 02: Getting data for use by our non-graph classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7145573-17f8-4773-9fcb-403c4f7c043e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  We're using a non-graph set of libraries from sklearn. As such, we need to pull\n",
    "#  the data out of the graph into DataFrames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0416c10d-a1cb-433d-ad5e-85498aaa1744",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  We'll consider these packages to be common knowledge. Else, return to the Compulsaries\n",
    "#  set of NoteBooks for sample use, introduction ..\n",
    "#\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "   #\n",
    "import dask.array as da\n",
    "import dask.dataframe as dd\n",
    "   #\n",
    "from dask.dataframe import from_pandas\n",
    "\n",
    "from tabulate import tabulate\n",
    "\n",
    "\n",
    "   ###\n",
    "    \n",
    "    \n",
    "from sklearn.feature_extraction.text import CountVectorizer            #  Build a sparse vector (matrix) of keywords\n",
    "from sklearn.feature_extraction.text import TfidfTransformer           #  Tool to norrmalize a condition; words that occur frequently, versus giving more weight to infrequent words\n",
    "   #                                                                   #        Ie., Having 6 toes is way more rare than having 5. Should I give more weight that a person is 5'10\", or has 6 toes ?\n",
    "from sklearn.naive_bayes import MultinomialNB                          #  A non-graph ML routine, a Classifier  (similar perhaps to node property prediction, but on non-graph)\n",
    "                                                                       #     Several variants of NaiveBayes, one being 'multi-nomial'.\n",
    "                                                                       #     'multi-nomial', used commonly for word count style problems\n",
    "        \n",
    "from sklearn.pipeline import Pipeline                                  #  Allow us to simplify exuection of many sequential steps\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "931b5da0-70ed-4105-bb14-525ac3e458c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee020c9482064c63bf4bc698147c7ad1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "          0/? [?op/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OperationError",
     "evalue": "FWDXqqTtC787CjTR2v9dhrVoqQ6ksgPzytnjiSa89sDN-2sJyFsjJJZSxyXKMq backtrace (QueryOperation.cpp:67): backtrace (Network.h:331): backtrace (QueryOperation.cpp:70): backtrace (QueryClient.cpp:624): opgraph check failed (OpGraph.cpp:458): Type Checking Pass (OpGraphErrorChecker.cpp:12): backtrace (TypeChecker.cpp:1757): backtrace (OpGraph.cpp:716): backtrace (OpGraph.cpp:716): backtrace (OpGraph.cpp:692): backtrace (TypeChecker.cpp:1216): checking func __katana_internal_in_list (TypeChecker.cpp:334): backtrace (TypeChecker.cpp:990): Expected list type for IN, got large_utf8 (ListFunc.cpp:440): TCK = SyntaxError:InvalidArgumentType\nKatana = ArgumentTypeError:ExpectedListType: TCK = SyntaxError:InvalidArgumentType\nKatana = ArgumentTypeError:ExpectedListType: TCK = SyntaxError:InvalidArgumentType\nKatana = ArgumentTypeError:ExpectedListType",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperationError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [44], line 16\u001b[0m\n\u001b[1;32m      1\u001b[0m l_movies \u001b[38;5;241m=\u001b[39m [ \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m8469\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m11848\u001b[39m\u001b[38;5;124m\"\u001b[39m ]\n\u001b[1;32m      3\u001b[0m l_query  \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      4\u001b[0m \n\u001b[1;32m      5\u001b[0m \u001b[38;5;124m   MATCH (n: Movies) \u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m \n\u001b[1;32m     14\u001b[0m \u001b[38;5;124m   \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(l_movies)\n\u001b[0;32m---> 16\u001b[0m l_result \u001b[38;5;241m=\u001b[39m \u001b[43mmy_graph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43ml_query\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m display(\u001b[38;5;28mprint\u001b[39m(l_result[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m5\u001b[39m]))\n\u001b[1;32m     20\u001b[0m display(l_result\u001b[38;5;241m.\u001b[39mtable())\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/katana_enterprise/async_to_sync.py:249\u001b[0m, in \u001b[0;36mAsyncToSync.<locals>.do_wrap.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(underlying_func)\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m registry\u001b[38;5;241m.\u001b[39masync_to_sync(\n\u001b[0;32m--> 249\u001b[0m         \u001b[43munderlying_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m            \u001b[49m\u001b[43mget_self_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mregistry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msync_to_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mregistry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msync_to_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    254\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/katana_enterprise/async_to_sync.py:176\u001b[0m, in \u001b[0;36masync_to_sync.<locals>.wrapper\u001b[0;34m(timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m    166\u001b[0m     registry \u001b[38;5;241m=\u001b[39m AsyncToSyncClassRegistry\u001b[38;5;241m.\u001b[39mget()\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m registry\u001b[38;5;241m.\u001b[39masync_to_sync(\n\u001b[1;32m    168\u001b[0m         wait_for(\n\u001b[1;32m    169\u001b[0m             async_func(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    174\u001b[0m         )\n\u001b[1;32m    175\u001b[0m     )\n\u001b[0;32m--> 176\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwait_for\u001b[49m\u001b[43m(\u001b[49m\u001b[43masync_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/katana_enterprise/async_to_sync.py:147\u001b[0m, in \u001b[0;36mwait_for\u001b[0;34m(coro, timeout)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    146\u001b[0m     future \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mrun_coroutine_threadsafe(timeout_coro, loop\u001b[38;5;241m=\u001b[39mAsyncRunnerThread\u001b[38;5;241m.\u001b[39mget()\u001b[38;5;241m.\u001b[39mloop)\n\u001b[0;32m--> 147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m    149\u001b[0m     inner_future\u001b[38;5;241m.\u001b[39mcancel()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/concurrent/futures/_base.py:444\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 444\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    446\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/concurrent/futures/_base.py:389\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 389\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    390\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    391\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    392\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/asyncio/tasks.py:455\u001b[0m, in \u001b[0;36mwait_for\u001b[0;34m(fut, timeout, loop)\u001b[0m\n\u001b[1;32m    450\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe loop argument is deprecated since Python 3.8, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    451\u001b[0m                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand scheduled for removal in Python 3.10.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    452\u001b[0m                   \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    454\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 455\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m fut\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    458\u001b[0m     fut \u001b[38;5;241m=\u001b[39m ensure_future(fut, loop\u001b[38;5;241m=\u001b[39mloop)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/katana_enterprise/remote/aio/graph.py:299\u001b[0m, in \u001b[0;36mGraph.query\u001b[0;34m(self, query, memory_usage_factor, contextualize, **parameters)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m memory_usage_factor:\n\u001b[1;32m    297\u001b[0m     parameters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__katana_internal_match_batch_limit_scale_factor\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m memory_usage_factor\n\u001b[0;32m--> 299\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_query(query, parameters\u001b[38;5;241m=\u001b[39mparameters)\n\u001b[1;32m    300\u001b[0m rows \u001b[38;5;241m=\u001b[39m response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrows\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    301\u001b[0m columns \u001b[38;5;241m=\u001b[39m response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/katana_enterprise/remote/aio/graph.py:277\u001b[0m, in \u001b[0;36mGraph._run_query\u001b[0;34m(self, query, parameters, parquet)\u001b[0m\n\u001b[1;32m    273\u001b[0m     data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcypher\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_type\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParquet\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    275\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_operation_metadata(data)\n\u001b[0;32m--> 277\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39m_run_on_graph_and_wait(\u001b[38;5;28mself\u001b[39m, data)\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/katana_enterprise/remote/aio/client.py:324\u001b[0m, in \u001b[0;36mDatabase._run_on_graph_and_wait\u001b[0;34m(self, graph, data)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m attempt \u001b[38;5;241m<\u001b[39m max_attempts:\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 324\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_op(op)\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mServerCommunicationError:\n\u001b[1;32m    326\u001b[0m         logging\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror while waiting, retrying (attempt=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattempt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/katana_enterprise/remote/aio/client.py:261\u001b[0m, in \u001b[0;36mDatabase._wait_op\u001b[0;34m(self, op)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m _progress_bar() \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m wait_fn(operation_id) \u001b[38;5;28;01mas\u001b[39;00m stream:\n\u001b[0;32m--> 261\u001b[0m         \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m update \u001b[38;5;129;01min\u001b[39;00m stream:\n\u001b[1;32m    262\u001b[0m             status \u001b[38;5;241m=\u001b[39m update[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    264\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m update\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprogress\u001b[39m\u001b[38;5;124m\"\u001b[39m, []):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/asyncstdlib/builtins.py:444\u001b[0m, in \u001b[0;36mmap\u001b[0;34m(function, *iterable)\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m ScopedIter(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39miterable)) \u001b[38;5;28;01mas\u001b[39;00m args_iter:\n\u001b[1;32m    443\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m args \u001b[38;5;129;01min\u001b[39;00m args_iter:\n\u001b[0;32m--> 444\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    445\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m result\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/asyncstdlib/_core.py:117\u001b[0m, in \u001b[0;36mAwaitify.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m async_call \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_async_call\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m async_call \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__wrapped__\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, Awaitable):\n\u001b[1;32m    119\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_async_call \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__wrapped__  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/katana_enterprise/rpc/client.py:166\u001b[0m, in \u001b[0;36m_OperationClient._event_stream.<locals>.parse_stream\u001b[0;34m(line)\u001b[0m\n\u001b[1;32m    163\u001b[0m status \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 166\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mOperationError(operation_id \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus_message\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCanceled\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mCanceledError()\n",
      "\u001b[0;31mOperationError\u001b[0m: FWDXqqTtC787CjTR2v9dhrVoqQ6ksgPzytnjiSa89sDN-2sJyFsjJJZSxyXKMq backtrace (QueryOperation.cpp:67): backtrace (Network.h:331): backtrace (QueryOperation.cpp:70): backtrace (QueryClient.cpp:624): opgraph check failed (OpGraph.cpp:458): Type Checking Pass (OpGraphErrorChecker.cpp:12): backtrace (TypeChecker.cpp:1757): backtrace (OpGraph.cpp:716): backtrace (OpGraph.cpp:716): backtrace (OpGraph.cpp:692): backtrace (TypeChecker.cpp:1216): checking func __katana_internal_in_list (TypeChecker.cpp:334): backtrace (TypeChecker.cpp:990): Expected list type for IN, got large_utf8 (ListFunc.cpp:440): TCK = SyntaxError:InvalidArgumentType\nKatana = ArgumentTypeError:ExpectedListType: TCK = SyntaxError:InvalidArgumentType\nKatana = ArgumentTypeError:ExpectedListType: TCK = SyntaxError:InvalidArgumentType\nKatana = ArgumentTypeError:ExpectedListType"
     ]
    }
   ],
   "source": [
    "\n",
    "l_movies = [ \"8469\", \"11848\" ]\n",
    "\n",
    "l_query  = \"\"\"\n",
    "\n",
    "   MATCH (n: Movies) \n",
    "   \n",
    "   WHERE n.id IN [ 0\n",
    "   // WHERE n.id = \"8469\"\n",
    "   // WHERE n.title = 'Animal House'\n",
    "   \n",
    "   RETURN n // .title\n",
    "   LIMIT 5\n",
    "\n",
    "   \"\"\".format(l_movies)\n",
    "\n",
    "l_result = my_graph.query(l_query)\n",
    "\n",
    "\n",
    "display(print(l_result[0:5]))\n",
    "display(l_result.table())\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93bb2e9-c5d5-47ff-8335-fc70ac4e10cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(type(l_result[\"m.id\"]))\n",
    "print(l_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9f1ed8-7b10-4da7-8fc7-9169d85fb7bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19941e7-983c-4ea5-ad26-e919ad881842",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec42307a-ac0e-46f7-8b4c-e60414b9631a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22b0453-7f75-4d50-8c0d-899a58785a81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17685275-1907-4209-b87d-d0e3e4131422",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb6485e-9a46-4fa3-b198-630356767763",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020c7bb3-06fe-4316-a78c-f6ac05daae99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b48a0b2d-d27e-4401-b4d6-d33a8a245398",
   "metadata": {},
   "source": [
    "#  Introduction to CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828d62e9-1cf8-4eee-9147-f351961b95f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Working with  sklearn.feature_extraction.text.CountVectorizer\n",
    "#\n",
    "\n",
    "#  .  The results array will be automatically sorted by input key value.\n",
    "#        ..  So below, baseball will occupy the output's zero'th position,\n",
    "#            where cricket will occupy the 1'st.\n",
    "#  .  Words will be automatically split on whitespace, other.\n",
    "#        ..  So I plan to do my own splitting beforehand, lest I not know what the output array represents.\n",
    "#               O-Hare          was not split\n",
    "#               football-helmet was split\n",
    "#  .  Duplicates do receive multiple entries in the output.\n",
    "#\n",
    "#  .  2d integer64 array is output;\n",
    "#        ..  1st col is a counter, index into array\n",
    "#        ..  2nd col is for each word in the array, and the reference to 1st col, this word's position in the input\n",
    "#\n",
    "\n",
    "#  Just vectorizers to start-\n",
    "#\n",
    "my_cv = CountVectorizer()\n",
    "\n",
    "my_data_arr = [ \n",
    "   \"baseball\",\n",
    "   \"football\",\n",
    "   \"cricket\",\n",
    "   \"golf\",\n",
    "   \"racing\",\n",
    "   \"fencing\",\n",
    "   \"cricket\",\n",
    "]\n",
    "\n",
    "\n",
    "my_cv_out = my_cv.fit_transform(my_data_arr)\n",
    "   #\n",
    "#  my_cv_out = my_cv.fit_transform(dd_airports.airport_code)\n",
    "#  my_cv_out = my_cv.fit_transform(dd_airports.airport_name)\n",
    "\n",
    "\n",
    "print(type(my_cv_out))                     #  <class 'scipy.sparse._csr.csr_matrix'>\n",
    "\n",
    "                                           #  For my_data_arr above\n",
    "                                           #  -----------------------------------\n",
    "print(my_cv_out.shape)                     #  (7, 6)\n",
    "print(my_cv_out      )                     #  (0, 0)    1\n",
    "                                           #  (1, 3)    1\n",
    "                                           #  (2, 1)    1\n",
    "                                           #  (3, 4)    1\n",
    "                                           #  (4, 5)    1\n",
    "                                           #  (5, 2)    1\n",
    "                    \n",
    "                                           #  For dd_airports.airport_code above\n",
    "                                           #  -----------------------------------\n",
    "                                           #  (5, 5)\n",
    "                                           #  (0, 2)    1\n",
    "                                           #  (1, 3)    1\n",
    "                                           #  (2, 4)    1\n",
    "                                           #  (3, 1)    1\n",
    "                                           #  (4, 0)    1\n",
    " \n",
    "                                           #  For dd_airports.airport_name above\n",
    "                                           #  -----------------------------------\n",
    "                                           #  (5, 8)\n",
    "                                           #  (0, 6)    1\n",
    "                                           #  (1, 1)    1\n",
    "                                           #  (1, 3)    1\n",
    "                                           #  (2, 7)    1\n",
    "                                           #  (2, 4)    1\n",
    "                                           #  (3, 5)    1\n",
    "                                           #  (3, 0)    1\n",
    "                                           #  (4, 2)    1\n",
    "        \n",
    "print(\"\")\n",
    "\n",
    "df_words = pd.DataFrame(my_cv_out.toarray())\n",
    "   #\n",
    "for l_each in df_words.iterrows():\n",
    "   print(l_each)                 \n",
    "\n",
    "\n",
    "#  For my_data_arr above  (printed as it is output)\n",
    "# \n",
    "#  (0, 0    1 1    0 2    0 3    0 4    0 5    0 Name: 0, dtype: int64)\n",
    "#  (1, 0    0 1    0 2    0 3    1 4    0 5    0 Name: 1, dtype: int64)\n",
    "#  (2, 0    0 1    1 2    0 3    0 4    0 5    0 Name: 2, dtype: int64)\n",
    "#  (3, 0    0 1    0 2    0 3    0 4    1 5    0 Name: 3, dtype: int64)\n",
    "#  (4, 0    0 1    0 2    0 3    0 4    0 5    1 Name: 4, dtype: int64)\n",
    "#  (5, 0    0 1    1 2    1 3    0 4    0 5    0 Name: 5, dtype: int64)\n",
    "#  (6, 0    0 1    1 2    0 3    0 4    0 5    0 Name: 6, dtype: int64)\n",
    "\n",
    "#  Above better formatted as,\n",
    "#\n",
    "#  (0,    0 1    1 0    2 0    3 0   4 0   5 0   Name: 0, dtype: int64)\n",
    "#  (1,    0 0    1 0    2 0    3 1   4 0   5 0   Name: 1, dtype: int64)\n",
    "#  (2,    0 0    1 1    2 0    3 0   4 0   5 0   Name: 2, dtype: int64)\n",
    "#  (3,    0 0    1 0    2 0    3 0   4 1   5 0   Name: 3, dtype: int64)\n",
    "#  (4,    0 0    1 0    2 0    3 0   4 0   5 1   Name: 4, dtype: int64)\n",
    "#  (5,    0 0    1 1    2 1    3 0   4 0   5 0   Name: 5, dtype: int64)\n",
    "#  (6,    0 0    1 1    2 0    3 0   4 0   5 0   Name: 6, dtype: int64)\n",
    "#\n",
    "#   A     B C    B C    B C   ......\n",
    "#\n",
    "#  So,\n",
    "#     A   == row number, offset into the array, 0-6 (7) total rows   from my_data_arr\n",
    "#     B   == col number, offset inside the row, 0-5 (6) unique words from my_data_arr\n",
    "#     C   ==  1|0  is this keyword  0-5 (6)  found in this row  0-6 (7)\n",
    "#\n",
    "#  So, if you had  1000  input records times  20  unique words, the array would be  1000x20\n",
    "#\n",
    "#\n",
    "#  If the value of row-2 was (football, cricket, golf), its entry would appear as,\n",
    "# \n",
    "#  (1,    0 0    1 1    2 0    3 1   4 1   5 0 Name: 1, dtype: int64)\n",
    "#\n",
    "#     Recall that the unique keywords sort as; (0)baseball (1)cricket (2)fencing (3)football (4)golf (5)racing\n",
    "\n",
    "\n",
    "print(\"--\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff5ac0a-32c9-401f-9fcb-b3b5500f533d",
   "metadata": {},
   "source": [
    "#  A further example, first step, training phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7e4b85-372f-491c-84b1-fc7714788163",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Above was just a vectorizer-\n",
    "#\n",
    "#  Now, multiple cells, we extend into classifiers. IE., fraud|not-fraud, or multi-value .. ..\n",
    "\n",
    "\n",
    "#  More complete [ training ] data, as required ..\n",
    "#\n",
    "#     .  data[]       -- Our records, emails, nodes in a graph, .. in this case, a simple word\n",
    "#     .  class[]      -- Our unique list of node labels, for example\n",
    "#     .  target[]     -- Matches data[] above, the code of the node label.\n",
    "\n",
    "my_train = {}\n",
    "\n",
    "my_train[\"data\"] = [                                        #  Just words, imagine these to be fuller records shortly.\n",
    "   \"baseball\" ,\n",
    "   \"football\" ,\n",
    "   \"cricket\"  ,\n",
    "   \"golf\"     ,\n",
    "   \"racing\"   ,\n",
    "   \"fencing\"  ,\n",
    "      #\n",
    "   \"eggs\"     ,\n",
    "   \"bread\"    ,\n",
    "   \"cheese\"   ,\n",
    "   \"wine\"     ,\n",
    "   ]\n",
    "   #\n",
    "my_train[\"class\"] = [                                       #  No direct relation to the array above, yet.\n",
    "   \"sport\",\n",
    "   \"food\" ,\n",
    "   ]\n",
    "\n",
    "\n",
    "my_train[\"data\"].sort()                                     #  We could have entered the data pre-sorted.\n",
    "my_train[\"class\"].sort()                                    #  Here, just reminding us that we should sort for ease of viewing.\n",
    "\n",
    "\n",
    "my_train[\"target\"] = [1, 0, 0, 1, 0, 1, 1, 1, 1, 0]         #  These values correspond to those in my_train[\"data\"]; they align by offset into the array\n",
    "                                                            #  Also, the value of  0,1,n  corrsepond to the position in the my_train[\"class\"] array\n",
    "    \n",
    "   ###\n",
    "\n",
    "    \n",
    "for l_index, l_zip in enumerate(zip(my_train[\"data\"], my_train[\"target\"])):\n",
    "   print(\"Data: %-18s   Class: %s\" % (l_zip[0], my_train[\"class\"][l_zip[1]]) )\n",
    "\n",
    "#  Sample output,\n",
    "#\n",
    "#     Data: baseball             Class: sport\n",
    "#     Data: bread                Class: food\n",
    "#     Data: cheese               Class: food\n",
    "#     Data: cricket              Class: sport\n",
    "#     Data: eggs                 Class: food\n",
    "#     Data: fencing              Class: sport\n",
    "#     Data: football             Class: sport\n",
    "#     Data: golf                 Class: sport\n",
    "#     Data: racing               Class: sport\n",
    "#     Data: wine                 Class: food\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780ba00b-d619-4f2c-8682-d986ee8c0776",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# #  So we've built the vector before-  Now we do it with new data\n",
    "#\n",
    "#  Here we add steps for,\n",
    "#\n",
    "#     .  Tf-idf      -- Weighting for words found often (less useful) versus words found rarely (more useful)\n",
    "#     .  NaiveBayes  -- An ML routine similar to node proprty prediction, before there were graphs\n",
    "\n",
    "#     **  We will replace all of this cell in the next cell, using a 'pipeline'\n",
    "\n",
    "\n",
    "my_cv = CountVectorizer()\n",
    "\n",
    "#  Step 1 of 3 ..\n",
    "#\n",
    "#  Old:  Same as we did before/above\n",
    "#\n",
    "my_cv_out = my_cv.fit_transform(my_train[\"data\"])\n",
    "#  print(my_cv_out.shape)\n",
    "\n",
    "\n",
    "#  Step 2 of 3 ..\n",
    "#\n",
    "#  New:  Now normalize fact that longer (documents/Nodes/other) have more words and would get unfair weights\n",
    "#        term frequency / inverse document frequency\n",
    "#        (Tf-idf)\n",
    "#\n",
    "#        Effectively, reduce the weight of words that occur in more (documents/Nodes/other),\n",
    "#           in favor of words that occur in fewer (documents/Nodes/other)\n",
    "#\n",
    "my_tfidf_train    = TfidfTransformer().fit_transform(my_cv_out)\n",
    "#  print(my_tfidf_train.shape)\n",
    "\n",
    "\n",
    "#  Step 3 of 3 ..\n",
    "#\n",
    "#  New: 'train' the classifier\n",
    "#\n",
    "my_classifier = MultinomialNB().fit(my_tfidf_train, my_train[\"target\"])\n",
    "\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84c2095-3b00-4cd5-a3f8-ccade725d563",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  The above had 3 steps, generally always used in sequence.\n",
    "#  Make this easier; use a pipeline\n",
    "#\n",
    "\n",
    "my_pipeline = Pipeline([\n",
    "   (\"cv\",     CountVectorizer()  ),\n",
    "   (\"tfidf\",  TfidfTransformer() ),\n",
    "   (\"clf\",    MultinomialNB()    ),\n",
    "   ])\n",
    "\n",
    "\n",
    "#  Here is the pipeline replacing the cell above-\n",
    "#\n",
    "#     (So this work was already done, Now we're showing it done more simply.)\n",
    "#\n",
    "my_classifier = my_pipeline.fit(my_train[\"data\"], my_train[\"target\"])\n",
    "\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47641439-ac2b-4515-8f47-b59c9cf5994a",
   "metadata": {},
   "source": [
    "#  Final piece, scoring (applying)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d87255-4a99-4c74-8a9a-8ed80cd19406",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Now we are ready to score (apply) the model.\n",
    "#\n",
    "\n",
    "\n",
    "#  Here is the test data\n",
    "#\n",
    "my_test = {}\n",
    "   #\n",
    "my_test[\"data\"] = [ \n",
    "   \"I hate baseball\" ,\n",
    "   \"I like wine\"     ,\n",
    "   \"eggs with cheese\",\n",
    "   \"eggs with cheese football\",\n",
    "   \"football with eggs and cheese\",\n",
    "   \"Wisconsin, America's Dairyland\",\n",
    "   ]\n",
    "\n",
    "\n",
    "#  Applying/scoring the model\n",
    "#\n",
    "my_result = my_classifier.predict(my_test[\"data\"])\n",
    "\n",
    "\n",
    "\n",
    "#  Recall that the sorted class array is,\n",
    "#\n",
    "#     my_train[\"class\"] = [ \"food\" , \"sport\", ]\n",
    "\n",
    "\n",
    "my_test[\"target\"] = [1, 0, 0, 0, 0, 0]                                     #  We only need this to gauge accuracy below, and the last one is wrong\n",
    "\n",
    "\n",
    "\n",
    "display(\"Average accuracy: %f\" % (np.mean(my_result == my_test[\"target\"])) )\n",
    "   #\n",
    "for l_index, l_zip in enumerate(zip(my_test[\"data\"], my_test[\"target\"])):\n",
    "   print(\"Data: %-36s   Correct Class: %-10s   Predict Class: %-10s\" % (l_zip[0], my_train[\"class\"][l_zip[1]], my_train[\"class\"][my_result[l_index]]) )\n",
    "\n",
    "#  Sample output,\n",
    "#\n",
    "#     'Average accuracy: 0.833333'\n",
    "#\n",
    "#     Data: I hate baseball                        Correct Class: sport        Predict Class: sport     \n",
    "#     Data: I like wine                            Correct Class: food         Predict Class: food      \n",
    "#     Data: eggs with cheese                       Correct Class: food         Predict Class: food      \n",
    "#     Data: eggs with cheese football              Correct Class: food         Predict Class: food      \n",
    "#     Data: football with eggs and cheese          Correct Class: food         Predict Class: food      \n",
    "#     Data: Wisconsin, America's Dairyland         Correct Class: food         Predict Class: sport     \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f49097-92db-4c90-b76c-199b5a7ca20b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
