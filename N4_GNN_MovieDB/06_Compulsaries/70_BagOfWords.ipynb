{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d776f754-ef98-4cbd-a423-647c483341ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Here we demonstrate the following Dask DataFrame techniques;\n",
    "#\n",
    "#     .  How to create a Dask DataFrame from an array.\n",
    "#     .  Introduction to  sklearn.feature_extraction.text import CountVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682150e3-6a50-4197-a239-80d1d2ea6edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "NUM_PARTITIONS  = 3\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0416c10d-a1cb-433d-ad5e-85498aaa1744",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "   #\n",
    "import dask.array as da\n",
    "import dask.dataframe as dd\n",
    "   #\n",
    "from dask.dataframe import from_pandas\n",
    "\n",
    "from tabulate import tabulate\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "13c6a99e-7b23-4cf6-9880-d70fffb4b889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#  We will also explain these as we use them below-\n",
    "#\n",
    "from sklearn.feature_extraction.text import CountVectorizer            #  Build a sparse vector (matrix) of keywords\n",
    "from sklearn.feature_extraction.text import TfidfTransformer           #  Tool to norrmalize a condition; words that occur frequently, versus giving more weight to infrequent words\n",
    "   #                                                                   #        Ie., Having 6 toes is way more rare than having 5. Should I give more weight that a person is 5'10\", or has 6 toes ?\n",
    "from sklearn.naive_bayes import MultinomialNB                          #  A non-graph ML routine, a Classifier  (similar perhaps to node property prediction, but on non-graph)\n",
    "                                                                       #     Several variants of NaiveBayes, one being 'multi-nomial'.\n",
    "                                                                       #     'multi-nomial', used commonly for word count style problems\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131298d0-5cb4-4b62-8413-5e5571ef4b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Create our source Dask DataFrame from an array.\n",
    "#     (We use this later, briefly, for comparisons.)\n",
    "\n",
    "np_airports = np.array([\n",
    "       #\n",
    "   [\"MKE\", \"Milwaukee\"     , \"WI\", \"Airport\"],\n",
    "   [\"ORD\", \"Chicago O-Hare\", \"IL\", \"Airport\"],\n",
    "   [\"SJC\", \"San Jose\"      , \"CA\", \"Airport\"],\n",
    "   [\"LAX\", \"Los Angeles\"   , \"CA\", \"Airport\"],\n",
    "   [\"DEN\", \"Denver\"        , \"CO\", \"Airport\"],\n",
    "       #\n",
    "   ], dtype=\"str\")\n",
    "\n",
    "pd_airports = pd.DataFrame(np_airports, columns = [\"airport_code\", \"airport_name\", \"state_code\", \"LABEL\"])\n",
    "   #\n",
    "dd_airports = from_pandas(pd_airports, npartitions = NUM_PARTITIONS)\n",
    "\n",
    "\n",
    "for l_each in dd_airports.itertuples():\n",
    "   print(\"Airport:  %3s   %-18s   %-2s   %-10s\" % (l_each.airport_code, l_each.airport_name, l_each.state_code, l_each.LABEL))\n",
    "\n",
    "print(\"--\")\n",
    "\n",
    "#  Sample output,\n",
    "#\n",
    "#  Airport:  MKE   Milwaukee            WI   Airport   \n",
    "#  Airport:  ORD   Chicago O-Hare       IL   Airport   \n",
    "#  Airport:  SJC   San Jose             CA   Airport   \n",
    "#  Airport:  LAX   Los Angeles          CA   Airport   \n",
    "#  Airport:  DEN   Denver               CO   Airport  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48a0b2d-d27e-4401-b4d6-d33a8a245398",
   "metadata": {},
   "source": [
    "#  Introduction to CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828d62e9-1cf8-4eee-9147-f351961b95f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Working with  sklearn.feature_extraction.text.CountVectorizer\n",
    "#\n",
    "#     See,\n",
    "#        https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html#tokenizing-text-with-scikit-learn\n",
    "\n",
    "\n",
    "#  .  The results array is sorted by input key value.\n",
    "#        ..  So below, baseball occupies the output's zero'th position,\n",
    "#            where cricket will occupy the 1'st.\n",
    "#  .  Words will be automatically split on whitespace, other.\n",
    "#        ..  So I plan to do my own splitting beforehand, lest I not know what the output array represents.\n",
    "#               O-Hare          was not split\n",
    "#               football-helmet was split\n",
    "#  .  Duplicates do receive multiple entries in the output. Not certain of the effect of that.\n",
    "#\n",
    "#  .  2d integer64 array is output;\n",
    "#        ..  1st col is a counter, index into array\n",
    "#        ..  2nd col is for each word in the array, and the reference to 1st col, this word's position in the input\n",
    "#\n",
    "\n",
    "my_cv = CountVectorizer()\n",
    "\n",
    "my_input_arr = [ \n",
    "   \"baseball\",\n",
    "   \"football\",\n",
    "   \"cricket\",\n",
    "   \"golf\",\n",
    "   \"racing\",\n",
    "   \"fencing\",\n",
    "   \"cricket\",\n",
    "]\n",
    "\n",
    "\n",
    "my_cv_counts = my_cv.fit_transform(my_input_arr)\n",
    "   #\n",
    "#  my_cv_counts = my_cv.fit_transform(dd_airports.airport_code)\n",
    "#  my_cv_counts = my_cv.fit_transform(dd_airports.airport_name)\n",
    "\n",
    "\n",
    "print(type(my_cv_counts))                  #  <class 'scipy.sparse._csr.csr_matrix'>\n",
    "\n",
    "                                           #  For my_input_arr above\n",
    "                                           #  -----------------------------------\n",
    "print(my_cv_counts.shape)                  #  (7, 6)\n",
    "print(my_cv_counts      )                  #  (0, 0)    1\n",
    "                                           #  (1, 3)    1\n",
    "                                           #  (2, 1)    1\n",
    "                                           #  (3, 4)    1\n",
    "                                           #  (4, 5)    1\n",
    "                                           #  (5, 2)    1\n",
    "                    \n",
    "                                           #  For dd_airports.airport_code above\n",
    "                                           #  -----------------------------------\n",
    "                                           #  (5, 5)\n",
    "                                           #  (0, 2)    1\n",
    "                                           #  (1, 3)    1\n",
    "                                           #  (2, 4)    1\n",
    "                                           #  (3, 1)    1\n",
    "                                           #  (4, 0)    1\n",
    " \n",
    "                                           #  For dd_airports.airport_name above\n",
    "                                           #  -----------------------------------\n",
    "                                           #  (5, 8)\n",
    "                                           #  (0, 6)    1\n",
    "                                           #  (1, 1)    1\n",
    "                                           #  (1, 3)    1\n",
    "                                           #  (2, 7)    1\n",
    "                                           #  (2, 4)    1\n",
    "                                           #  (3, 5)    1\n",
    "                                           #  (3, 0)    1\n",
    "                                           #  (4, 2)    1\n",
    "        \n",
    "print(\"\")\n",
    "\n",
    "df_words = pd.DataFrame(my_cv_counts.toarray())\n",
    "   #\n",
    "for l_each in df_words.iterrows():\n",
    "   print(l_each)                 \n",
    "\n",
    "\n",
    "#  For my_input_arr above  (printed as it is output)\n",
    "# \n",
    "#  (0, 0    1 1    0 2    0 3    0 4    0 5    0 Name: 0, dtype: int64)\n",
    "#  (1, 0    0 1    0 2    0 3    1 4    0 5    0 Name: 1, dtype: int64)\n",
    "#  (2, 0    0 1    1 2    0 3    0 4    0 5    0 Name: 2, dtype: int64)\n",
    "#  (3, 0    0 1    0 2    0 3    0 4    1 5    0 Name: 3, dtype: int64)\n",
    "#  (4, 0    0 1    0 2    0 3    0 4    0 5    1 Name: 4, dtype: int64)\n",
    "#  (5, 0    0 1    1 2    1 3    0 4    0 5    0 Name: 5, dtype: int64)\n",
    "#  (6, 0    0 1    1 2    0 3    0 4    0 5    0 Name: 6, dtype: int64)\n",
    "\n",
    "#  Above better formatted as,\n",
    "#\n",
    "#  (0,    0 1    1 0    2 0    3 0   4 0   5 0   Name: 0, dtype: int64)\n",
    "#  (1,    0 0    1 0    2 0    3 1   4 0   5 0   Name: 1, dtype: int64)\n",
    "#  (2,    0 0    1 1    2 0    3 0   4 0   5 0   Name: 2, dtype: int64)\n",
    "#  (3,    0 0    1 0    2 0    3 0   4 1   5 0   Name: 3, dtype: int64)\n",
    "#  (4,    0 0    1 0    2 0    3 0   4 0   5 1   Name: 4, dtype: int64)\n",
    "#  (5,    0 0    1 1    2 1    3 0   4 0   5 0   Name: 5, dtype: int64)\n",
    "#  (6,    0 0    1 1    2 0    3 0   4 0   5 0   Name: 6, dtype: int64)\n",
    "#\n",
    "#   A     B C    B C    B C   ......\n",
    "#\n",
    "#  So,\n",
    "#     A   == row number, offset into the array, 0-6 (7) total rows   from my_input_arr\n",
    "#     B   == col number, offset inside the row, 0-5 (6) unique words from my_input_arr\n",
    "#     C   ==  1|0  is this keyword  0-5 (6)  found in this row  0-6 (7)\n",
    "#\n",
    "#  So, if you had  1000  input records times  20  unique words, the array would be  1000x20\n",
    "#\n",
    "#\n",
    "#  If the value of row-2 was (football, cricket, golf), its entry would appear as,\n",
    "# \n",
    "#  (1,    0 0    1 1    2 0    3 1   4 1   5 0 Name: 1, dtype: int64)\n",
    "#\n",
    "#     Recall that the unique keywords sort as; (0)baseball (1)cricket (2)fencing (3)football (4)golf (5)racing\n",
    "\n",
    "\n",
    "print(\"--\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff5ac0a-32c9-401f-9fcb-b3b5500f533d",
   "metadata": {},
   "source": [
    "#  A more complete example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8a7e4b85-372f-491c-84b1-fc7714788163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data: baseball             Category: sport\n",
      "Data: bread                Category: food\n",
      "Data: cheese               Category: food\n",
      "Data: cricket              Category: sport\n",
      "Data: eggs                 Category: food\n",
      "Data: fencing              Category: sport\n",
      "Data: football             Category: sport\n",
      "Data: golf                 Category: sport\n",
      "Data: racing               Category: sport\n",
      "Data: wine                 Category: food\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#  More complete [ training ] data, as required ..\n",
    "#\n",
    "#     .  data[]       -- Our records, emails, nodes in a graph, .. in this case, a simple word\n",
    "#     .  category[]   -- Our unique list of node labels, for example\n",
    "#     .  target[]     -- Matches data[] above, the code of the node label.\n",
    "\n",
    "my_train = {}\n",
    "\n",
    "my_train[\"data\"] = [ \n",
    "   \"baseball\" ,\n",
    "   \"football\" ,\n",
    "   \"cricket\"  ,\n",
    "   \"golf\"     ,\n",
    "   \"racing\"   ,\n",
    "   \"fencing\"  ,\n",
    "      #\n",
    "   \"eggs\"     ,\n",
    "   \"bread\"    ,\n",
    "   \"cheese\"   ,\n",
    "   \"wine\"     ,\n",
    "   ]\n",
    "   #\n",
    "my_train[\"category\"] = [\n",
    "   \"sport\",\n",
    "   \"food\" ,\n",
    "   ]\n",
    "\n",
    "\n",
    "my_train[\"data\"].sort()                #  We could have entered the data pre-sorted\n",
    "my_train[\"category\"].sort()            #  Here, just reminding us that we should sort.\n",
    "\n",
    "\n",
    "my_train[\"target\"] = [1, 0, 0, 1, 0, 1, 1, 1, 1, 0]\n",
    "\n",
    "   ###\n",
    "    \n",
    "for l_index, l_zip in enumerate(zip(my_train[\"data\"], my_train[\"target\"])):\n",
    "   print(\"Data: %-18s   Category: %s\" % (l_zip[0], my_train[\"category\"][l_zip[1]]) )\n",
    "\n",
    "#  Sample output,\n",
    "#\n",
    "#     Data: baseball             Category: sport\n",
    "#     Data: bread                Category: food\n",
    "#     Data: cheese               Category: food\n",
    "#     Data: cricket              Category: sport\n",
    "#     Data: eggs                 Category: food\n",
    "#     Data: fencing              Category: sport\n",
    "#     Data: football             Category: sport\n",
    "#     Data: golf                 Category: sport\n",
    "#     Data: racing               Category: sport\n",
    "#     Data: wine                 Category: food\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3d5b19c6-dd02-4476-964a-9ab6ea263120",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "y should be a 1d array, got an array of shape () instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [57], line 22\u001b[0m\n\u001b[1;32m     16\u001b[0m my_tfidf_train    \u001b[38;5;241m=\u001b[39m TfidfTransformer()\u001b[38;5;241m.\u001b[39mfit_transform(my_cv_counts)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# print(my_tfidf_train.shape)\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \n\u001b[1;32m     19\u001b[0m \n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m#  New: 'train' the classifier\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m my_classifier \u001b[38;5;241m=\u001b[39m \u001b[43mMultinomialNB\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmy_tfidf_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmy_cv_counts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/sklearn/naive_bayes.py:699\u001b[0m, in \u001b[0;36m_BaseDiscreteNB.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    679\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    680\u001b[0m     \u001b[38;5;124;03m\"\"\"Fit Naive Bayes classifier according to X, y.\u001b[39;00m\n\u001b[1;32m    681\u001b[0m \n\u001b[1;32m    682\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    697\u001b[0m \u001b[38;5;124;03m        Returns the instance itself.\u001b[39;00m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 699\u001b[0m     X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    700\u001b[0m     _, n_features \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    702\u001b[0m     labelbin \u001b[38;5;241m=\u001b[39m LabelBinarizer()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/sklearn/naive_bayes.py:553\u001b[0m, in \u001b[0;36m_BaseDiscreteNB._check_X_y\u001b[0;34m(self, X, y, reset)\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_X_y\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    552\u001b[0m     \u001b[38;5;124;03m\"\"\"Validate X and y in fit methods.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/sklearn/base.py:596\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    594\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[1;32m    595\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 596\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    597\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m    599\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/sklearn/utils/validation.py:1090\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1070\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1071\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1072\u001b[0m     )\n\u001b[1;32m   1074\u001b[0m X \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[1;32m   1075\u001b[0m     X,\n\u001b[1;32m   1076\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1087\u001b[0m     input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1088\u001b[0m )\n\u001b[0;32m-> 1090\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43m_check_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmulti_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmulti_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_numeric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_numeric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1092\u001b[0m check_consistent_length(X, y)\n\u001b[1;32m   1094\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X, y\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/sklearn/utils/validation.py:1111\u001b[0m, in \u001b[0;36m_check_y\u001b[0;34m(y, multi_output, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1109\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1110\u001b[0m     estimator_name \u001b[38;5;241m=\u001b[39m _check_estimator_name(estimator)\n\u001b[0;32m-> 1111\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mcolumn_or_1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1112\u001b[0m     _assert_all_finite(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, estimator_name\u001b[38;5;241m=\u001b[39mestimator_name)\n\u001b[1;32m   1113\u001b[0m     _ensure_no_complex_data(y)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/sklearn/utils/validation.py:1156\u001b[0m, in \u001b[0;36mcolumn_or_1d\u001b[0;34m(y, warn)\u001b[0m\n\u001b[1;32m   1147\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1148\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA column-vector y was passed when a 1d array was\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1149\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m expected. Please change the shape of y to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1152\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m   1153\u001b[0m         )\n\u001b[1;32m   1154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mravel(y)\n\u001b[0;32m-> 1156\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1157\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my should be a 1d array, got an array of shape \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(shape)\n\u001b[1;32m   1158\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: y should be a 1d array, got an array of shape () instead."
     ]
    }
   ],
   "source": [
    "\n",
    "my_cv = CountVectorizer()\n",
    "\n",
    "#  Same as we did before/above\n",
    "#\n",
    "my_cv_counts = my_cv.fit_transform(my_train[\"data\"])\n",
    "# print(my_cv_counts.shape)\n",
    "\n",
    "\n",
    "#  New: Now normalize fact that longer (documents/Nodes/other) have more words and would get unfair weights\n",
    "#       term frequency / inverse document frequency\n",
    "#       (Tf-idf)\n",
    "#\n",
    "#       Effectively, reduce the weight of words that occur in more (documents/Nodes/other),\n",
    "#          in favor of words that occur in fewer (documents/Nodes/other)\n",
    "\n",
    "aaa = TfidfTransformer(use_idf=True).fit(my_cv_counts)\n",
    "bbb = \n",
    "   #\n",
    "# my_tfidf_train    = TfidfTransformer().fit_transform(my_cv_counts)\n",
    "\n",
    "# print(my_tfidf_train.shape)\n",
    "\n",
    "\n",
    "#  New: 'train' the classifier\n",
    "#\n",
    "my_classifier = MultinomialNB().fit(my_tfidf_train, my_cv_counts)\n",
    "\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d87255-4a99-4c74-8a9a-8ed80cd19406",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c3cf4b-6b60-4e2c-aba4-a04c47a7cd23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0264d8-fc6c-416d-ac38-335e00fdd2ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f49097-92db-4c90-b76c-199b5a7ca20b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
