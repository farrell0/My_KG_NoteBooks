{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064cde52-a7e1-486e-a7c4-3ce3aaaa01da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  This NoteBook contains code to run classic ML routines against a \n",
    "#  number of familiar data sets ..\n",
    "#\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d433bb83-5f71-406e-ab1c-1b0ef02583f1",
   "metadata": {
    "tags": []
   },
   "source": [
    "#  Step 00: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba7589c-6f1e-4f5c-a37b-21f8944c2499",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Largely code to control how print statements and related work\n",
    "#\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%xmode Minimal\n",
    "\n",
    "\n",
    "#  Setting display options \n",
    "#\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.width\", 640)\n",
    "   #\n",
    "import numpy as np\n",
    "np.set_printoptions(edgeitems = 30, linewidth = 100000, \n",
    "   formatter = dict(float = lambda x: \"%.3g\" % x))\n",
    "\n",
    "#  Sets horizontal scroll for wide outputs\n",
    "#\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"\"))\n",
    "\n",
    "   ###\n",
    "    \n",
    "from tabulate import tabulate\n",
    "#\n",
    "#  How to use tabulate-\n",
    "#\n",
    "#  l_result = [{ \"col1\": 20, \"col2\": 30}]\n",
    "#  #\n",
    "#  print(tabulate(l_result, headers='keys', tablefmt='psql', showindex=False))\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a747a0-4aec-4cd2-bf38-8849deee7ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Helper functions for what we want to do below-\n",
    "#\n",
    "#  **  You must run this cell to do much of anything in this NoteBook\n",
    "\n",
    "#  We use these objects to store the history of results; display only\n",
    "#\n",
    "class HistoryIterator:\n",
    "   def __init__(self, history):\n",
    "       self._history = history\n",
    "       self._index = 0\n",
    "\n",
    "   def __next__(self):\n",
    "       if (self._index < len(self._history._events)):\n",
    "           result = (self._history._events[self._index][\"event\"] , self._history._events[self._index][\"measure\"])\n",
    "           self._index +=1\n",
    "           return result\n",
    "       raise StopIteration\n",
    "\n",
    "class History:\n",
    "   def __init__(self):\n",
    "      self._events = list()\n",
    "\n",
    "   def clear(self):\n",
    "      self._events = list()\n",
    "    \n",
    "   def add(self, event, measure):\n",
    "      self._events.append({\"event\": event, \"measure\": measure})\n",
    "\n",
    "   def __iter__(self):\n",
    "      return HistoryIterator(self)\n",
    "\n",
    "\n",
    "l_history = History()\n",
    "\n",
    "\n",
    "#  The sklearn ML routines follow a very consistent pattern. As such, we\n",
    "#  put these in a function, reduce redundant code below-\n",
    "#\n",
    "\n",
    "def do_model(i_routine, i_train_data, i_train_labels, i_test_data, i_test_labels, i_name_of_test):\n",
    "\n",
    "   #  Train whatever model\n",
    "   #\n",
    "   i_routine.fit(i_train_data, i_train_labels)\n",
    "   \n",
    "   #  Predict on the test data\n",
    "   #\n",
    "   l_predicted_labels = i_routine.predict(i_test_data)\n",
    "   l_accuracy         = (i_routine.score(i_test_data, i_test_labels) * 100)\n",
    "      #\n",
    "   l_history.add(event = i_name_of_test, measure = l_accuracy)\n",
    "   \n",
    "   #  Output results\n",
    "   #\n",
    "   print(i_name_of_test + \" ...\")\n",
    "   print(\"   Actual    labels from test......... %s\" % (i_test_labels     ) )\n",
    "   print(\"   Predicted labels from test......... %s\" % (l_predicted_labels) )\n",
    "   print(   \"   ###\")\n",
    "   print(\"   Accuracy: %0.4f %%\" % (l_accuracy))\n",
    "\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e65a137-2d39-411c-9316-aff11a9b6108",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Sometimes we want to clear history-\n",
    "#\n",
    "\n",
    "l_history.clear()\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebe2eac-169f-4068-b5a4-d70e1e2856a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  To add a blank line to history-\n",
    "#\n",
    "\n",
    "l_history.add(event = \"\", measure = \"\")\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317e2979-c711-419c-91b4-e035bafbe79c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#  Step A1: Iris Data load, encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d1be95-f9df-4435-ab56-add6aa68b8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Read the Iris data into a Pandas Dataframe\n",
    "#\n",
    "#     Features\n",
    "#     1. sepal length in cm\n",
    "#     2. sepal width in cm\n",
    "#     3. petal length in cm\n",
    "#     4. petal width in cm\n",
    "#     5. class: \n",
    "#        Iris-setosa\n",
    "#        Iris-versicolour\n",
    "#        Iris-virginica\n",
    "#\n",
    "#  To convert class into a numeric, we use sklearn.preprocessing.LabelEncoder\n",
    "#  See,\n",
    "#     https://www.turing.com/kb/convert-categorical-data-in-pandas-and-scikit-learn\n",
    "#\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "my_le = LabelEncoder()\n",
    "   #\n",
    "l_folder = \"20_Data\"\n",
    "l_file   = \"11_iris.data.txt\"\n",
    "\n",
    "\n",
    "pd_iris  = pd.read_csv((l_folder + \"/\" + l_file), header = 0, sep = \",\",\n",
    "   names = [\"sl\", \"sw\", \"pl\", \"pw\", \"class\"],\n",
    "   dtype = {\"sl\": \"float\", \"sw\": \"float\", \"pl\": \"float\", \"pw\": \"float\", \"class\": \"string\"} )\n",
    "      #\n",
    "pd_iris[\"class_encoded\"]  =  my_le.fit_transform(pd_iris[\"class\"])\n",
    "   #\n",
    "pd_iris = pd_iris.drop([\"class\"], axis = 1)\n",
    "    \n",
    "    \n",
    "#  Pandas.Dataframe.sample() returns a randomized set of rows, versus\n",
    "#  say head(), which always returns the first n ..\n",
    "#\n",
    "print(tabulate(pd_iris.sample(5), headers='keys', tablefmt='psql', showindex=False))\n",
    "print(\"Number of rows: %d\" % (len(pd_iris)))\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd64b3f-30aa-4680-b983-462d5da51304",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Split data into training and test.\n",
    "#  Convert the data into numpy arrays, since the ml libraries we use later expect that.\n",
    "#\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np_iris = {}\n",
    "   #\n",
    "np_iris[\"train\"], np_iris[\"test\"] = train_test_split(pd_iris.to_numpy(),              #  random_state calls to shuffle the data,\n",
    "   test_size = 0.20, random_state = 40)                                               #    which had arrived sorted\n",
    "                                                                                      #  10% yields way too high of an accuracy\n",
    "                                                                                      #    far below\n",
    "print(\"Number of total rows: %d   Training rows: %d   Test rows: %d\" %\n",
    "  (len(pd_iris), len(np_iris[\"train\"]), len(np_iris[\"test\"])) )\n",
    "\n",
    "print()\n",
    "print(\"Train data:\")\n",
    "print(\"%s\" % (np_iris[\"train\"][0:5]))\n",
    "print()\n",
    "print(\"Test  data:\")\n",
    "print(\"%s\" % (np_iris[\"test\" ][0:5]))\n",
    "print()\n",
    "   #\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62191e70-57c7-49d1-ba47-cca3517b0d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Boxplot, and normalize ..\n",
    "#\n",
    "#  Normalize from,\n",
    "#     https://datascience.stackexchange.com/questions/39142/normalize-matrix-in-python-numpy\n",
    "#\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "   #\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "print(type(np_iris[\"train\"]))\n",
    "\n",
    "plt.boxplot(np_iris[\"train\"])\n",
    "plt.show()\n",
    "\n",
    "   ###\n",
    "\n",
    "def my_normalize(X, x_min, x_max):\n",
    "   nom = (X-X.min(axis=0))*(x_max-x_min)\n",
    "   denom = X.max(axis=0) - X.min(axis=0)\n",
    "   denom[denom==0] = 1\n",
    "   return x_min + nom/denom \n",
    "\n",
    "#  If we normalize the \"class\" column, we lose the categorical nature\n",
    "#  of that data. So, create a deep copy, then just normalize the non-\n",
    "#  class columns.\n",
    "#\n",
    "np_iris[\"train_norm\"] = np.copy(np_iris[\"train\"])\n",
    "np_iris[\"test_norm\" ] = np.copy(np_iris[\"test\" ])\n",
    "   #\n",
    "np_iris[\"train_norm\"][:, :4] = my_normalize(np_iris[\"train_norm\"][:, :4], 0, 1)\n",
    "np_iris[\"test_norm\" ][:, :4] = my_normalize(np_iris[\"test_norm\" ][:, :4], 0, 1)\n",
    "\n",
    "plt.boxplot(np_iris[\"train_norm\"])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(\"--\")\n",
    "\n",
    "#  Legend:\n",
    "#\n",
    "#  Per a given column-\n",
    "#\n",
    "#    . The lower box line, marks 1st quartile. 25% of entries are below this line\n",
    "#      Middle line, 2nd.                       50% above/below\n",
    "#      Top box line, 3rd.                      25% above\n",
    "#\n",
    "#    . The true top and bottom lines-\n",
    "#      There's a formula for this; marks the boundary for entries considered outliers.\n",
    "#\n",
    "#           o            Outliers\n",
    "#           o\n",
    "#\n",
    "#          ---           Q3 + 1.5 QR\n",
    "#           |\n",
    "#           |\n",
    "#           |\n",
    "#           |\n",
    "#       +-------+        Q3      <-----------------+\n",
    "#       |       |                                  |\n",
    "#       |       |                                 1 QR\n",
    "#       +-------+        Median  (Q2)              |\n",
    "#       |       |                                  |\n",
    "#       |       |                                  |\n",
    "#       +-------+        Q1      <-----------------+\n",
    "#           |\n",
    "#           |\n",
    "#          ---           Q1 - 1.5 QR\n",
    "#\n",
    "#           o\n",
    "#           o\n",
    "#           o            Outliers\n",
    "#           o\n",
    "#\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5642d7fa-e89f-47c8-90c4-13e33fc2106c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#  Step A2: Iris Data train, test .. NearestCentroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d15b429-f361-434a-a3c4-f91f48d61824",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "\n",
    "#  Our numpy array has 5 columns, with the last column being the class.\n",
    "#  To review numpy array slicing,\n",
    "#\n",
    "#     To get the first 4 columns use,\n",
    "#        np_iris[\"train\"][:, :4]\n",
    "#     To get the last column use,\n",
    "#        np_iris[\"train\"][:, -1]\n",
    "#\n",
    "\n",
    "do_model(NearestCentroid(), np_iris[\"train\"][:, :4], np_iris[\"train\"][:, -1], np_iris[\"test\"][:, :4], np_iris[\"test\"][:, -1], \"Iris: Centroid\") \n",
    "print()\n",
    "print(tabulate(l_history, headers=[\"Category\", \"Accuracy\"], tablefmt='psql', showindex=False))\n",
    "\n",
    "#  do_model(NearestCentroid(), np_iris[\"train_norm\"][:, :4], np_iris[\"train_norm\"][:, -1], np_iris[\"test_norm\"][:, :4], np_iris[\"test_norm\"][:, -1], \"Iris: Centroid Normalized\") \n",
    "#  print()\n",
    "#  print(tabulate(l_history, headers=[\"Category\", \"Accuracy\"], tablefmt='psql', showindex=False))\n",
    "\n",
    "print(\"--\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c25a3d4-cf32-4503-8c86-e14146b7fa9d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#  Step A3: Iris Data train, test .. kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c1202c-c630-4e27-aea8-4ed425b139ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#  Our numpy array has 5 columns, with the last column being the class.\n",
    "#  To review numpy array slicing,\n",
    "#\n",
    "#     To get the first 4 columns use,\n",
    "#        np_iris[\"train\"][:, :4]\n",
    "#     To get the last column use,\n",
    "#        np_iris[\"train\"][:, -1]\n",
    "#\n",
    "\n",
    "do_model(KNeighborsClassifier(n_neighbors = 3), np_iris[\"train\"][:, :4], np_iris[\"train\"][:, -1], np_iris[\"test\"][:, :4], np_iris[\"test\"][:, -1], \"Iris: kNN=3\" ) \n",
    "print()\n",
    "print(tabulate(l_history, headers=[\"Category\", \"Accuracy\"], tablefmt='psql', showindex=False))\n",
    "\n",
    "#  do_model(KNeighborsClassifier(n_neighbors = 3), np_iris[\"train_norm\"][:, :4], np_iris[\"train_norm\"][:, -1], np_iris[\"test_norm\"][:, :4], np_iris[\"test_norm\"][:, -1], \"Iris: kNN=3 Normalized\" ) \n",
    "#  print()\n",
    "#  print(tabulate(l_history, headers=[\"Category\", \"Accuracy\"], tablefmt='psql', showindex=False))\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5328044-4f54-41b6-a37a-0f96a11e2250",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#  Step A4: Iris Data train, test .. Naive Bayes, Gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c1740a-f656-4d6d-bdf3-cb2f053fef60",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "#  Naive Bayes, Gaussian\n",
    "#\n",
    "#     Gaussian usually does better than the Multinomial below because,\n",
    "#        Gaussian expects continuous values\n",
    "#        Multinomial expects discreet values\n",
    "#\n",
    "#     And our values are continuous\n",
    "#\n",
    "\n",
    "do_model(GaussianNB(), np_iris[\"train\"][:, :4], np_iris[\"train\"][:, -1], np_iris[\"test\"][:, :4], np_iris[\"test\"][:, -1], \"Iris: GaussianNB\" ) \n",
    "print()\n",
    "print(tabulate(l_history, headers=[\"Category\", \"Accuracy\"], tablefmt='psql', showindex=False))\n",
    "\n",
    "#  do_model(GaussianNB(), np_iris[\"train_norm\"][:, :4], np_iris[\"train_norm\"][:, -1], np_iris[\"test_norm\"][:, :4], np_iris[\"test_norm\"][:, -1], \"Iris: GaussianNB Normalized\" ) \n",
    "#  print()\n",
    "#  print(tabulate(l_history, headers=[\"Category\", \"Accuracy\"], tablefmt='psql', showindex=False))\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f815d9-6eb7-4aaa-863c-3c7ce613d127",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#  Step A5: Iris Data train, test .. Naive Bayes, Multinomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de942e0e-31d0-44c1-bdb3-0ee3c5ca3315",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "#  Naive Bayes, Multinomial\n",
    "#\n",
    "\n",
    "do_model(MultinomialNB(), np_iris[\"train\"][:, :4], np_iris[\"train\"][:, -1], np_iris[\"test\"][:, :4], np_iris[\"test\"][:, -1], \"Iris: MultinomialNB\" ) \n",
    "print()\n",
    "print(tabulate(l_history, headers=[\"Category\", \"Accuracy\"], tablefmt='psql', showindex=False))\n",
    "\n",
    "#  do_model(MultinomialNB(), np_iris[\"train_norm\"][:, :4], np_iris[\"train_norm\"][:, -1], np_iris[\"test_norm\"][:, :4], np_iris[\"test_norm\"][:, -1], \"Iris: MultinomialNB Normalized\" ) \n",
    "#  print()\n",
    "#  print(tabulate(l_history, headers=[\"Category\", \"Accuracy\"], tablefmt='psql', showindex=False))\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662897e8-2b96-4d7a-bf24-74aa2da5c006",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#  Step A6: Iris Data train, test .. Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4adb7874-3d2c-4f27-83db-147b953228c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "#  Decision Tree\n",
    "#\n",
    "\n",
    "do_model(DecisionTreeClassifier(), np_iris[\"train\"][:, :4], np_iris[\"train\"][:, -1], np_iris[\"test\"][:, :4], np_iris[\"test\"][:, -1], \"Iris: DecisionTree\" ) \n",
    "print()\n",
    "print(tabulate(l_history, headers=[\"Category\", \"Accuracy\"], tablefmt='psql', showindex=False))\n",
    "\n",
    "#  do_model(DecisionTreeClassifier(), np_iris[\"train_norm\"][:, :4], np_iris[\"train_norm\"][:, -1], np_iris[\"test_norm\"][:, :4], np_iris[\"test_norm\"][:, -1], \"Iris: DecisionTree Normalized\" ) \n",
    "#  print()\n",
    "#  print(tabulate(l_history, headers=[\"Category\", \"Accuracy\"], tablefmt='psql', showindex=False))\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f5d6be-ecfa-4ca1-b518-0b37274c04de",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#  Step A7: Iris Data train, test .. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0a74fa-57c5-44ef-ade3-dd5438751064",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#  Random Forest\n",
    "#\n",
    "\n",
    "#  n_estimators, number of random trees created and trained\n",
    "#\n",
    "\n",
    "do_model(RandomForestClassifier(n_estimators = 5), np_iris[\"train\"][:, :4], np_iris[\"train\"][:, -1], np_iris[\"test\"][:, :4], np_iris[\"test\"][:, -1], \"Iris: RandomForest\" ) \n",
    "print()\n",
    "print(tabulate(l_history, headers=[\"Category\", \"Accuracy\"], tablefmt='psql', showindex=False))\n",
    "\n",
    "#  do_model(RandomForestClassifier(n_estimators = 5), np_iris[\"train_norm\"][:, :4], np_iris[\"train_norm\"][:, -1], np_iris[\"test_norm\"][:, :4], np_iris[\"test_norm\"][:, -1], \"Iris: RandomForest Normalized\" ) \n",
    "#  print()\n",
    "#  print(tabulate(l_history, headers=[\"Category\", \"Accuracy\"], tablefmt='psql', showindex=False))\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7973bf-5a1a-4999-810d-91e52c13bd0b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#  Step A8: Iris Data train, test .. Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2dc0262-190c-455d-b2f9-99056c1bea18",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "#  Support Vector Machine\n",
    "#\n",
    "#  We run this one with a number of configurations ..\n",
    "#\n",
    "#     C      ==  margin constant\n",
    "#     gamma  ==  used by the Gaussian kernel\n",
    "#\n",
    "\n",
    "do_model(SVC(kernel = \"linear\", C = 1.0), np_iris[\"train\"][:, :4], np_iris[\"train\"][:, -1], np_iris[\"test\"][:, :4], np_iris[\"test\"][:, -1], \"Iris: SVC/Linear\" ) \n",
    "print()\n",
    "print(tabulate(l_history, headers=[\"Category\", \"Accuracy\"], tablefmt='psql', showindex=False))\n",
    "print()\n",
    "             \n",
    "#  do_model(SVC(kernel = \"linear\", C = 1.0), np_iris[\"train_norm\"][:, :4], np_iris[\"train_norm\"][:, -1], np_iris[\"test_norm\"][:, :4], np_iris[\"test_norm\"][:, -1], \"Iris: SVC/Linear Normalized\" ) \n",
    "#  print()\n",
    "#  print(tabulate(l_history, headers=[\"Category\", \"Accuracy\"], tablefmt='psql', showindex=False))\n",
    "#  print()\n",
    "\n",
    "             \n",
    "do_model(SVC(kernel = \"rbf\", C = 1.0, gamma = 0.25), np_iris[\"train\"][:, :4], np_iris[\"train\"][:, -1], np_iris[\"test\"][:, :4], np_iris[\"test\"][:, -1], \"Iris: RBF\" ) \n",
    "print()\n",
    "print(tabulate(l_history, headers=[\"Category\", \"Accuracy\"], tablefmt='psql', showindex=False))\n",
    "print()\n",
    "             \n",
    "#  do_model(SVC(kernel = \"rbf\", C = 1.0, gamma = 0.25), np_iris[\"train_norm\"][:, :4], np_iris[\"train_norm\"][:, -1], np_iris[\"test_norm\"][:, :4], np_iris[\"test_norm\"][:, -1], \"Iris: RBF Normalized\" ) \n",
    "#  print()\n",
    "#  print(tabulate(l_history, headers=[\"Category\", \"Accuracy\"], tablefmt='psql', showindex=False))\n",
    "#  print()\n",
    "\n",
    "             \n",
    "do_model(SVC(kernel = \"rbf\", C = 1.0, gamma = 0.001), np_iris[\"train\"][:, :4], np_iris[\"train\"][:, -1], np_iris[\"test\"][:, :4], np_iris[\"test\"][:, -1], \"Iris: RBF 2\" ) \n",
    "print()\n",
    "print(tabulate(l_history, headers=[\"Category\", \"Accuracy\"], tablefmt='psql', showindex=False))\n",
    "print()\n",
    "\n",
    "#  do_model(SVC(kernel = \"rbf\", C = 1.0, gamma = 0.001), np_iris[\"train_norm\"][:, :4], np_iris[\"train_norm\"][:, -1], np_iris[\"test_norm\"][:, :4], np_iris[\"test_norm\"][:, -1], \"Iris: RBF 2 Normalized\" ) \n",
    "#  print()\n",
    "#  print(tabulate(l_history, headers=[\"Category\", \"Accuracy\"], tablefmt='psql', showindex=False))\n",
    "#  print()\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc4cf6a-434a-4fcf-bfdb-edc3308c5252",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#  Step B1:  Breast Cancer Data load, encode, normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafe262a-4fb5-4954-9d6f-c8cbea09d763",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Read the Breast Cancer data into a Pandas Dataframe\n",
    "#\n",
    "#     Features\n",
    "#     1)     ID number\n",
    "#     2)     Diagnosis (M = malignant, B = benign)\n",
    "#     3-32)\n",
    "#       Ten real-valued features are computed for each cell nucleus:\n",
    "#     \n",
    "#     \ta) radius (mean of distances from center to points on the perimeter)\n",
    "#     \tb) texture (standard deviation of gray-scale values)\n",
    "#     \tc) perimeter\n",
    "#     \td) area\n",
    "#     \te) smoothness (local variation in radius lengths)\n",
    "#     \tf) compactness (perimeter^2 / area - 1.0)\n",
    "#     \tg) concavity (severity of concave portions of the contour)\n",
    "#     \th) concave points (number of concave portions of the contour)\n",
    "#     \ti) symmetry \n",
    "#     \tj) fractal dimension (\"coastline approximation\" - 1)\n",
    "#\n",
    "#  Sample data line,\n",
    "#     842302,M,\n",
    "#     17.99,    10.38,    122.8,    1001,    0.1184,    0.2776,    0.3001,    0.1471,    0.2419,    0.07871,         #  10 count\n",
    "#     1.095,    0.9053,   8.589,    153.4,   0.006399,  0.04904,   0.05373,   0.01587,   0.03003,   0.006193,\n",
    "#     25.38,    17.33,    184.6,    2019,    0.1622,    0.6656,    0.7119,    0.2654,    0.4601     ,0.1189\n",
    "#\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "my_le = LabelEncoder()\n",
    "   #\n",
    "l_folder = \"20_Data\"\n",
    "l_file   = \"22_wdbc.data.txt\"\n",
    "\n",
    "\n",
    "pd_bc  = pd.read_csv((l_folder + \"/\" + l_file), header = 0, sep = \",\",\n",
    "   names = [\"id\", \"class\",\n",
    "            \"f01\", \"f02\", \"f03\", \"f04\", \"f05\", \"f06\", \"f07\", \"f08\", \"f09\", \"f10\", \n",
    "            \"f11\", \"f12\", \"f13\", \"f14\", \"f15\", \"f16\", \"f17\", \"f18\", \"f19\", \"f20\", \n",
    "            \"f21\", \"f22\", \"f23\", \"f24\", \"f25\", \"f26\", \"f27\", \"f28\", \"f29\", \"f30\", \n",
    "           ],\n",
    "   dtype = {\"id\": \"int\", \"class\": \"string\",\n",
    "            \"f01\": \"float\", \"f02\": \"float\", \"f03\": \"float\", \"f04\": \"float\", \"f05\": \"float\", \"f06\": \"float\", \"f07\": \"float\", \"f08\": \"float\", \"f09\": \"float\", \"f10\": \"float\", \n",
    "            \"f11\": \"float\", \"f12\": \"float\", \"f13\": \"float\", \"f14\": \"float\", \"f15\": \"float\", \"f16\": \"float\", \"f17\": \"float\", \"f18\": \"float\", \"f19\": \"float\", \"f20\": \"float\", \n",
    "            \"f21\": \"float\", \"f22\": \"float\", \"f23\": \"float\", \"f24\": \"float\", \"f25\": \"float\", \"f26\": \"float\", \"f27\": \"float\", \"f28\": \"float\", \"f29\": \"float\", \"f30\": \"float\", \n",
    "           } )\n",
    "      #\n",
    "pd_bc[\"class_encoded\"]  =  my_le.fit_transform(pd_bc[\"class\"])\n",
    "   #\n",
    "pd_bc = pd_bc.drop([\"class\", \"id\"], axis = 1)\n",
    "    \n",
    "    \n",
    "#  Pandas.Dataframe.sample() returns a randomized count of rows\n",
    "#\n",
    "print(tabulate(pd_bc.sample(5), headers='keys', tablefmt='psql', showindex=False))\n",
    "print(\"Number of rows: %d\" % (len(pd_bc)))\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe05409-32ac-4da0-bd05-6d97b35df9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Split data into training and test.\n",
    "#  Convert the data into numpy arrays, since the ml libraries we use later expect that.\n",
    "#\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np_bc = {}\n",
    "   #\n",
    "np_bc[\"train\"], np_bc[\"test\"] = train_test_split(pd_bc.to_numpy(),                    #  random_state calls to shuffle the data,\n",
    "   test_size = 0.20, random_state = 40)                                               #    which had arrived sorted\n",
    "                                                                                      #  10% yields way too high of an accuracy\n",
    "                                                                                      #    far below\n",
    "print(\"Number of total rows: %d   Training rows: %d   Test rows: %d\" %\n",
    "  (len(pd_bc), len(np_bc[\"train\"]), len(np_bc[\"test\"])) )\n",
    "\n",
    "print()\n",
    "print(\"Train data:\")\n",
    "print(\"%s\" % (np_bc[\"train\"][0:5]))\n",
    "print()\n",
    "print(\"Test  data:\")\n",
    "print(\"%s\" % (np_bc[\"test\" ][0:5]))\n",
    "print()\n",
    "   #\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f885ac6-db00-4fd0-9ef3-c7b5bd130542",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Normalizing the data\n",
    "#\n",
    "\n",
    "def my_normalize(X, x_min, x_max):\n",
    "   nom = (X-X.min(axis=0))*(x_max-x_min)\n",
    "   denom = X.max(axis=0) - X.min(axis=0)\n",
    "   denom[denom==0] = 1\n",
    "   return x_min + nom/denom \n",
    "\n",
    "\n",
    "print(\"Number of columns in matrix: %d\" % (np_bc[\"train\"].shape[1]))\n",
    "      \n",
    "#  If we normalize the \"class\" column, we lose the categorical nature\n",
    "#  of that data. So, create a deep copy, then just normalize the non-\n",
    "#  class columns.\n",
    "#\n",
    "np_bc[\"train_norm\"] = np.copy(np_bc[\"train\"])\n",
    "np_bc[\"test_norm\" ] = np.copy(np_bc[\"test\" ])\n",
    "   #\n",
    "np_bc[\"train_norm\"][:, :30] = my_normalize(np_bc[\"train_norm\"][:, :30], 0, 1)\n",
    "np_bc[\"test_norm\" ][:, :30] = my_normalize(np_bc[\"test_norm\" ][:, :30], 0, 1)\n",
    "\n",
    "plt.boxplot(np_bc[\"train\"     ])\n",
    "plt.show()\n",
    "plt.boxplot(np_bc[\"train_norm\"])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a63d6f-96d6-42c3-9782-8be69c555bef",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#  Step B2:  Breast Cancer Data, run against all models .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf36b4ec-23da-4af1-8cb4-fe5669cc5cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  All of our model libraries were imported above, but ..\n",
    "#\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "#  To add a blank line to history-\n",
    "#\n",
    "l_history.add(event = \"\", measure = \"\")\n",
    "\n",
    "\n",
    "#  Our numpy array has many columns, with the last column being the class.\n",
    "#\n",
    "#  To review numpy array slicing,\n",
    "#\n",
    "#     To get the first 30 columns use,\n",
    "#        np_iris[\"train\"][:, :30]\n",
    "#     To get the last column use,\n",
    "#        np_iris[\"train\"][:, -1]\n",
    "#\n",
    "\n",
    "do_model(NearestCentroid(), np_bc[\"train\"][:, :30], np_bc[\"train\"][:, -1], np_bc[\"test\"][:, :30], np_bc[\"test\"][:, -1], \"Breast Cancer: Centroid\") \n",
    "#  do_model(NearestCentroid(), np_bc[\"train_norm\"][:, :30], np_bc[\"train_norm\"][:, -1], np_bc[\"test_norm\"][:, :30], np_bc[\"test_norm\"][:, -1], \"Breast Cancer: Centroid Normalized\") \n",
    "print()\n",
    "\n",
    "do_model(KNeighborsClassifier(n_neighbors = 3), np_bc[\"train\"][:, :30], np_bc[\"train\"][:, -1], np_bc[\"test\"][:, :30], np_bc[\"test\"][:, -1], \"Breast Cancer: kNN=3\") \n",
    "#  do_model(KNeighborsClassifier(n_neighbors = 3), np_bc[\"train_norm\"][:, :30], np_bc[\"train_norm\"][:, -1], np_bc[\"test_norm\"][:, :30], np_bc[\"test_norm\"][:, -1], \"Breast Cancer: kNN=3 Normalized\") \n",
    "print()\n",
    "\n",
    "do_model(GaussianNB(), np_bc[\"train\"][:, :30], np_bc[\"train\"][:, -1], np_bc[\"test\"][:, :30], np_bc[\"test\"][:, -1], \"Breast Cancer: GaussianNB\") \n",
    "#  do_model(GaussianNB(), np_bc[\"train_norm\"][:, :30], np_bc[\"train_norm\"][:, -1], np_bc[\"test_norm\"][:, :30], np_bc[\"test_norm\"][:, -1], \"Breast Cancer: GaussianNB Normalized\") \n",
    "print()\n",
    "\n",
    "do_model(MultinomialNB(), np_bc[\"train\"][:, :30], np_bc[\"train\"][:, -1], np_bc[\"test\"][:, :30], np_bc[\"test\"][:, -1], \"Breast Cancer: MultinomialNB\") \n",
    "#  do_model(MultinomialNB(), np_bc[\"train_norm\"][:, :30], np_bc[\"train_norm\"][:, -1], np_bc[\"test_norm\"][:, :30], np_bc[\"test_norm\"][:, -1], \"Breast Cancer: MultinomialNB Normalized\") \n",
    "print()\n",
    "\n",
    "do_model(DecisionTreeClassifier(), np_bc[\"train\"][:, :30], np_bc[\"train\"][:, -1], np_bc[\"test\"][:, :30], np_bc[\"test\"][:, -1], \"Breast Cancer: DecisionTree\") \n",
    "#  do_model(DecisionTreeClassifier(), np_bc[\"train_norm\"][:, :30], np_bc[\"train_norm\"][:, -1], np_bc[\"test_norm\"][:, :30], np_bc[\"test_norm\"][:, -1], \"Breast Cancer: DecisionTree Normalized\") \n",
    "print()\n",
    "\n",
    "#  n_estimators, number of random trees created and trained\n",
    "#\n",
    "do_model(RandomForestClassifier(n_estimators = 5), np_bc[\"train\"][:, :30], np_bc[\"train\"][:, -1], np_bc[\"test\"][:, :30], np_bc[\"test\"][:, -1], \"Breast Cancer: Random Forest = 5\") \n",
    "#  do_model(RandomForestClassifier(n_estimators = 5), np_bc[\"train_norm\"][:, :30], np_bc[\"train_norm\"][:, -1], np_bc[\"test_norm\"][:, :30], np_bc[\"test_norm\"][:, -1], \"Breast Cancer: Random Forest = 5 Normalized\") \n",
    "print()\n",
    "\n",
    "   ###\n",
    "\n",
    "#  Support Vector Machine\n",
    "#\n",
    "#  We run this one with a number of configurations ..\n",
    "#\n",
    "#     C      ==  margin constant\n",
    "#     gamma  ==  used by the Gaussian kernel\n",
    "#\n",
    "\n",
    "do_model(SVC(kernel = \"linear\", C = 1.0), np_bc[\"train\"][:, :30], np_bc[\"train\"][:, -1], np_bc[\"test\"][:, :30], np_bc[\"test\"][:, -1], \"Breast Cancer: SVC/Linear\") \n",
    "#  do_model(SVC(kernel = \"linear\", C = 1.0), np_bc[\"train_norm\"][:, :30], np_bc[\"train_norm\"][:, -1], np_bc[\"test_norm\"][:, :30], np_bc[\"test_norm\"][:, -1], \"Breast Cancer: SVC/Linear Normalized\") \n",
    "print()\n",
    "\n",
    "do_model(SVC(kernel = \"rbf\", C = 1.0, gamma = 1.0), np_bc[\"train\"][:, :30], np_bc[\"train\"][:, -1], np_bc[\"test\"][:, :30], np_bc[\"test\"][:, -1], \"Breast Cancer: RBF\") \n",
    "#  do_model(SVC(kernel = \"rbf\", C = 1.0, gamma = 1.0), np_bc[\"train_norm\"][:, :30], np_bc[\"train_norm\"][:, -1], np_bc[\"test_norm\"][:, :30], np_bc[\"test_norm\"][:, -1], \"Breast Cancer: RBF Normalized\") \n",
    "print()\n",
    "\n",
    "do_model(SVC(kernel = \"rbf\", C = 1.0, gamma = 0.001), np_bc[\"train\"][:, :30], np_bc[\"train\"][:, -1], np_bc[\"test\"][:, :30], np_bc[\"test\"][:, -1], \"Breast Cancer: RBF 2\") \n",
    "#  do_model(SVC(kernel = \"rbf\", C = 1.0, gamma = 0.001), np_bc[\"train_norm\"][:, :30], np_bc[\"train_norm\"][:, -1], np_bc[\"test_norm\"][:, :30], np_bc[\"test_norm\"][:, -1], \"Breast Cancer: RBF 2 Normalized\") \n",
    "print()\n",
    "\n",
    "\n",
    "print()\n",
    "print(tabulate(l_history, headers=[\"Category\", \"Accuracy\"], tablefmt='psql', showindex=False))\n",
    "print()\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25592f04-c344-4a58-8c1d-1d8684656a40",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#  Step C1:  MNist Data load, encode, other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09eb083d-60ba-4c42-b65c-f8f7c0864b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  We will be using Keras, so pip install it inside the Jupyter NoteBook container ..\n",
    "#\n",
    "\n",
    "l_package1 = \"keras\"\n",
    "l_package2 = \"tensorflow\"\n",
    "    \n",
    "def my_func(arg1):\n",
    "    \n",
    "   import sys\n",
    "   import subprocess\n",
    "    \n",
    "   subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", arg1 ])\n",
    "      #\n",
    "   print(\"From each host ..\")\n",
    "      #\n",
    "   return\n",
    "\n",
    "\n",
    "   ##########################################\n",
    "    \n",
    "\n",
    "print(\"Install Python Pip packages on Jupyter container ..\")\n",
    "   #\n",
    "my_return = my_func(l_package1)\n",
    "my_return = my_func(l_package2)\n",
    "print()\n",
    "    \n",
    "\n",
    "#  Use this if installing o nthe KGIP worker nodes ..\n",
    "#\n",
    "#  print(\"Install Python Pip packages on KGIP worker node containers ..\")\n",
    "#     # \n",
    "#  my_return = my_graph.run(lambda g: my_func(l_package))\n",
    "#  print()\n",
    "    \n",
    "print(\"--\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f9a359-02b1-4bb0-aced-216eb7cad1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Intead of loading MNist from disk, we load it from the Keras library ..\n",
    "#\n",
    "\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "\n",
    "\n",
    "np_mnist = {}\n",
    "   #\n",
    "(np_mnist[\"train\"], np_mnist[\"train_label\"]), (np_mnist[\"test\"], np_mnist[\"test_label\"]) = mnist.load_data()\n",
    "\n",
    "\n",
    "#  train and test ccome in as an array [(n), 28, 28] where n == 60000 for train,\n",
    "#  and 100000 for test\n",
    "#\n",
    "#  We need that 28*28 as a vector, so ..\n",
    "#\n",
    "np_mnist[\"train_v\"] = np_mnist[\"train\"].reshape(-1, 28*28)\n",
    "np_mnist[\"test_v\"]  = np_mnist[\"test\" ].reshape(-1, 28*28)\n",
    "\n",
    "\n",
    "print(\"Train shape ................ %s\" % (str(np_mnist[\"train\"].shape)))\n",
    "print(\"Train label shape .......... %s\" % (str(np_mnist[\"train_label\"].shape)))\n",
    "   #\n",
    "print(\"Test  shape ................ %s\" % (str(np_mnist[\"test\"].shape)))\n",
    "print(\"Test  label shape .......... %s\" % (str(np_mnist[\"test_label\"].shape)))\n",
    "   #\n",
    "print(\"Train vector shape ......... %s\" % (str(np_mnist[\"train_v\"].shape)))\n",
    "print(\"Test  vector shape ......... %s\" % (str(np_mnist[\"test_v\" ].shape)))\n",
    "   #\n",
    "print()\n",
    "\n",
    "\n",
    "#  tabulate() displays poorly with this wide data. Straight up print() works well.\n",
    "#\n",
    "# print(tabulate(np_mnist[\"train\"][0:2], headers='keys', tablefmt='psql', showindex=False))\n",
    "print(np_mnist[\"train\"][0:2])\n",
    "print(\"Number of rows: %d\" % (len(np_mnist[\"train\"])))\n",
    "   #\n",
    "print()\n",
    "\n",
    "print(np_mnist[\"train_label\"][0:2])\n",
    "print(\"Number of rows: %d\" % (len(np_mnist[\"train_label\"])))\n",
    "   #\n",
    "print()\n",
    "\n",
    "\n",
    "print(np_mnist[\"test\"][0:2])\n",
    "print(\"Number of rows: %d\" % (len(np_mnist[\"test\"])))\n",
    "   #\n",
    "print()\n",
    "\n",
    "print(np_mnist[\"test_label\"][0:2])\n",
    "print(\"Number of rows: %d\" % (len(np_mnist[\"test_label\"])))\n",
    "   #\n",
    "print()\n",
    "\n",
    "    \n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c425b03-8d9c-4208-af6b-9f6115104b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Sample results\n",
    "#\n",
    "#     Train shape ................ (60000, 28, 28)\n",
    "#     Train label shape .......... (60000,)\n",
    "#     Test  shape ................ (10000, 28, 28)\n",
    "#     Test  label shape .......... (10000,)\n",
    "#     Train vector shape ......... (60000, 784)\n",
    "#     Test  vector shape ......... (10000, 784)\n",
    "#     \n",
    "#     [[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   3  18  18  18 126 136 175  26 166 255 247 127   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0  30  36  94 154 170 253 253 253 253 253 225 172 253 242 195  64   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0  49 238 253 253 253 253 253 253 253 253 251  93  82  82  56  39   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0  18 219 253 253 253 253 253 198 182 247 241   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0  80 156 107 253 253 205  11   0  43 154   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0  14   1 154 253  90   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0 139 253 190   2   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0  11 190 253  70   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0  35 241 225 160 108   1   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0  81 240 253 253 119  25   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  45 186 253 253 150  27   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  16  93 252 253 187   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 249 253 249  64   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  46 130 183 253 253 207   2   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0  39 148 229 253 253 253 250 182   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0  24 114 221 253 253 253 253 201  78   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0  23  66 213 253 253 253 253 198  81   2   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0  18 171 219 253 253 253 253 195  80   9   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0  55 172 226 253 253 253 253 244 133  11   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0 136 253 253 253 212 135 132  16   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]]\n",
    "#     \n",
    "#      [[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  51 159 253 159  50   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  48 238 252 252 252 237   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0  54 227 253 252 239 233 252  57   6   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0  10  60 224 252 253 252 202  84 252 253 122   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0 163 252 252 252 253 252 252  96 189 253 167   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0  51 238 253 253 190 114 253 228  47  79 255 168   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0  48 238 252 252 179  12  75 121  21   0   0 253 243  50   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0  38 165 253 233 208  84   0   0   0   0   0   0 253 252 165   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   7 178 252 240  71  19  28   0   0   0   0   0   0 253 252 195   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0  57 252 252  63   0   0   0   0   0   0   0   0   0 253 252 195   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0 198 253 190   0   0   0   0   0   0   0   0   0   0 255 253 196   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0  76 246 252 112   0   0   0   0   0   0   0   0   0   0 253 252 148   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0  85 252 230  25   0   0   0   0   0   0   0   0   7 135 253 186  12   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0  85 252 223   0   0   0   0   0   0   0   0   7 131 252 225  71   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0  85 252 145   0   0   0   0   0   0   0  48 165 252 173   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0  86 253 225   0   0   0   0   0   0 114 238 253 162   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0  85 252 249 146  48  29  85 178 225 253 223 167  56   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0  85 252 252 252 229 215 252 252 252 196 130   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0  28 199 252 252 253 252 252 233 145   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0  25 128 252 253 252 141  37   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]]]\n",
    "#     Number of rows: 60000\n",
    "#     \n",
    "#     [5 0]\n",
    "#     Number of rows: 60000\n",
    "#     \n",
    "#     [[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0  84 185 159 151  60  36   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0 222 254 254 254 254 241 198 198 198 198 198 198 198 198 170  52   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0  67 114  72 114 163 227 254 225 254 254 254 250 229 254 254 140   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0  17  66  14  67  67  67  59  21 236 254 106   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  83 253 209  18   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  22 233 255  83   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 129 254 238  44   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  59 249 254  62   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 133 254 187   5   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   9 205 248  58   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 126 254 182   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  75 251 240  57   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0  19 221 254 166   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   3 203 254 219  35   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0  38 254 254  77   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0  31 224 254 115   1   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0 133 254 254  52   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0  61 242 254 254  52   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0 121 254 254 219  40   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0 121 254 207  18   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]]\n",
    "#     \n",
    "#      [[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0 116 125 171 255 255 150  93   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0 169 253 253 253 253 253 253 218  30   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0 169 253 253 253 213 142 176 253 253 122   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0  52 250 253 210  32  12   0   6 206 253 140   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0  77 251 210  25   0   0   0 122 248 253  65   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0  31  18   0   0   0   0 209 253 253  65   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0 117 247 253 198  10   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0  76 247 253 231  63   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0 128 253 253 144   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0 176 246 253 159  12   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0  25 234 253 233  35   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0 198 253 253 141   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0  78 248 253 189  12   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0  19 200 253 253 141   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0 134 253 253 173  12   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0 248 253 253  25   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0 248 253 253  43  20  20  20  20   5   0   5  20  20  37 150 150 150 147  10   0]\n",
    "#       [  0   0   0   0   0   0   0   0 248 253 253 253 253 253 253 253 168 143 166 253 253 253 253 253 253 253 123   0]\n",
    "#       [  0   0   0   0   0   0   0   0 174 253 253 253 253 253 253 253 253 253 253 253 249 247 247 169 117 117  57   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0 118 123 123 123 166 253 253 253 155 123 123  41   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]]]\n",
    "#     Number of rows: 10000\n",
    "#     \n",
    "#     [7 2]\n",
    "#     Number of rows: 10000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6cda4d-95a6-46ac-8871-aa7826962680",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#  Step C2: MNist train, test .. (All)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6bcf95-6f46-448e-a618-4bf0fb15e098",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Sometimes we want to clear history-\n",
    "#\n",
    "\n",
    "l_history.clear()\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97564456-56d7-4f55-b4d8-94ec6ddde2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  All of our model libraries were imported above, but ..\n",
    "#\n",
    "\n",
    "#  Here we run given ML routines against the MNist data set\n",
    "#\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "#  Adding these to the above\n",
    "#\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import decomposition\n",
    "\n",
    "import time\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e883ec4-3c77-43cd-8d71-588f58f12503",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  To add a blank line to history-\n",
    "#\n",
    "l_history.add(event = \"\", measure = \"\")\n",
    "\n",
    "\n",
    "do_model(NearestCentroid(), np_mnist[\"train_v\"], np_mnist[\"train_label\"], np_mnist[\"test_v\"], np_mnist[\"test_label\"], \"MNist: Centroid\") \n",
    "print()\n",
    "\n",
    "do_model(KNeighborsClassifier(n_neighbors =  3), np_mnist[\"train_v\"], np_mnist[\"train_label\"], np_mnist[\"test_v\"], np_mnist[\"test_label\"], \"MNist: kNN=3\" ) \n",
    "do_model(KNeighborsClassifier(n_neighbors =  7), np_mnist[\"train_v\"], np_mnist[\"train_label\"], np_mnist[\"test_v\"], np_mnist[\"test_label\"], \"MNist: kNN=7\") \n",
    "print()\n",
    "\n",
    "do_model(GaussianNB(), np_mnist[\"train_v\"], np_mnist[\"train_label\"], np_mnist[\"test_v\"], np_mnist[\"test_label\"], \"MNist: GaussianNB\") \n",
    "print()\n",
    "\n",
    "do_model(MultinomialNB(), np_mnist[\"train_v\"], np_mnist[\"train_label\"], np_mnist[\"test_v\"], np_mnist[\"test_label\"], \"MNist: MultinomialNB\") \n",
    "print()\n",
    "\n",
    "do_model(DecisionTreeClassifier(), np_mnist[\"train_v\"], np_mnist[\"train_label\"], np_mnist[\"test_v\"], np_mnist[\"test_label\"], \"MNist: DecisionTree\") \n",
    "print()\n",
    "\n",
    "#  n_estimators, number of random trees created and trained\n",
    "#\n",
    "do_model(RandomForestClassifier(n_estimators = 5   ), np_mnist[\"train_v\"], np_mnist[\"train_label\"], np_mnist[\"test_v\"], np_mnist[\"test_label\"], \"MNist: Random Forest = 5   \") \n",
    "do_model(RandomForestClassifier(n_estimators = 50  ), np_mnist[\"train_v\"], np_mnist[\"train_label\"], np_mnist[\"test_v\"], np_mnist[\"test_label\"], \"MNist: Random Forest = 50  \") \n",
    "do_model(RandomForestClassifier(n_estimators = 500 ), np_mnist[\"train_v\"], np_mnist[\"train_label\"], np_mnist[\"test_v\"], np_mnist[\"test_label\"], \"MNist: Random Forest = 500 \") \n",
    "do_model(RandomForestClassifier(n_estimators = 5000), np_mnist[\"train_v\"], np_mnist[\"train_label\"], np_mnist[\"test_v\"], np_mnist[\"test_label\"], \"MNist: Random Forest = 5000\") \n",
    "\n",
    "print()\n",
    "\n",
    "   ###\n",
    "\n",
    "#  Support Vector Machine\n",
    "#\n",
    "#  We run this one with a number of configurations ..\n",
    "#\n",
    "#     C      ==  margin constant\n",
    "#     gamma  ==  used by the Gaussian kernel\n",
    "#\n",
    "\n",
    "#  As configured, these throw warnings, never settle ..\n",
    "#\n",
    "\n",
    "#  do_model(LinearSVC(C = 0.01), np_mnist[\"train_v\"], np_mnist[\"train_label\"], np_mnist[\"test_v\"], np_mnist[\"test_label\"], \"MNist: LinearSVC c=0.01   \") \n",
    "#  print()\n",
    "#  do_model(LinearSVC(C = 0.1 ), np_mnist[\"train_v\"], np_mnist[\"train_label\"], np_mnist[\"test_v\"], np_mnist[\"test_label\"], \"MNist: LinearSVC c=0.1    \") \n",
    "#  print()\n",
    "#  do_model(LinearSVC(C = 1.0 ), np_mnist[\"train_v\"], np_mnist[\"train_label\"], np_mnist[\"test_v\"], np_mnist[\"test_label\"], \"MNist: LinearSVC c=1.0    \") \n",
    "#  print()\n",
    "#  do_model(LinearSVC(C = 10.0), np_mnist[\"train_v\"], np_mnist[\"train_label\"], np_mnist[\"test_v\"], np_mnist[\"test_label\"], \"MNist: LinearSVC c=10.0   \") \n",
    "#  print()\n",
    "\n",
    "print()\n",
    "print(tabulate(l_history, headers=[\"Category\", \"Accuracy\"], tablefmt='psql', showindex=False))\n",
    "print()\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1b1c44-a03b-4965-9f81-c838e23136ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Sample results\n",
    "#\n",
    "#     MNist: Centroid ...\n",
    "#        Actual    labels from test......... [7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4 9 6 6 5 4 0 7 4 0 1 ... 5 2 4 9 4 3 6 4 1 7 2 6 5 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6]\n",
    "#        Predicted labels from test......... [7 2 1 0 4 1 4 9 2 9 0 2 9 0 1 5 9 7 3 4 7 6 4 5 4 0 7 4 0 1 ... 3 2 4 9 4 2 6 4 1 7 0 6 6 0 1 8 8 4 5 6 7 8 4 0 1 2 3 4 5 6]\n",
    "#        ###\n",
    "#        Accuracy: 82.0300 %\n",
    "#     \n",
    "#     MNist: kNN=3 ...\n",
    "#        Actual    labels from test......... [7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4 9 6 6 5 4 0 7 4 0 1 ... 5 2 4 9 4 3 6 4 1 7 2 6 5 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6]\n",
    "#        Predicted labels from test......... [7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4 9 6 6 5 4 0 7 4 0 1 ... 5 2 4 9 4 3 6 4 1 7 2 6 3 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6]\n",
    "#        ###\n",
    "#        Accuracy: 97.0500 %\n",
    "#     MNist: kNN=7 ...\n",
    "#        Actual    labels from test......... [7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4 9 6 6 5 4 0 7 4 0 1 ... 5 2 4 9 4 3 6 4 1 7 2 6 5 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6]\n",
    "#        Predicted labels from test......... [7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4 9 6 6 5 4 0 7 4 0 1 ... 5 2 4 9 4 3 6 4 1 7 2 6 6 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6]\n",
    "#        ###\n",
    "#        Accuracy: 96.9400 %\n",
    "#     \n",
    "#     MNist: GaussianNB ...\n",
    "#        Actual    labels from test......... [7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4 9 6 6 5 4 0 7 4 0 1 ... 5 2 4 9 4 3 6 4 1 7 2 6 5 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6]\n",
    "#        Predicted labels from test......... [9 2 1 0 9 1 8 9 4 9 0 6 9 0 1 0 9 7 2 9 9 6 6 8 9 0 7 9 0 1 ... 6 0 8 9 8 8 6 9 1 9 3 6 6 0 1 9 8 9 8 6 9 8 9 0 1 8 8 9 8 6]\n",
    "#        ###\n",
    "#        Accuracy: 55.5800 %\n",
    "#     \n",
    "#     MNist: MultinomialNB ...\n",
    "#        Actual    labels from test......... [7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4 9 6 6 5 4 0 7 4 0 1 ... 5 2 4 9 4 3 6 4 1 7 2 6 5 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6]\n",
    "#        Predicted labels from test......... [7 2 1 0 4 1 4 9 4 9 0 6 9 0 1 3 9 7 3 4 9 6 6 5 4 0 7 4 0 1 ... 2 2 4 9 4 2 6 4 1 7 2 6 6 0 1 8 8 4 5 6 7 8 9 0 1 2 3 9 8 6]\n",
    "#        ###\n",
    "#        Accuracy: 83.6500 %\n",
    "#     \n",
    "#     MNist: DecisionTree ...\n",
    "#        Actual    labels from test......... [7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4 9 6 6 5 4 0 7 4 0 1 ... 5 2 4 9 4 3 6 4 1 7 2 6 5 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6]\n",
    "#        Predicted labels from test......... [7 2 1 0 4 1 4 7 6 9 0 6 9 0 1 5 9 7 6 4 9 6 6 5 4 0 7 4 0 1 ... 5 2 4 9 4 3 8 4 1 7 5 6 8 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6]\n",
    "#        ###\n",
    "#        Accuracy: 87.6700 %\n",
    "#     \n",
    "#     MNist: Random Forest = 5    ...\n",
    "#        Actual    labels from test......... [7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4 9 6 6 5 4 0 7 4 0 1 ... 5 2 4 9 4 3 6 4 1 7 2 6 5 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6]\n",
    "#        Predicted labels from test......... [7 2 1 0 4 1 4 9 4 9 0 6 9 0 1 5 9 7 3 4 9 6 6 5 4 0 7 4 0 1 ... 5 2 4 9 4 2 6 4 1 7 8 6 6 0 1 2 3 4 5 6 7 3 9 0 1 2 3 4 5 6]\n",
    "#        ###\n",
    "#        Accuracy: 91.9100 %\n",
    "#     MNist: Random Forest = 50   ...\n",
    "#        Actual    labels from test......... [7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4 9 6 6 5 4 0 7 4 0 1 ... 5 2 4 9 4 3 6 4 1 7 2 6 5 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6]\n",
    "#        Predicted labels from test......... [7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4 9 6 6 5 4 0 7 4 0 1 ... 5 2 4 9 4 2 6 4 1 7 3 6 3 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6]\n",
    "#        ###\n",
    "#        Accuracy: 96.7000 %\n",
    "#     MNist: Random Forest = 500  ...\n",
    "#        Actual    labels from test......... [7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4 9 6 6 5 4 0 7 4 0 1 ... 5 2 4 9 4 3 6 4 1 7 2 6 5 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6]\n",
    "#        Predicted labels from test......... [7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4 9 6 6 5 4 0 7 4 0 1 ... 5 2 4 9 4 2 6 4 1 7 2 6 6 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6]\n",
    "#        ###\n",
    "#        Accuracy: 97.0400 %\n",
    "#     MNist: Random Forest = 5000 ...\n",
    "#        Actual    labels from test......... [7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4 9 6 6 5 4 0 7 4 0 1 ... 5 2 4 9 4 3 6 4 1 7 2 6 5 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6]\n",
    "#        Predicted labels from test......... [7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4 9 6 6 5 4 0 7 4 0 1 ... 3 2 4 9 4 2 6 4 1 7 2 6 6 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6]\n",
    "#        ###\n",
    "#        Accuracy: 97.1800 %\n",
    "#     \n",
    "#     /opt/conda/lib/python3.8/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
    "#       warnings.warn(\n",
    "#     \n",
    "#     MNist: LinearSVC c=0.01    ...\n",
    "#        Actual    labels from test......... [7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4 9 6 6 5 4 0 7 4 0 1 ... 5 2 4 9 4 3 6 4 1 7 2 6 5 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6]\n",
    "#        Predicted labels from test......... [7 2 1 0 4 1 4 9 6 9 0 6 9 0 1 5 9 7 2 4 9 6 6 5 4 0 7 4 0 1 ... 3 2 4 9 4 3 6 4 1 7 3 6 6 0 1 2 3 4 5 6 7 3 9 0 1 2 3 4 5 6]\n",
    "#        ###\n",
    "#        Accuracy: 87.1200 %\n",
    "#     \n",
    "#     /opt/conda/lib/python3.8/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
    "#       warnings.warn(\n",
    "#     \n",
    "#     MNist: LinearSVC c=0.1     ...\n",
    "#        Actual    labels from test......... [7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4 9 6 6 5 4 0 7 4 0 1 ... 5 2 4 9 4 3 6 4 1 7 2 6 5 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6]\n",
    "#        Predicted labels from test......... [7 2 1 0 4 1 4 1 6 4 0 6 9 0 1 5 9 7 2 4 9 6 6 5 4 0 7 4 0 1 ... 3 2 4 4 4 2 6 4 1 7 3 6 6 0 1 2 3 4 5 6 7 3 4 0 1 2 3 4 5 6]\n",
    "#        ###\n",
    "#        Accuracy: 86.4700 %\n",
    "#     \n",
    "#     /opt/conda/lib/python3.8/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
    "#       warnings.warn(\n",
    "#     \n",
    "#     MNist: LinearSVC c=1.0     ...\n",
    "#        Actual    labels from test......... [7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4 9 6 6 5 4 0 7 4 0 1 ... 5 2 4 9 4 3 6 4 1 7 2 6 5 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6]\n",
    "#        Predicted labels from test......... [7 2 1 0 4 1 4 9 4 9 0 6 9 0 1 5 9 7 3 4 9 6 6 5 4 0 7 4 0 1 ... 3 2 4 9 4 3 6 4 1 7 3 6 2 0 1 2 3 4 5 6 7 3 9 0 1 2 3 4 5 6]\n",
    "#        ###\n",
    "#        Accuracy: 83.9900 %\n",
    "#     \n",
    "#     MNist: LinearSVC c=10.0    ...\n",
    "#        Actual    labels from test......... [7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4 9 6 6 5 4 0 7 4 0 1 ... 5 2 4 9 4 3 6 4 1 7 2 6 5 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6]\n",
    "#        Predicted labels from test......... [7 2 1 0 4 1 8 9 6 9 0 6 9 0 1 5 9 7 3 4 9 6 6 5 4 0 7 4 0 1 ... 3 2 4 9 4 3 6 4 1 7 3 6 6 0 1 8 8 4 5 6 7 8 9 0 1 8 3 5 5 6]\n",
    "#        ###\n",
    "#        Accuracy: 83.6900 %\n",
    "#     \n",
    "#     \n",
    "#     +-----------------------------+------------+\n",
    "#     | Category                    | Accuracy   |\n",
    "#     |-----------------------------+------------|\n",
    "#     |                             |            |\n",
    "#     | MNist: Centroid             | 82.03      |\n",
    "#     | MNist: kNN=3                | 97.05      |\n",
    "#     | MNist: kNN=7                | 96.94      |\n",
    "#     | MNist: GaussianNB           | 55.58      |\n",
    "#     | MNist: MultinomialNB        | 83.65      |\n",
    "#     | MNist: DecisionTree         | 87.67      |\n",
    "#     | MNist: Random Forest = 5    | 91.91      |\n",
    "#     | MNist: Random Forest = 50   | 96.7       |\n",
    "#     | MNist: Random Forest = 500  | 97.04      |\n",
    "#     | MNist: Random Forest = 5000 | 97.18      |\n",
    "#     | MNist: LinearSVC c=0.01     | 87.12      |\n",
    "#     | MNist: LinearSVC c=0.1      | 86.47      |\n",
    "#     | MNist: LinearSVC c=1.0      | 83.99      |\n",
    "#     | MNist: LinearSVC c=10.0     | 83.69      |\n",
    "#     +-----------------------------+------------+\n",
    "#     \n",
    "#     --\n",
    "#     \n",
    "#     /opt/conda/lib/python3.8/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
    "#       warnings.warn(\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8612d30f-3774-4620-a843-7c04c7a6095f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Regarding this,\n",
    "#\n",
    "#     /opt/conda/lib/python3.8/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning:\n",
    "#        Liblinear failed to converge, increase the number of iterations.\n",
    "#        warnings.warn(\n",
    "#\n",
    "#  From,\n",
    "#     https://stackoverflow.com/questions/52670012/convergencewarning-liblinear-failed-to-converge-increase-the-number-of-iterati\n",
    "#\n",
    "#      Normally when an optimization algorithm does not converge, it is usually because the problem is not well-conditioned,\n",
    "#      perhaps due to a poor scaling of the decision variables. There are a few things you can try.\n",
    "#      \n",
    "#          Normalize your training data so that the problem hopefully becomes more well conditioned, which in turn can speed up\n",
    "#          convergence. One possibility is to scale your data to 0 mean, unit standard deviation using Scikit-Learn's StandardScaler\n",
    "#          for an example.\n",
    "#\n",
    "#          Note that you have to apply the StandardScaler fitted on the training data to the test data. Also, if you have discrete\n",
    "#          features, make sure they are transformed properly so that scaling them makes sense.\n",
    "#\n",
    "#          Related to 1), make sure the other arguments such as regularization weight, C, is set appropriately. C has to be > 0.\n",
    "#          Typically one would try various values of C in a logarithmic scale (1e-5, 1e-4, 1e-3, ..., 1, 10, 100, ...) before\n",
    "#          finetuning it at finer granularity within a particular interval. These days, it probably make more sense to tune\n",
    "#          parameters using, for e.g., Bayesian Optimization using a package such as Scikit-Optimize.\n",
    "#\n",
    "#          Set max_iter to a larger value. The default is 1000. This should be your last resort. If the optimization process does\n",
    "#          not converge within the first 1000 iterations, having it converge by setting a larger max_iter typically masks other\n",
    "#          problems such as those described in 1) and 2). It might even indicate that you have some in appropriate features or\n",
    "#          strong correlations in the features. Debug those first before taking this easy way out.\n",
    "#\n",
    "#          Set dual = True if number of features > number of examples and vice versa. This solves the SVM optimization problem using\n",
    "#          the dual formulation. Thanks @Nino van Hooff for pointing this out, and @JamesKo for spotting my mistake.\n",
    "#\n",
    "#          Use a different solver, for e.g., the L-BFGS solver if you are using Logistic Regression. See @5ervant's answer.\n",
    "#      \n",
    "#      Note: One should not ignore this warning.\n",
    "#      \n",
    "#      This warning came about because\n",
    "#      \n",
    "#          Solving the linear SVM is just solving a quadratic optimization problem. The solver is typically an iterative algorithm\n",
    "#          that keeps a running estimate of the solution (i.e., the weight and bias for the SVM). It stops running when the solution\n",
    "#          corresponds to an objective value that is optimal for this convex optimization problem, or when it hits the maximum number\n",
    "#          of iterations set.\n",
    "#      \n",
    "#          If the algorithm does not converge, then the current estimate of the SVM's parameters are not guaranteed to be any good, \n",
    "#          hence the predictions can also be complete garbage.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013d986c-991a-472c-b2a0-62ac654473fc",
   "metadata": {
    "tags": []
   },
   "source": [
    "#  Step C3: Create MNist test when data is entirely randomized row by row .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff57e71-9dc7-4e7c-9e6c-eb447e07867a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#  Effect of randomness, moving the bits around inside each image.\n",
    "#\n",
    "#  Note; each row is randomzied by its own unique pattern.\n",
    "#\n",
    "\n",
    "#  The following variables are in scope ..\n",
    "#\n",
    "#     np_mnist[\"train\"] \n",
    "#     np_mnist[\"train_label\"]\n",
    "#     np_mnist[\"test\"]\n",
    "#     np_mnist[\"test_label\"]\n",
    "#     np_mnist[\"train_v\"]           #  vectors of the two data sets above\n",
    "#     np_mnist[\"test_v\"] \n",
    "#\n",
    "\n",
    "#  Here we want to copy the two \"v\" arrays and randomize them\n",
    "#\n",
    "np_mnist[\"train_v_s\"] = np.copy(np_mnist[\"train_v\"])\n",
    "np_mnist[\"test_v_s\" ] = np.copy(np_mnist[\"test_v\" ])\n",
    "   #\n",
    "for i in range(np_mnist[\"train_v_s\"].shape[0]):\n",
    "   np.random.shuffle(np_mnist[\"train_v_s\"][i, :])\n",
    "for i in range(np_mnist[\"test_v_s\" ].shape[0]):\n",
    "   np.random.shuffle(np_mnist[\"test_v_s\" ][i, :])\n",
    "\n",
    "\n",
    "#  Looking at the non-scrambled, and yes-scrambled data\n",
    "#\n",
    "#  Currently the data lives as a vector. To look at it, copy\n",
    "#  it back to a 28*28 numpy array. We only need this for two\n",
    "#  rows we wish to view, and we choose to use test.\n",
    "#\n",
    "np_mnist[\"test_s\"] = np.zeros((2, np_mnist[\"test\"].shape[1], np_mnist[\"test\"].shape[2]), dtype = int)\n",
    "   #\n",
    "for i in range(np_mnist[\"test_s\"].shape[0]):\n",
    "   np_mnist[\"test_s\"][i,:,:] = np_mnist[\"test_v_s\"][i].reshape(28, 28)\n",
    "\n",
    "#  And the actual print\n",
    "#\n",
    "#  Non-randomized bits\n",
    "#\n",
    "print(np_mnist[\"test_label\"][0:2])\n",
    "print(np_mnist[\"test\"][0:2])\n",
    "print(\"Number of rows: %d\" % (len(np_mnist[\"test\"])))\n",
    "   #\n",
    "print()\n",
    "\n",
    "#  Problems with print formatting. These lines help\n",
    "#\n",
    "np.set_printoptions()\n",
    "np.set_printoptions(edgeitems = 30, linewidth = 100000,  formatter = dict(int = lambda x: \"%3i\" % x))\n",
    "\n",
    "#  Randomized bits\n",
    "#\n",
    "print(np_mnist[\"test_label\"][0:2])\n",
    "print(np_mnist[\"test_s\"][0:2])\n",
    "print(\"Number of rows: %d\" % (len(np_mnist[\"test_s\"])))\n",
    "   #\n",
    "print()\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1983d12-36a3-42fd-83dc-3fb41fe464d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Sample results\n",
    "#\n",
    "#     [  7   2]\n",
    "#     [[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0  84 185 159 151  60  36   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0 222 254 254 254 254 241 198 198 198 198 198 198 198 198 170  52   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0  67 114  72 114 163 227 254 225 254 254 254 250 229 254 254 140   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0  17  66  14  67  67  67  59  21 236 254 106   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  83 253 209  18   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  22 233 255  83   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 129 254 238  44   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  59 249 254  62   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 133 254 187   5   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   9 205 248  58   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 126 254 182   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  75 251 240  57   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0  19 221 254 166   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   3 203 254 219  35   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0  38 254 254  77   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0  31 224 254 115   1   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0 133 254 254  52   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0  61 242 254 254  52   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0 121 254 254 219  40   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0 121 254 207  18   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]]\n",
    "#     \n",
    "#      [[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0 116 125 171 255 255 150  93   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0 169 253 253 253 253 253 253 218  30   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0 169 253 253 253 213 142 176 253 253 122   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0  52 250 253 210  32  12   0   6 206 253 140   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0  77 251 210  25   0   0   0 122 248 253  65   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0  31  18   0   0   0   0 209 253 253  65   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0 117 247 253 198  10   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0  76 247 253 231  63   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0 128 253 253 144   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0 176 246 253 159  12   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0  25 234 253 233  35   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0 198 253 253 141   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0  78 248 253 189  12   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0  19 200 253 253 141   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0 134 253 253 173  12   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0 248 253 253  25   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0 248 253 253  43  20  20  20  20   5   0   5  20  20  37 150 150 150 147  10   0]\n",
    "#       [  0   0   0   0   0   0   0   0 248 253 253 253 253 253 253 253 168 143 166 253 253 253 253 253 253 253 123   0]\n",
    "#       [  0   0   0   0   0   0   0   0 174 253 253 253 253 253 253 253 253 253 253 253 249 247 247 169 117 117  57   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0 118 123 123 123 166 253 253 253 155 123 123  41   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]]]\n",
    "#     Number of rows: 10000\n",
    "#     \n",
    "#     [  7   2]\n",
    "#     [[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [ 61   0  38   0 254   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   9   0   0   0   0   0   0   0   0   0   0   0   0   0   0 254   0   0   0 198 241   0   0   0]\n",
    "#       [  0   0   0   0   0   0 170   0 121   0 233 254   0   0   0 115   0 185 198   0   0   0   0   0 129   0   0   0]\n",
    "#       [  0   0 225   0   0   0   0   0   0   0   0   0   0 205   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [159   0   0  19   0   0   0   0   0  18   0   0   0   0  58   0  21 254   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0 229   0   0   0   0 121   0   0   0   0   0   0   0   0   0   0   0   0   0   0 254   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 254   0  60   0   0 238  67   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0 151   0   0   0  59   0 254   0   0   0   0   0   0   0   0 207   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0 209   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0  67   0   0   0   0   0 254   0   0   0   0   0   0   0   0  52]\n",
    "#       [  0  57 163  77   0   0   0   0   0   0  84   0   0   0   0   0   0   0   0   0   0   0  52   0   0   0   0 248]\n",
    "#       [  0   0   0   0   0   0   0   0 254   0   0   0 236   0   0   0 249   0   0   0   0   0   0   0  59   0   0   0]\n",
    "#       [  0   0   0   1   0   0   0   0 221   0   0   0   0   0   0   0   0   0   0   0   0 254   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0 133   0   0   0   0   0   0 240   0   0   0 219   0   0   0   0]\n",
    "#       [  0 251   0   0   0   0   0  36   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  17]\n",
    "#       [  0   0   0  31   0   0   0 254   0 254   0   0   0   0   0   0   0 219   0  66   0   0   0   0   0   0   0   0]\n",
    "#       [166   0   0   0   0 254   0   0 254   0   0   0   0   0   0   0   0   0   0   0   0   0  62   0   0   3   0   0]\n",
    "#       [  0   0   0   0   0  18   0   0   0   0   0   0 254   0   0   0   0 254   0   0  83   0   0   0   0  40   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0 250  75   0   0 198   0   0   0   0   0   0   0 203   0   0   0  67 114   0   0]\n",
    "#       [  0   0   0 140   0   0   0   0   0   0   0 254   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0 106   0 227   0  52   0   0   0   0   0   0   0   0   0   0  14   0   0   0   0   0   0   0 222]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 254   0 182]\n",
    "#       [126   0   0   0   0   0   0   0   0   0   0 254   0   0   0   0   0   0   0   0 242   0   0  44   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0 254 198 254   0   0   0 114   0   0   0   0 133   0   0 198   0   0   0   0]\n",
    "#       [  0   0   0   0   0 254 253   5   0   0   0 187   0   0   0   0  67   0   0   0  72 198   0  83   0   0   0   0]\n",
    "#       [  0   0 254 198   0 254 254   0   0   0   0   0 254   0   0   0   0   0   0   0   0   0   0  35   0   0  22   0]\n",
    "#       [  0   0   0 255   0   0   0   0 254   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 198   0 224]]\n",
    "#     \n",
    "#      [[  0   0 247   0   0 253  12 118  31   0 248   0   0   0   0   0 176 166   0   0   0   0   0  25   0   0   0   0]\n",
    "#       [  0   0 209   0  93   0   0   0   0   0   0   0   0   0   0 123   0   0   0   0 200   0  35  10   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0 141   0 253 253   0   0   0   0   0   0   0   0 253   0   0   0 253   0   0 253]\n",
    "#       [  0   0   0   0   0   0   0   0   0 117   0   0 206 122 253  10   0 150   0   0   0   0 253 253   0   0 253 253]\n",
    "#       [ 43   0   0   0   0   0   0   0   0   0   0   0 253   0   0   0 253   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0  32   0   0 253   0   0   0   0 253 155   0   0   0   0   0   0   0   0 198 140   0   0   0   0   0]\n",
    "#       [  0   0   0  12   0   0   0 141   0 255   0   0   0  30   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [249   0   0   0   0 123 166   0   0   0   0   0   0 253   0   0   0   0 210 248   0   0   0  20   0   0   0   0]\n",
    "#       [  0   0 123  18   0   0   0   0   0   0   0   0  20 253   0   0   0   0   0   0 198   0 253   0   0   0   0   0]\n",
    "#       [  0 255   0   0 253   0   0   0   0   0   0   0 253   0   0   0   0   0   0   0   0   0   0  63   0   0 253   0]\n",
    "#       [  0   0   0   0 116   6   0   0   0   0  20   0   0   0 248   0   0  12 142   0   0   0   0   0 253   0 253   0]\n",
    "#       [  0   0   0   0 253   0   0   0   0   0 150   0   0   0   0 253 234   0   0 253   0   0   0   0  78   0   0   0]\n",
    "#       [  0   0   0 253   0   0   0   0   0   0   0 169   0   0 218   0   0   0   0   0 253   0   0 253   0   0   5   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0 213   0   0   0   0   0   0 253   0   0 253   0   0 231   0   0   0 253]\n",
    "#       [  0   0 253   0   0   0   0   0   0 134   0   0   0   0   0   0 253   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [174   0   0   0   0   0   0   0  19   0 123   0   0   0   0   0   0   0   0 171 168   0   0   0   0   0   0 128]\n",
    "#       [  0   0  20  41 253   0 253 144   0   0  76   0   0   0   0 253   0   0   0 176   0 159   0 253   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0 248   0  20   0   0   0   0   0   0   0 253   0   0   0   0   0   0   0   0   0]\n",
    "#       [246   0   0   0   0   0   0   0   0   0   0   0  25   0   0   0   0 253   0 253   0   0 253   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0 253 253   0   0   0   0   0   0   0   0 123   0   0   0 117   0   0   0   0   0 125   5]\n",
    "#       [  0   0   0   0   0   0 253   0   0   0   0   0   0   0 250   0 253   0   0  52   0 122 123   0   0   0   0   0]\n",
    "#       [  0 147   0 253   0   0 169   0   0   0   0   0 253   0 253   0   0   0   0   0   0 143   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0 253   0   0   0   0 247   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 150   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0 253   0 253   0   0   0   0  77   0 253   0   0   0   0]\n",
    "#       [248  37   0   0   0   0   0  65   0   0   0   0 173 233   0   0 253   0   0 253   0   0   0   0   0   0   0   0]\n",
    "#       [150   0 169 247   0   0   0   0   0  12   0   0   0   0   0   0  25   0   0   0   0  65   0   0   0   0 189   0]\n",
    "#       [  0 253   0 251   0   0   0   0   0   0   0   0 253 253   0   0   0   0   0 253   0   0   0   0   0   0   0   0]\n",
    "#       [117   0 247   0   0   0   0   0   0   0   0   0   0   0   0   0   0 253  20 210 253   0  57   0   0   0   0   0]]]\n",
    "#     Number of rows: 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b208e4-2d65-4efa-9ed1-f4f51be6bf35",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Creating a bar chart; Are these the same values ?\n",
    "#\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "l_hs = np.hstack(np_mnist[\"test_v\"][0])\n",
    "_ = plt.hist(l_hs, bins='auto') \n",
    "   #\n",
    "plt.title(\"784 possible values, range 0-256: Image pre-randomization\")\n",
    "plt.xlabel('RGB Value')\n",
    "plt.ylabel('Count of Said Value')\n",
    "   #\n",
    "plt.show()\n",
    "\n",
    "l_hs = np.hstack(np_mnist[\"test_v_s\"][0])\n",
    "_ = plt.hist(l_hs, bins='auto') \n",
    "   #\n",
    "plt.title(\"784 possible values, range 0-256: Image post-randomization\")\n",
    "plt.xlabel('RGB Value')\n",
    "plt.ylabel('Count of Said Value')\n",
    "   #\n",
    "plt.show()\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266e165c-4d92-48cb-8ee2-85782fc4119d",
   "metadata": {},
   "source": [
    "<div> \n",
    "<img src=\"./01_Images/07_Results_BarChart.png\" alt=\"Drawing\" style=\"width: 800px;\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c2fe66-9ac6-4a9b-ad85-090e5e0c3ef0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#  Step C4:  Run ML tests on the randomized NNist data .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6316f92-89a4-440c-9861-090b6f6e8fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Rerun ML routines now on the scrambled images\n",
    "#\n",
    "\n",
    "#  To add a blank line to history-\n",
    "#\n",
    "l_history.add(event = \"\", measure = \"\")\n",
    "\n",
    "\n",
    "do_model(NearestCentroid(), np_mnist[\"train_v_s\"], np_mnist[\"train_label\"], np_mnist[\"test_v_s\"], np_mnist[\"test_label\"], \"MNist: Centroid, Scramble\") \n",
    "print()\n",
    "\n",
    "do_model(KNeighborsClassifier(n_neighbors =  3), np_mnist[\"train_v_s\"], np_mnist[\"train_label\"], np_mnist[\"test_v_s\"], np_mnist[\"test_label\"], \"MNist: kNN=3, Scramble\" ) \n",
    "do_model(KNeighborsClassifier(n_neighbors =  7), np_mnist[\"train_v_s\"], np_mnist[\"train_label\"], np_mnist[\"test_v_s\"], np_mnist[\"test_label\"], \"MNist: kNN=7, Scramble\") \n",
    "print()\n",
    "\n",
    "do_model(GaussianNB(), np_mnist[\"train_v_s\"], np_mnist[\"train_label\"], np_mnist[\"test_v_s\"], np_mnist[\"test_label\"], \"MNist: GaussianNB, Scramble\") \n",
    "print()\n",
    "\n",
    "do_model(MultinomialNB(), np_mnist[\"train_v_s\"], np_mnist[\"train_label\"], np_mnist[\"test_v_s\"], np_mnist[\"test_label\"], \"MNist: MultinomialNB, Scramble\") \n",
    "print()\n",
    "\n",
    "do_model(DecisionTreeClassifier(), np_mnist[\"train_v_s\"], np_mnist[\"train_label\"], np_mnist[\"test_v_s\"], np_mnist[\"test_label\"], \"MNist: DecisionTree, Scramble\") \n",
    "print()\n",
    "\n",
    "#  n_estimators, number of random trees created and trained\n",
    "#\n",
    "#  do_model(RandomForestClassifier(n_estimators = 5   ), np_mnist[\"train_scramble_v\"], np_mnist[\"train_label\"], np_mnist[\"test_scramble_v\"], np_mnist[\"test_label\"], \"MNist: Random Forest = 5   , Scramble\") \n",
    "#  do_model(RandomForestClassifier(n_estimators = 50  ), np_mnist[\"train_scramble_v\"], np_mnist[\"train_label\"], np_mnist[\"test_scramble_v\"], np_mnist[\"test_label\"], \"MNist: Random Forest = 50  , Scramble\") \n",
    "#  do_model(RandomForestClassifier(n_estimators = 500 ), np_mnist[\"train_scramble_v\"], np_mnist[\"train_label\"], np_mnist[\"test_scramble_v\"], np_mnist[\"test_label\"], \"MNist: Random Forest = 500 , Scramble\") \n",
    "#  do_model(RandomForestClassifier(n_estimators = 5000), np_mnist[\"train_scramble_v\"], np_mnist[\"train_label\"], np_mnist[\"test_scramble_v\"], np_mnist[\"test_label\"], \"MNist: Random Forest = 5000, Scramble\") \n",
    "\n",
    "print()\n",
    "\n",
    "   ###\n",
    "\n",
    "print()\n",
    "print(tabulate(l_history, headers=[\"Category\", \"Accuracy\"], tablefmt='psql', showindex=False))\n",
    "print()\n",
    "\n",
    "print(\"--\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf646e34-43c1-48a0-b369-7aa59aed6fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  The results below were on images that were not scrambled.\n",
    "#\n",
    "#     +-----------------------------+-------------------+\n",
    "#     | Category                    | Accuracy          |\n",
    "#     |-----------------------------+-------------------|\n",
    "#     | MNist: Centroid             | 82.03             |\n",
    "#     | MNist: kNN=3                | 97.05             |\n",
    "#     | MNist: kNN=7                | 96.94             |\n",
    "#     | MNist: GaussianNB           | 55.58             |\n",
    "#     | MNist: MultinomialNB        | 83.65             |\n",
    "#     | MNist: DecisionTree         | 87.72             |\n",
    "#     | MNist: Random Forest = 5    | 92.36999999999999 |\n",
    "#     | MNist: Random Forest = 50   | 96.67999999999999 |\n",
    "#     | MNist: Random Forest = 500  | 97.15             |\n",
    "#     | MNist: Random Forest = 5000 | 97.17             |\n",
    "#     |                             |                   |\n",
    "#     +-----------------------------+-------------------+\n",
    "#\n",
    "#  The results below on images when data is entirely randomized row by row ..\n",
    "#\n",
    "#     +--------------------------------+--------------------+\n",
    "#     | Category                       | Accuracy           |\n",
    "#     |--------------------------------+--------------------|\n",
    "#     | MNist: Centroid, Scramble      | 22.03              |\n",
    "#     | MNist: kNN=3, Scramble         | 11.540000000000001 |\n",
    "#     | MNist: kNN=7, Scramble         | 11.379999999999999 |\n",
    "#     | MNist: GaussianNB, Scramble    | 21.37              |\n",
    "#     | MNist: MultinomialNB, Scramble | 9.93               |\n",
    "#     | MNist: DecisionTree, Scramble  | 12.889999999999999 |\n",
    "#     |                                |                    |\n",
    "#     +--------------------------------+--------------------+\n",
    "#\n",
    "#  Takeaway,\n",
    "#\n",
    "#     .  Our data is 256 value bits * 2-dim array of (28 * 28).\n",
    "#        28 * 28 == 784\n",
    "#        Per each 28*28 image (784 bits), 650-700 of those bits are blank/zero.\n",
    "#\n",
    "#       So, to randomize the location of order bits itself is not bad, but it\n",
    "#       does not leave us with enough training data.\n",
    "#\n",
    "#       Meaning; a totally random \"7\" [ might ] be unique enough from a \"2\" or\n",
    "#       any other number, but we'd need a lot more data.\n",
    "#\n",
    "#     .  The approach now is to randomize the bits, but on a single consistent\n",
    "#        pattern applied to each row.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ad9bbf-15c7-4ea0-b690-5f1e7f43cebe",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Step C5: Compare MNist test when data is randomized by one shared pattern, row by row .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbebe7e5-79b4-4e75-8ab9-8eb30b370e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Effect of randomness, moving the bits around inside each image.\n",
    "#\n",
    "#  Note; each row is randomized by one shared pattern.\n",
    "#\n",
    "\n",
    "#  The following variables are in scope ..\n",
    "#\n",
    "#     np_mnist[\"train\"] \n",
    "#     np_mnist[\"train_label\"]\n",
    "#     np_mnist[\"test\"]\n",
    "#     np_mnist[\"test_label\"]\n",
    "#     np_mnist[\"train_v\"]           #  vectors of the two data sets above\n",
    "#     np_mnist[\"test_v\"] \n",
    "#\n",
    "\n",
    "#  Here we want to copy the two \"v\" arrays \n",
    "#\n",
    "np_mnist[\"train_v_s2\"] = np.copy(np_mnist[\"train_v\"])\n",
    "np_mnist[\"test_v_s2\" ] = np.copy(np_mnist[\"test_v\" ])\n",
    "\n",
    "\n",
    "#  Make a new vector of values 0-n, randomize that, uses this as a map to\n",
    "#  consistently 'randomize' all remaining data\n",
    "#\n",
    "np_random = np.arange(0, np_mnist[\"train_v_s2\"].shape[1] -1, 1, dtype = int)\n",
    "np.random.shuffle(np_random)\n",
    "\n",
    "\n",
    "#  Apply the actual 'randomization'\n",
    "#\n",
    "for i in range(np_mnist[\"train_v_s2\"].shape[0]):\n",
    "   l_tmp = np.zeros(np_mnist[\"train_v_s2\"][i].shape[0], dtype = int)\n",
    "   for j in range(np_mnist[\"train_v_s2\"][i].shape[0] -1):\n",
    "      l_tmp[j] = np_mnist[\"train_v_s2\"][i][ np_random[j] ]\n",
    "   np_mnist[\"train_v_s2\"][i, :] = l_tmp[:]\n",
    "\n",
    "\n",
    "       #\n",
    "for i in range(np_mnist[\"test_v_s2\"].shape[0]):\n",
    "   l_tmp = np.zeros(np_mnist[\"test_v_s2\"][i].shape[0], dtype = int)\n",
    "   for j in range(np_mnist[\"test_v_s2\"][i].shape[0] - 1):\n",
    "      l_tmp[j] = np_mnist[\"test_v_s2\"][i][ np_random[j] ]\n",
    "   np_mnist[\"test_v_s2\"][i, :] = l_tmp[:]\n",
    "\n",
    "\n",
    "#  Looking at the yes-scrambled data\n",
    "#\n",
    "#  Currently the data lives as a vector. To look at it, copy\n",
    "#  it back to a 28*28 numpy array. We only need this for two\n",
    "#  rows we wish to view, and we choose to use test.\n",
    "#\n",
    "np_mnist[\"test_s\"] = np.zeros((2, np_mnist[\"test\"].shape[1], np_mnist[\"test\"].shape[2]), dtype = int)\n",
    "   #\n",
    "for i in range(np_mnist[\"test_s\"].shape[0]):\n",
    "   np_mnist[\"test_s\"][i,:,:] = np_mnist[\"test_v_s2\"][i].reshape(np_mnist[\"test\"].shape[1], np_mnist[\"test\"].shape[2])\n",
    "\n",
    "\n",
    "#  Problems with print formatting. These lines help\n",
    "#\n",
    "np.set_printoptions()\n",
    "np.set_printoptions(edgeitems = 30, linewidth = 100000,  formatter = dict(int = lambda x: \"%3i\" % x))\n",
    "\n",
    "#  Randomized bits\n",
    "#\n",
    "print(np_mnist[\"test_label\"][0:2])\n",
    "print(np_mnist[\"test_s\"][0:2])\n",
    "print(\"Number of rows: %d\" % (len(np_mnist[\"test_s\"])))\n",
    "   #\n",
    "print()\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a02b6db-d527-4398-a4c9-782dc693ba79",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Sample output\n",
    "#\n",
    "#     [  7   2]\n",
    "#     [[[  0   0 133 205   0   0   0   0   0   0   0   0  18 219   0   0 254   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0 198   0   0 198   0   0   0 254   0   0   0   0   0   0  57   0   0   0   0 198   0   0   0   0]\n",
    "#       [  0 151   0   0   0   0 254   0   0   0   0   0   0   0   0  75   0   0   0   0   0  17   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0 254   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 198   0   0   0   0   0]\n",
    "#       [  0  21   0   0   0   0   0   0   0   0   0   0   0   0   0   0 236   0   0   0   0 254   0   0 121   0   0   0]\n",
    "#       [  0   0 241   0   0   0   0   0   0   0   0   0   0   0   0   0   0 254   0   0   0 255   0 254   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0 254   0   0   0   0   0 254   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   3   0   0   0 187   0  22   0 254   0 207   0   0   0  44   0   0   0  36   0   0   0  60   0 249   0]\n",
    "#       [  0   0   0   0   0 240  38   0 159   0   0   0   0 209   0 133   0   0   0   0   0   0   0  61 254   0   0   0]\n",
    "#       [  0   0   0 253   0   0 238   0 224   0   0   0   0   0   0  83   0  52   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0 254   0   0   0 229   0   0   0   0   0 254   0   0   0   0   0 114   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0 129   0   0   0   0   0 198   0   0   0   0   0 115   0   0   0   0  58   0   0   0 221]\n",
    "#       [  0 251   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 163   0   0   0]\n",
    "#       [  0  59   9   0   0  67   0   0   0   0   0   0   0 182   0   0   0   0 198   0   0   0   0   0   0   0  35   0]\n",
    "#       [  0   0 250   0   0  19   0 254   0   0   0   0   0   0   0   0   0   0   0   0   0  52 254   0   0   5   0   0]\n",
    "#       [  0   0  66 198   0   0   0   0   0   0 126   0  52   0   0   0 140   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0 254   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  14]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0  67   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 254]\n",
    "#       [  0   0   0   0   0   0   0 254   0 254   0   0   0 185   0   0  67   0   0   0   0   0   0   0  40 121   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0 166   0   0   0   0   0   0  31   0   0   0   0 222 203   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0 254   0 254   0   0   0   0   0   0   0   0 227   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 254   0 225   0  72   0   0   0  84   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0  18   0   0   0  59   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0 219   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 170   0   0   0 254   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0  77   0 106   0   0   0   0   0   0   0   0   0  67   0 233   0 198]\n",
    "#       [  0  62   0   0   0   0   0   0  83 248   0 254   0   0   0   0   0   0   0 254   0   0   0   0   0   0   0 242]\n",
    "#       [  0   0   0   0   0 114   0   0   0   0   0   0   0   0   1   0   0   0   0   0 254   0   0   0 254   0   0   0]]\n",
    "#     \n",
    "#      [[ 43   0   0   0   0   0 255   0   0   0   0   0   0   0   0   0  20   0   0   0   0   0   0   0  65   0  12   0]\n",
    "#       [  0   0   0   0 253   0 147 253 128   0   0 253   0   0   0  10   0   0   0   0   0   0   0   0   0 253   0   0]\n",
    "#       [  0 210   0   0 210  12   0 253   0 253   0 141 248 249   0   0 253   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0 144   0   0   0 253   0   0   0   0   0   0   0 253   0   0 142   0   0   0   0   0   0   0   0   0 253]\n",
    "#       [  0   0   0 253   0   0   0   0   0 246   0   0   0   0   0   0   0   0 253   0   0   0   0   0   0   0   0   0]\n",
    "#       [169   0   0   0   0   0   0   5   0   0   0   0   0   0   0 253   0 198 248   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0 253   0 247   0 253   0 253   0 166   0 250 253   0 143   0   0]\n",
    "#       [  0   0 253   0   0   0   0   0   0   0   0   0  12   0   0   0  25 150   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [253   0 253  20 253   0   0 253   0 176   0   0   0   0   0 125   0   0 253   0   0 169   0   0  25   0   0   0]\n",
    "#       [118   0   0   0 140   0 253   0 251 174   0  93   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 206]\n",
    "#       [  0   0   0   0   0 234   0   0 123   0   0   0   0   0   0   0   0   0   0   0   0  35   0   0   0   0   0   0]\n",
    "#       [  0 253   0 150  30   0   0   0 176   0   0 253   0   0   0   0   0   0   0 166   0   0   0   0   0   0 248   0]\n",
    "#       [  0   0   0   0 122   0   0   0   0   0   0   0   0   0   0   0   0   0 253   0 253   0   0   0  57   0  20  20]\n",
    "#       [  0   0   0   0 123   0   0   0   0   0   0   0   0   0  41 116   0   0   0   0   0 253 141 253   0   0   0 189]\n",
    "#       [  0   0   0   0   0  63   0   0   0   0   0   0 247   0   0   0   0 253 209   0   0 123   0   0   0   0 168 253]\n",
    "#       [ 25   0  10 150   0  20   0   0   0   0   0   0   0   0   0   0   0 253   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0  76   0   0   0 122   0   0 253   0 253   0   0   0   0   0   0 134   0   0   0   0   0   0   0   0   0]\n",
    "#       [ 37 213 253   0   0  18   0   0   0   0 253   0 253   0   0   0   0 253 253   0   0   0   0 117   0 248   0 247]\n",
    "#       [  0 255   0   0   0   0   0   0   0 253   0   0   0   0   0   0   0   0 253   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0 173   0   0   0   0   0  31   0   0   0   0   0  77  32   0 253   0 117 253   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0 169   0   0   0   0   0   5   0   0   0   0   0   0 123   0   0   0   0   0 253 198   0   0   0]\n",
    "#       [  0 253   0   0   0   0   0 253   0   0 253   0  20   0   0   0   0 123   0   0   0   0   0 123   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0 155   0   0  78   0   0   0 253   0 117   0   0   0 200   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 253   0]\n",
    "#       [  0   0 253   0   0 253   0 159   0   0   0   0   0   0 247   0   0   0   0   0   0   0   0   0 233   0   0   0]\n",
    "#       [  0   0 248 218   0   0 171   0   0   0   0 253   0   0   0   0 150   0   0 253   0 253   0 231  52   0   0  65]\n",
    "#       [  0   0   0   0   0   0   0  12   0   0   0   0   0   0 253   0   0   0   0   0   0   0   0   0   0   0  19   0]\n",
    "#       [253   0   0   0   6   0   0   0 253   0   0   0   0   0 253   0   0   0 253   0   0   0   0   0 253   0   0   0]]]\n",
    "#     Number of rows: 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346226c5-d1d2-4ee9-aa9b-192e950e22c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Rerun ML routines now on the scrambled images, those randomized by one shared pattern\n",
    "#\n",
    "\n",
    "#  To add a blank line to history-\n",
    "#\n",
    "l_history.add(event = \"\", measure = \"\")\n",
    "\n",
    "\n",
    "do_model(NearestCentroid(), np_mnist[\"train_v_s2\"], np_mnist[\"train_label\"], np_mnist[\"test_v_s2\"], np_mnist[\"test_label\"], \"MNist: Centroid, Scramble-2\") \n",
    "print()\n",
    "\n",
    "do_model(KNeighborsClassifier(n_neighbors =  3), np_mnist[\"train_v_s2\"], np_mnist[\"train_label\"], np_mnist[\"test_v_s2\"], np_mnist[\"test_label\"], \"MNist: kNN=3, Scramble-2\" ) \n",
    "do_model(KNeighborsClassifier(n_neighbors =  7), np_mnist[\"train_v_s2\"], np_mnist[\"train_label\"], np_mnist[\"test_v_s2\"], np_mnist[\"test_label\"], \"MNist: kNN=7, Scramble-2\") \n",
    "print()\n",
    "\n",
    "do_model(GaussianNB(), np_mnist[\"train_v_s2\"], np_mnist[\"train_label\"], np_mnist[\"test_v_s2\"], np_mnist[\"test_label\"], \"MNist: GaussianNB, Scramble-2\") \n",
    "print()\n",
    "\n",
    "do_model(MultinomialNB(), np_mnist[\"train_v_s2\"], np_mnist[\"train_label\"], np_mnist[\"test_v_s2\"], np_mnist[\"test_label\"], \"MNist: MultinomialNB, Scramble-2\") \n",
    "print()\n",
    "\n",
    "do_model(DecisionTreeClassifier(), np_mnist[\"train_v_s2\"], np_mnist[\"train_label\"], np_mnist[\"test_v_s2\"], np_mnist[\"test_label\"], \"MNist: DecisionTree, Scramble-2\") \n",
    "print()\n",
    "\n",
    "#  n_estimators, number of random trees created and trained\n",
    "#\n",
    "#  do_model(RandomForestClassifier(n_estimators = 5   ), np_mnist[\"train_scramble_v\"], np_mnist[\"train_label\"], np_mnist[\"test_scramble_v\"], np_mnist[\"test_label\"], \"MNist: Random Forest = 5   , Scramble\") \n",
    "#  do_model(RandomForestClassifier(n_estimators = 50  ), np_mnist[\"train_scramble_v\"], np_mnist[\"train_label\"], np_mnist[\"test_scramble_v\"], np_mnist[\"test_label\"], \"MNist: Random Forest = 50  , Scramble\") \n",
    "#  do_model(RandomForestClassifier(n_estimators = 500 ), np_mnist[\"train_scramble_v\"], np_mnist[\"train_label\"], np_mnist[\"test_scramble_v\"], np_mnist[\"test_label\"], \"MNist: Random Forest = 500 , Scramble\") \n",
    "#  do_model(RandomForestClassifier(n_estimators = 5000), np_mnist[\"train_scramble_v\"], np_mnist[\"train_label\"], np_mnist[\"test_scramble_v\"], np_mnist[\"test_label\"], \"MNist: Random Forest = 5000, Scramble\") \n",
    "\n",
    "print()\n",
    "\n",
    "   ###\n",
    "\n",
    "print()\n",
    "print(tabulate(l_history, headers=[\"Category\", \"Accuracy\"], tablefmt='psql', showindex=False))\n",
    "print()\n",
    "\n",
    "print(\"--\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420ad074-e31f-44ff-844a-2c77217929f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Sample output,\n",
    "#\n",
    "#  The results below were on images that were not scrambled.\n",
    "#\n",
    "#     +-----------------------------+-------------------+\n",
    "#     | Category                    | Accuracy          |\n",
    "#     |-----------------------------+-------------------|\n",
    "#     | MNist: Centroid             | 82.03             |\n",
    "#     | MNist: kNN=3                | 97.05             |\n",
    "#     | MNist: kNN=7                | 96.94             |\n",
    "#     | MNist: GaussianNB           | 55.58             |\n",
    "#     | MNist: MultinomialNB        | 83.65             |\n",
    "#     | MNist: DecisionTree         | 87.72             |\n",
    "#     | MNist: Random Forest = 5    | 92.36999999999999 |\n",
    "#     | MNist: Random Forest = 50   | 96.67999999999999 |\n",
    "#     | MNist: Random Forest = 500  | 97.15             |\n",
    "#     | MNist: Random Forest = 5000 | 97.17             |\n",
    "#     |                             |                   |\n",
    "#     +-----------------------------+-------------------+\n",
    "#\n",
    "#  The results below on images when data is randomized by one shared pattern ..\n",
    "#\n",
    "#     +----------------------------------+------------+\n",
    "#     | Category                         | Accuracy   |\n",
    "#     |----------------------------------+------------|\n",
    "#     |                                  |            |\n",
    "#     | MNist: Centroid, Scramble-2      | 82.03      |\n",
    "#     | MNist: kNN=3, Scramble-2         | 97.05      |\n",
    "#     | MNist: kNN=7, Scramble-2         | 96.94      |\n",
    "#     | MNist: GaussianNB, Scramble-2    | 55.58      |\n",
    "#     | MNist: MultinomialNB, Scramble-2 | 83.65      |\n",
    "#     | MNist: DecisionTree, Scramble-2  | 87.7       |\n",
    "#     +----------------------------------+------------+\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e3abee-88df-4bff-ad53-18988ae635a6",
   "metadata": {},
   "source": [
    "#  Step D1: Compare data volumes and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2506248-ba2a-4b82-82fc-afbb6b909e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  The following variables are in scope ..\n",
    "#\n",
    "#     np_mnist[\"train\"] \n",
    "#     np_mnist[\"train_label\"]\n",
    "#     np_mnist[\"test\"]\n",
    "#     np_mnist[\"test_label\"]\n",
    "#     np_mnist[\"train_v\"]           #  vectors of the two data sets above\n",
    "#     np_mnist[\"test_v\"] \n",
    "#\n",
    "\n",
    "#  Show the effects of amount of training data and its effect on accuracy\n",
    "#\n",
    "\n",
    "#  To add a blank line to history-\n",
    "#\n",
    "l_history.clear()\n",
    "\n",
    "#  This outputs a lot of data to the screen\n",
    "#\n",
    "#   for n in range(100, 60000 + 1, 100):\n",
    "#     do_model(NearestCentroid(), np_mnist[\"train_v\"][0:n, :], np_mnist[\"train_label\"][0:n], np_mnist[\"test_v\"], np_mnist[\"test_label\"], \"MNist: Centroid, Volume-%-5d\" % n) \n",
    "#     print()\n",
    "\n",
    "print()\n",
    "print(tabulate(l_history, headers=[\"Category\", \"Accuracy\"], tablefmt='psql', showindex=False))\n",
    "print()\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee49c01-e2a6-43c5-82c7-e3c76815d2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Sample output  (edited for space)\n",
    "#\n",
    "#     +-------------------------------+------------+\n",
    "#     | Category                      |   Accuracy |\n",
    "#     |-------------------------------+------------|\n",
    "#     | MNist: Centroid, Volume-100   |      65.74 |\n",
    "#     | MNist: Centroid, Volume-200   |      68.85 |\n",
    "#     | MNist: Centroid, Volume-300   |      71.07 |\n",
    "#     | MNist: Centroid, Volume-400   |      73.05 |\n",
    "#     | MNist: Centroid, Volume-500   |      73.88 |\n",
    "#     | MNist: Centroid, Volume-600   |      74.6  |\n",
    "#     | MNist: Centroid, Volume-700   |      76.04 |\n",
    "#     | MNist: Centroid, Volume-800   |      77.3  |\n",
    "#     | MNist: Centroid, Volume-900   |      76.8  |\n",
    "#     | MNist: Centroid, Volume-1000  |      76.55 |\n",
    "#     | MNist: Centroid, Volume-2000  |      78.92 |\n",
    "#     | MNist: Centroid, Volume-3000  |      80    |\n",
    "#     | MNist: Centroid, Volume-4000  |      80.87 |\n",
    "#     | MNist: Centroid, Volume-5000  |      80.84 |\n",
    "#     | MNist: Centroid, Volume-6000  |      81.04 |\n",
    "#     | MNist: Centroid, Volume-7000  |      80.99 |\n",
    "#     | MNist: Centroid, Volume-8000  |      81.21 |\n",
    "#     | MNist: Centroid, Volume-9000  |      80.97 |\n",
    "#     | MNist: Centroid, Volume-10000 |      81.04 |\n",
    "#     | MNist: Centroid, Volume-20000 |      81.59 |\n",
    "#     | MNist: Centroid, Volume-30000 |      82.07 |\n",
    "#     | MNist: Centroid, Volume-40000 |      82.03 |\n",
    "#     | MNist: Centroid, Volume-50000 |      82    |\n",
    "#     | MNist: Centroid, Volume-60000 |      82.03 |\n",
    "#     +-------------------------------+------------+\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "15c16a43-f6f7-4e67-b7c1-2ba2cdbe9590",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "#  2-d Chart of results \n",
    "#\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "l_data = {}\n",
    "   #\n",
    "l_data[\"xycoord\"], l_data[\"label\"] = ( np.array([[100  , 200  , 300  , 400  , 500  , 600  , 700  ,\n",
    "      800  , 900  , 1000 , 2000 , 3000 , 4000 , 5000 , 6000 , 7000 , 8000 , 9000 , 10000, 20000,\n",
    "      30000, 40000, 50000, 60000, ],\n",
    "   [65.74, 68.85, 71.07, 73.05, 73.88, 74.6 , 76.04, 77.3 , 76.8 , 76.55, 78.92, 80   , 80.87,\n",
    "      80.84, 81.04, 80.99, 81.21, 80.97, 81.04, 81.59, 82.07, 82.03, 82   , 82.03, ] ]),\n",
    "   [100  , \"\"  , \"\"  , \"\"  , \"\"  , \"\"  , \"\"  , \"\"  , \"\"  , \"\" , \"\" , \"\" , \"\" ,\n",
    "      \"\" , \"\" , \"\" , \"\" , \"\" , 10000, 20000, 30000, 40000, 50000, 60000, ] )\n",
    "                                      \n",
    "                                      \n",
    "plt.plot(l_data[\"xycoord\"][0, :], l_data[\"xycoord\"][1, :])\n",
    "   #\n",
    "plt.xticks(l_data[\"xycoord\"][0, :], l_data[\"label\"], rotation = \"horizontal\")\n",
    "\n",
    "#  Pad margins so that markers don't get clipped by the axes\n",
    "#\n",
    "plt.margins(0.2)\n",
    "\n",
    "#  Tweak spacing to prevent clipping of tick-labels\n",
    "#\n",
    "plt.subplots_adjust(bottom = 0.05)\n",
    "\n",
    "plt.title(\"Effects of Number of Training Rows relative to Accuracy\")\n",
    "plt.xlabel(\"Count of training rows\")\n",
    "plt.ylabel(\"Accuracy (percent)\")\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d18ea4-7903-4745-9923-9ce6b08f729f",
   "metadata": {},
   "source": [
    "<div> \n",
    "<img src=\"./01_Images/08_Results_Chart.png\" alt=\"Drawing\" style=\"width: 800px;\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b89fa9-100e-4b6f-b728-e281d8b4d725",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3976e37-8702-4f53-bf97-e28a958675f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9499d30f-d8c8-46d2-9cef-22fd45a841b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a397bde-64f0-4ec9-a728-d37e4c51ecea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
