{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "643ecda4-818d-480c-b90f-c3b7b9958e98",
   "metadata": {},
   "source": [
    "<div class=\"header\">\n",
    "  <img src=\"img/kg_logo_white_side.png\" alt=\"logo\" style=\"width: 300px;\"/>\n",
    "  <h1>Transaction Monitoring with Graph Neural Networks</h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3c3495-0f0a-494b-abb5-78abaa1a0951",
   "metadata": {},
   "source": [
    "The Elliptic Data Set maps Bitcoin transactions to real entities belonging to licit categories (exchanges, wallet providers, miners, licit services, etc.) versus illicit ones (scams, malware, terrorist organizations, ransomware, Ponzi schemes, etc.). The task on the dataset is to classify the illicit and licit nodes in the graph.\n",
    "We will use Graph Neural Networks (GraphSAGE) to perform the node classification task. This demo shows the end-to-end pipeline that can be done directly on the Katana Graph platform. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6576b2ca-74c9-4e95-9147-1bbe93fa4452",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2338e3-10ad-45c4-9e71-be3894e85318",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f143ed-bc3d-4c28-ab0b-f3a3f21a59d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os                                                                        \n",
    "import time                                                                      \n",
    "import json\n",
    "import uuid\n",
    "import pandas as pd\n",
    "import numpy\n",
    "import argparse\n",
    "from timeit import default_timer as timer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from katana import remote\n",
    "from katana.remote import import_data\n",
    "from katana.remote.import_data import Operation\n",
    "import dask.dataframe as dd\n",
    "os.environ[\"KATANA_SERVER_ADDRESS\"] = \"localhost:8080\"\n",
    "\n",
    "print(\"--\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a03513-59f5-4b5a-bbb7-0d86cfda4afd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Dask Dataframe Import"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c11e37-e5a2-4aac-9c84-e7af7dafef83",
   "metadata": {},
   "source": [
    "Use Python dataframes interface to prepare and generate graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3906640e-f7ec-4a30-aaf0-fc51c6de2a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_classes = \"gs://katana-demo-datasets/fsi/solution_raw_data/elliptic/elliptic_txs_classes.csv\"\n",
    "tx_edges = \"gs://katana-demo-datasets/fsi/solution_raw_data/elliptic/elliptic_txs_edgelist.csv\"\n",
    "tx_features = \"gs://katana-demo-datasets/fsi/solution_raw_data/elliptic/elliptic_txs_features.csv\"\n",
    "\n",
    "print(\"--\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aebfa2d-5e1a-4f73-8d81-1d441a51b02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_col_names = [\"txId\", \"timestamp\"]\n",
    "local_feats_name = [f\"local_feat_{i}\" for i in range(2,95)]\n",
    "agg_feats_name = [f\"agg_feat_{i}\" for i in range(95,167)]\n",
    "feat_col_names.extend(local_feats_name)\n",
    "feat_col_names.extend(agg_feats_name)\n",
    "\n",
    "print(\"--\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3be8f12-96f1-4190-91cc-f6bfefdc0765",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_types = {\n",
    "    \"class\": \"string\",\n",
    "    \"timestamp\": \"string\", \n",
    "    \"target\": \"float\",\n",
    "    \"node_type\": \"string\"\n",
    "}\n",
    "local_cols = {}\n",
    "for i in range(2,95):\n",
    "    local_cols[f\"local_feat_{i}\"] = \"float\"\n",
    "agg_cols = {}\n",
    "for i in range(95,167):\n",
    "    agg_cols[f\"agg_feat_{i}\"] = \"float\"\n",
    "feat_types.update(local_cols)\n",
    "feat_types.update(agg_cols)\n",
    "\n",
    "print(\"--\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f8761b-118b-4615-b883-03264f5cfac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "classes = dd.read_csv(tx_classes)\n",
    "edges = dd.read_csv(tx_edges)\n",
    "features = dd.read_csv(tx_features, header=None, names=feat_col_names)\n",
    "\n",
    "print(\"--\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840aee1c-2092-46d6-bdb5-9b31a3b872df",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes['target'] = classes['class'].map({'unknown': -1.0, '1': 1.0, '2': 0.0})\n",
    "classes['node_type'] = classes['class'].map({'unknown': 'Unclassified_Txn', '1': 'Classified_Txn', '2': 'Classified_Txn'})\n",
    "\n",
    "print(\"--\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad66061-d46c-47ae-bc7f-776c54547bbd",
   "metadata": {},
   "source": [
    "Some of the elliptic transactions are classified as either licit (0) or illicit (1). We can filter for only classified transactions for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1abac41-5af0-4d25-be92-4b03a3780269",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8ba55b-cb2d-4db3-836b-db840fa5e761",
   "metadata": {},
   "outputs": [],
   "source": [
    "edges.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbb78b0-7300-463d-88f8-c67a4e568738",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = features.merge(classes)\n",
    "\n",
    "print(\"--\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e317c7ad-69c2-4a21-afcf-46611abe4a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77d42e6-be1a-498e-beba-6bacbca76f4c",
   "metadata": {},
   "source": [
    "There is a 4 node cluster running on GCP, which is why we select 4 partitions. As the data scales, we can increase the number of machines in the cluster and the number of partitions in our graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387ff97a-deb4-4809-9ff5-bef2b607a5f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "NUM_PARTITIONS = 6\n",
    "\n",
    "graph = remote.Client(disable_version_check=False).create_graph(\n",
    "    num_partitions = NUM_PARTITIONS\n",
    ")\n",
    "\n",
    "print(\"graph id:\", graph.graph_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7e1d935-edd8-4233-ab49-93c036401e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VERBOSE: ../../../../external/katana/libgalois/include/katana/AsyncPool.h:152: 3148: async pool tids: 140215910070016, 140215907968768, 140215905867520\n",
      "VERBOSE: ../../../../external/katana/libgalois/include/katana/AsyncPool.h:152: 3148: async pool tids: 140215903766272, 140215899567872\n",
      "VERBOSE: ../../../../external/katana/libgalois/include/katana/AsyncPool.h:152: 3148: async pool tids: 140215895369472, 140215886976768\n",
      "VERBOSE: ../../../../external/katana/libgalois/include/katana/AsyncPool.h:152: 3148: async pool tids: 140215878584064, 140215861802752\n",
      "VERBOSE: ../../../../external/katana/libgalois/include/katana/AsyncPool.h:152: 3148: async pool tids: 140215845021440, 140215842920192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<katana_enterprise.remote.sync_wrappers.Client object at 0x7f86d4f0f3d0>\n"
     ]
    }
   ],
   "source": [
    "#  Get a KatanaGraph Connection Handle ..\n",
    "\n",
    "from katana import remote\n",
    "from katana.remote import import_data\n",
    "\n",
    "\n",
    "NUM_PARTITIONS = 8\n",
    "   #\n",
    "DB_NAME        = \"my_db\"\n",
    "GRAPH_NAME     = \"my_graph\"\n",
    "\n",
    "\n",
    "my_client = remote.Client()\n",
    "\n",
    "print(my_client)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f01c4488-e2c6-4072-a69d-20570e4c98e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_Graph my_graph, J9qMFV5XdTMTuq5vbVgZwWnNrfH77rRFM8NMHbPAEmKC, 1>\n"
     ]
    }
   ],
   "source": [
    "#  CONNECT TO GRAPH\n",
    "\n",
    "for l_graph in my_client.graphs():\n",
    "   if (l_graph.name == GRAPH_NAME):\n",
    "      my_graph=my_client.get_database(name=DB_NAME).get_graph_by_id(id=l_graph.graph_id)\n",
    "        \n",
    "print(my_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d3a714-b1a5-4219-a33b-27ff70da51ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "reverse_edges=True\n",
    "with import_data.DataFrameImporter(graph) as df_importer:   \n",
    "    \n",
    "    df_importer.nodes_dataframe(nodes,\n",
    "                            id_column=\"txId\",\n",
    "                            id_space=\"transaction\", \n",
    "                            property_columns=feat_types,\n",
    "                            label_column=\"node_type\")\n",
    "    \n",
    "    df_importer.edges_dataframe(edges,\n",
    "                            source_id_space=\"transaction\",\n",
    "                            destination_id_space=\"transaction\",\n",
    "                            source_column=\"txId1\",\n",
    "                            destination_column=\"txId2\",\n",
    "                            type=\"tx_flow\")\n",
    "    if reverse_edges:\n",
    "        df_importer.edges_dataframe(edges,\n",
    "                        source_id_space=\"transaction\",\n",
    "                        destination_id_space=\"transaction\",\n",
    "                        source_column=\"txId2\",\n",
    "                        destination_column=\"txId1\",\n",
    "                        type=\"rev_tx_flow\")\n",
    "        \n",
    "print(\"--\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa88e14c-75cc-4b6f-b036-d1028be0050c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of nodes: {graph.num_nodes():,}\")\n",
    "print(f\"Number of edges: {graph.num_edges():,}\")\n",
    "\n",
    "print(\"--\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dec14ce-65a7-4374-a26b-aced88ff3c0d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a600b9-d689-44d7-8a9f-978d013190cf",
   "metadata": {},
   "source": [
    "See all relationships from transaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2d9a54-359e-4b4d-9252-80172c8e5428",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "graph.query(\"\"\"\n",
    "MATCH (tx1)-[:tx_flow]->(tx2)\n",
    "RETURN tx1, tx2\n",
    "LIMIT 500\n",
    "\"\"\", contextualize=True).visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fd3b23-e999-44c9-9daa-bdbf9b1e160a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Feature Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b087090-7c99-421e-9c01-1b83d0c15211",
   "metadata": {},
   "source": [
    "To initialize features for the GNN, we will combine all node properties into a feature vector and save as a new feature on the graph. Katana Graph supports saving binary feature vectors as individual properties on the graph. In this case, we save 3 different feature vectors: \n",
    "\n",
    "1. ```local_feats``` - raw features provided for each transaction\n",
    "2. ```agg_feats```  - aggregated features from each node's neighborhood\n",
    "3. ```h_init``` - both ```local_feats``` + ```agg_feats``` combined into one feature. This will be the starting point for our GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e6587a-cc32-4fd1-820d-1f2f581b7bea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "def run_feature_init(g): \n",
    "    import sys, os\n",
    "    import numpy as np\n",
    "    sys.path.append(os.path.join(\"/home/gsteck_katanagraph_com/solutions/fsi/src\"))\n",
    "    from katana_ai import get_node_property_list, visualize_embeddings, train_test_split_mask, save_features_to_graph\n",
    "    # extract features\n",
    "    local_feats = get_node_property_list(g, property_list=local_feats_name)\n",
    "    agg_feats = get_node_property_list(g, property_list=agg_feats_name)\n",
    "    feat_vec = np.concatenate([local_feats, agg_feats], axis=-1)\n",
    "    # save new features vector to graph\n",
    "    g = save_features_to_graph(g, feat_vec, feature_name=\"h_init\")\n",
    "    g = save_features_to_graph(g, local_feats, feature_name=\"local_feats\")\n",
    "    g = save_features_to_graph(g, agg_feats, feature_name=\"agg_feats\")\n",
    "    # create train/test split mask\n",
    "    g = train_test_split_mask(g, train_test_validation_split=[0.8, 0.15, 0.05])\n",
    "    g.write()\n",
    "\n",
    "graph.run(lambda g: run_feature_init(g))\n",
    "\n",
    "print(\"--\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9a34b57-ae87-453c-9442-88a226e1c752",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da43314ea8e34cf1babc4db3969c24d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "          0/? [?op/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Host 0 errors:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/miniconda/lib/python3.8/site-packages/katana_enterprise/worker/worker.py\", line 85, in execute\n",
      "    value = function(graph)\n",
      "  File \"/tmp/ipykernel_3148/3080334400.py\", line 7, in <lambda>\n",
      "  File \"/tmp/ipykernel_3148/3080334400.py\", line 5, in run_feature_init\n",
      "ModuleNotFoundError: No module named 'katana_ai'\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'katana_ai'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/tmp\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkatana_ai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_node_property_list, visualize_embeddings, train_test_split_mask, save_features_to_graph\n\u001b[0;32m----> 7\u001b[0m \u001b[43mmy_graph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mg\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_feature_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/katana_enterprise/async_to_sync.py:249\u001b[0m, in \u001b[0;36mAsyncToSync.<locals>.do_wrap.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(underlying_func)\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m registry\u001b[38;5;241m.\u001b[39masync_to_sync(\n\u001b[0;32m--> 249\u001b[0m         \u001b[43munderlying_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m            \u001b[49m\u001b[43mget_self_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mregistry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msync_to_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mregistry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msync_to_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    254\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/katana_enterprise/async_to_sync.py:176\u001b[0m, in \u001b[0;36masync_to_sync.<locals>.wrapper\u001b[0;34m(timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m    166\u001b[0m     registry \u001b[38;5;241m=\u001b[39m AsyncToSyncClassRegistry\u001b[38;5;241m.\u001b[39mget()\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m registry\u001b[38;5;241m.\u001b[39masync_to_sync(\n\u001b[1;32m    168\u001b[0m         wait_for(\n\u001b[1;32m    169\u001b[0m             async_func(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    174\u001b[0m         )\n\u001b[1;32m    175\u001b[0m     )\n\u001b[0;32m--> 176\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwait_for\u001b[49m\u001b[43m(\u001b[49m\u001b[43masync_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/katana_enterprise/async_to_sync.py:147\u001b[0m, in \u001b[0;36mwait_for\u001b[0;34m(coro, timeout)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    146\u001b[0m     future \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mrun_coroutine_threadsafe(timeout_coro, loop\u001b[38;5;241m=\u001b[39mAsyncRunnerThread\u001b[38;5;241m.\u001b[39mget()\u001b[38;5;241m.\u001b[39mloop)\n\u001b[0;32m--> 147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m    149\u001b[0m     inner_future\u001b[38;5;241m.\u001b[39mcancel()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/concurrent/futures/_base.py:444\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 444\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    446\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/concurrent/futures/_base.py:389\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 389\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    390\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    391\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    392\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/asyncio/tasks.py:455\u001b[0m, in \u001b[0;36mwait_for\u001b[0;34m(fut, timeout, loop)\u001b[0m\n\u001b[1;32m    450\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe loop argument is deprecated since Python 3.8, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    451\u001b[0m                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand scheduled for removal in Python 3.10.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    452\u001b[0m                   \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    454\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 455\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m fut\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    458\u001b[0m     fut \u001b[38;5;241m=\u001b[39m ensure_future(fut, loop\u001b[38;5;241m=\u001b[39mloop)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/katana_enterprise/remote/aio/graph.py:515\u001b[0m, in \u001b[0;36mGraph.run\u001b[0;34m(self, function)\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[38;5;28mprint\u001b[39m(result\u001b[38;5;241m.\u001b[39mstdout, file\u001b[38;5;241m=\u001b[39msys\u001b[38;5;241m.\u001b[39mstdout, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    514\u001b[0m \u001b[38;5;28mprint\u001b[39m(result\u001b[38;5;241m.\u001b[39mstderr, file\u001b[38;5;241m=\u001b[39msys\u001b[38;5;241m.\u001b[39mstderr, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 515\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/katana_enterprise/remote/run_result.py:21\u001b[0m, in \u001b[0;36mRunResult.value\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalue\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 21\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise_if_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msuccess\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/katana_enterprise/remote/run_result.py:17\u001b[0m, in \u001b[0;36mRunResult.reraise_if_error\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreraise_if_error\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msuccess:\n\u001b[0;32m---> 17\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'katana_ai'"
     ]
    }
   ],
   "source": [
    "\n",
    "def run_feature_init(g): \n",
    "    import sys, os\n",
    "    sys.path.append(os.path.join(\"/tmp\"))\n",
    "    \n",
    "    from katana_ai import get_node_property_list, visualize_embeddings, train_test_split_mask, save_features_to_graph\n",
    "\n",
    "my_graph.run(lambda g: run_feature_init(g))\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2ee217-52b7-4e6a-b61f-13080a86245f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Analyze Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431e6fb4-d95c-44ef-be95-6cd508cda6a7",
   "metadata": {},
   "source": [
    "Generate run ID for Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de749d2b-da8b-4050-a090-9cc90342fefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id = uuid.uuid4().hex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401a94ae-f3b7-4f32-b679-0e1a3a58136c",
   "metadata": {},
   "source": [
    "Analyze initial feature vectors in Tensorboard on ```Projector``` tab. Features can be visualized using T-SNE / UMAP plots. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb84b3be-ac6c-490b-bf2f-4f235c4d45a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def analyze_features(g): \n",
    "    import sys, os\n",
    "    sys.path.append(os.path.join(\"/home/gsteck_katanagraph_com/solutions/fsi/src\"))\n",
    "    from katana_ai import visualize_embeddings\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "    writer = SummaryWriter(f\"/tmp/tensorboard/elliptic-embed-init-{run_id}\")\n",
    "    # analyze features in tensorboard\n",
    "    visualize_embeddings(g, writer, feature_name=\"h_init\", target_name=\"target\", filter_node_type=\"Classified_Txn\", sample_size=2000)\n",
    "    visualize_embeddings(g, writer, feature_name=\"local_feats\", target_name=\"target\", filter_node_type=\"Classified_Txn\", sample_size=2000)\n",
    "    visualize_embeddings(g, writer, feature_name=\"agg_feats\", target_name=\"target\", filter_node_type=\"Classified_Txn\", sample_size=2000)\n",
    "    \n",
    "graph.run(lambda g: analyze_features(g))\n",
    "print(f\"See results at https://demo-finance-tensorboard.katanagraph.com/\")\n",
    "print(f\"Run ID: elliptic-embed-init-{run_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc2899a-c85b-4cd4-8521-b05853114327",
   "metadata": {
    "tags": []
   },
   "source": [
    "## GNN Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7782cf9d-5bc0-4a68-852d-5528fcd8de5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "args = argparse.Namespace(\n",
    "    feat_name=\"h_init\",\n",
    "    label_name=\"target\",\n",
    "    label_dtype=numpy.float32,\n",
    "    split_name=\"train_test_val_mask\",\n",
    "    distributed_execution=True,\n",
    "    tensorboard_dir=f\"/tmp/tensorboard/elliptic-remote-{run_id}\",\n",
    "    model_dir=\"/tmp/models\",\n",
    "    katana_ai_dir=\"/home/gsteck_katanagraph_com/solutions/fsi/src\",\n",
    "    pred_node_label=\"Classified_Txn\",\n",
    "    pred_node_label_prop=\"node_type\",\n",
    "    pos_weight=8,\n",
    "    in_dim=165,\n",
    "    hidden_dim=256,\n",
    "    train_fan_in=\"100,100,100,100\",\n",
    "    test_fan_in=\"100,100,100,100\",\n",
    "    num_layers=4,\n",
    "    out_dim=1,\n",
    "    minibatch_size=1024,\n",
    "    max_minibatches=20,\n",
    "    lr=0.001,\n",
    "    dropout=0.2,\n",
    "    num_epochs=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea64fbf-5f1b-44bf-80b2-9804899e48ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_gnn(graph, args):\n",
    "    from calendar import c\n",
    "    import torch\n",
    "    import numpy\n",
    "    import katana\n",
    "    from katana_enterprise.distributed.pytorch import init_workers\n",
    "    from katana_enterprise.ai.data import PyGNodeSubgraphSampler, SampledSubgraphConfig \n",
    "    from katana_enterprise.ai.data import NodeDataLoader\n",
    "    from torch_geometric.nn import SAGEConv\n",
    "    from torch.nn.parallel import DistributedDataParallel as torch_DDP\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "    import sys, os\n",
    "    sys.path.append(os.path.join(args.katana_ai_dir))\n",
    "    from katana_ai import get_split, train_model\n",
    "    os.environ['MODIN_ENGINE']='python'\n",
    "    #katana.distributed.initialize()\n",
    "\n",
    "    katana.set_active_threads(32)\n",
    "    exec_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # model definition\n",
    "    class DistSAGE(torch.nn.Module):\n",
    "        def __init__(self, in_dim, hidden_dim, out_dim, num_layers,\n",
    "                     dropout):\n",
    "            super(DistSAGE, self).__init__()\n",
    "\n",
    "            self.convs = torch.nn.ModuleList()\n",
    "            self.convs.append(SAGEConv(in_dim, hidden_dim))\n",
    "            self.bns = torch.nn.ModuleList()\n",
    "            self.bns.append(torch.nn.BatchNorm1d(hidden_dim))\n",
    "            for _ in range(num_layers - 2):\n",
    "                self.convs.append(SAGEConv(hidden_dim, hidden_dim))\n",
    "                self.bns.append(torch.nn.BatchNorm1d(hidden_dim))\n",
    "            self.convs.append(SAGEConv(hidden_dim, out_dim))\n",
    "            self.activation = torch.nn.functional.relu\n",
    "            self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "        def reset_parameters(self):\n",
    "            for conv in self.convs:\n",
    "                conv.reset_parameters()\n",
    "            for bn in self.bns:\n",
    "                bn.reset_parameters()\n",
    "\n",
    "        def forward(self, data):\n",
    "            # unpack data loader\n",
    "            x, edges = data.x, data.adjs\n",
    "            #x.to(exec_device)\n",
    "            #edges.to(exec_device)\n",
    "\n",
    "            for i, conv in enumerate(self.convs):\n",
    "                # for multilayer, set x_target for each layer\n",
    "                x_target = x[:data.dest_count[i]]\n",
    "                x = conv((x, x_target), edges[i])\n",
    "                if i != len(self.convs) - 1:\n",
    "                    x = self.bns[i](x)\n",
    "                    x = self.activation(x)\n",
    "                    x = self.dropout(x) \n",
    "                    embed = x\n",
    "            return x, embed\n",
    "    \n",
    "    # initialize torch mpi process\n",
    "    main_start = time.time()\n",
    "    if args.distributed_execution:\n",
    "        init_workers()\n",
    "\n",
    "    # tensorboard writer\n",
    "    writer = SummaryWriter(args.tensorboard_dir)\n",
    "\n",
    "    # split train / test node idx\n",
    "    train_nodes = get_split(graph, 0, split_name=args.split_name, node_label=args.pred_node_label, node_label_prop=args.pred_node_label_prop)\n",
    "    test_nodes = get_split(graph, 1, split_name=args.split_name, node_label=args.pred_node_label, node_label_prop=args.pred_node_label_prop)\n",
    "\n",
    "    # initialize the multiminibatch sampler\n",
    "    train_sampler = PyGNodeSubgraphSampler(\n",
    "        graph, \n",
    "        SampledSubgraphConfig(\n",
    "        layer_fan=[int(fan_in) for fan_in in args.train_fan_in.split(',')], \n",
    "            max_minibatches=args.max_minibatches, \n",
    "            batch_props_to_pull=args.max_minibatches,\n",
    "            feat_prop_name=args.feat_name,\n",
    "            label_prop_name=args.label_name,\n",
    "            label_dtype=args.label_dtype,\n",
    "            multilayer_export=True\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # test sampler used for evaluation; it samples 100s per hop to simulate getting\n",
    "    test_sampler = PyGNodeSubgraphSampler(\n",
    "        graph, \n",
    "        SampledSubgraphConfig(\n",
    "        layer_fan=[int(fan_in) for fan_in in args.train_fan_in.split(',')], \n",
    "            max_minibatches=args.max_minibatches, \n",
    "            batch_props_to_pull=args.max_minibatches,\n",
    "            feat_prop_name=args.feat_name,\n",
    "            label_prop_name=args.label_name,\n",
    "            label_dtype=args.label_dtype,\n",
    "            pull_edge_types=args.load_edge_types,\n",
    "            multilayer_export=True\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # shuffle seeds between epochs + balance seed nodes across hosts\n",
    "    train_dataloader = NodeDataLoader(\n",
    "        train_sampler, \n",
    "        local_batch_size=args.minibatch_size, \n",
    "        node_ids=train_nodes,  \n",
    "        shuffle=True, \n",
    "        drop_last=True,\n",
    "        balance_seeds=True)\n",
    "    test_dataloader = NodeDataLoader(\n",
    "        test_sampler, \n",
    "        local_batch_size=args.minibatch_size, \n",
    "        node_ids=test_nodes, \n",
    "        balance_seeds=True)\n",
    "\n",
    "    # model initialization\n",
    "    model = DistSAGE(\n",
    "        in_dim=args.in_dim, \n",
    "        hidden_dim=args.hidden_dim, \n",
    "        out_dim=args.out_dim, \n",
    "        num_layers=args.num_layers,\n",
    "        dropout=args.dropout\n",
    "    ).to(exec_device)\n",
    "\n",
    "    if args.distributed_execution:\n",
    "        model = torch_DDP(model)\n",
    "    \n",
    "    # optimizer and loss fn\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "    loss_function = torch.nn.BCEWithLogitsLoss(pos_weight=torch.Tensor([args.pos_weight]))\n",
    "    \n",
    "    # train model\n",
    "    train_model(model, loss_function, optimizer, writer, train_dataloader, test_dataloader, args)\n",
    "    \n",
    "    # save model\n",
    "    #ts = time.time()\n",
    "    #torch.save(model.state_dict(), os.path.join(args.model_dir, 'graph_sage.'+str(ts)+'.pth'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44b22b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4c2b8563932708ac4f64e3bc4797ea8eb43fc7530baa7f01daadf5b46d7b1fd4"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
