{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064cde52-a7e1-486e-a7c4-3ce3aaaa01da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  This NoteBook contains code to run classic ML routines against a \n",
    "#  number of familiar data sets ..\n",
    "#\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d433bb83-5f71-406e-ab1c-1b0ef02583f1",
   "metadata": {},
   "source": [
    "#  Step 00: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba7589c-6f1e-4f5c-a37b-21f8944c2499",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Largely code to control how print statements and related work\n",
    "#\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%xmode Minimal\n",
    "\n",
    "\n",
    "#  Setting display options \n",
    "#\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.width\", 640)\n",
    "   #\n",
    "import numpy as np\n",
    "np.set_printoptions(edgeitems = 30, linewidth = 100000, \n",
    "   formatter = dict(float = lambda x: \"%.3g\" % x))\n",
    "\n",
    "#  Sets horizontal scroll for wide outputs\n",
    "#\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"\"))\n",
    "\n",
    "   ###\n",
    "    \n",
    "from tabulate import tabulate\n",
    "#\n",
    "#  How to use tabulate-\n",
    "#\n",
    "#  l_result = [{ \"col1\": 20, \"col2\": 30}]\n",
    "#  #\n",
    "#  print(tabulate(l_result, headers='keys', tablefmt='psql', showindex=False))\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a747a0-4aec-4cd2-bf38-8849deee7ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Helper functions for what we want to do below-\n",
    "#\n",
    "#  **  You must run this cell to do much of anything in this NoteBook\n",
    "\n",
    "#  We use these objects to store the history of results; display only\n",
    "#\n",
    "class HistoryIterator:\n",
    "   def __init__(self, history):\n",
    "       self._history = history\n",
    "       self._index = 0\n",
    "\n",
    "   def __next__(self):\n",
    "       if (self._index < len(self._history._events)):\n",
    "           result = (self._history._events[self._index][\"event\"] , self._history._events[self._index][\"measure\"])\n",
    "           self._index +=1\n",
    "           return result\n",
    "       raise StopIteration\n",
    "\n",
    "class History:\n",
    "   def __init__(self):\n",
    "      self._events = list()\n",
    "\n",
    "   def clear(self):\n",
    "      self._events = list()\n",
    "    \n",
    "   def add(self, event, measure):\n",
    "      self._events.append({\"event\": event, \"measure\": measure})\n",
    "\n",
    "   def __iter__(self):\n",
    "      return HistoryIterator(self)\n",
    "\n",
    "\n",
    "l_history = History()\n",
    "\n",
    "\n",
    "#  The sklearn ML routines follow a very consistent pattern. As such, we\n",
    "#  put these in a function, reduce redundant code below-\n",
    "#\n",
    "\n",
    "def do_model(i_routine, i_train_data, i_train_labels, i_test_data, i_test_labels, i_name_of_test):\n",
    "\n",
    "   #  Train whatever model\n",
    "   #\n",
    "   i_routine.fit(i_train_data, i_train_labels)\n",
    "   \n",
    "   #  Predict on the test data\n",
    "   #\n",
    "   l_predicted_labels = i_routine.predict(i_test_data)\n",
    "   l_accuracy         = (i_routine.score(i_test_data, i_test_labels) * 100)\n",
    "      #\n",
    "   l_history.add(event = i_name_of_test, measure = l_accuracy)\n",
    "   \n",
    "   #  Output results\n",
    "   #\n",
    "   print(i_name_of_test + \" ...\")\n",
    "   print(\"   Actual    labels from test......... %s\" % (i_test_labels     ) )\n",
    "   print(\"   Predicted labels from test......... %s\" % (l_predicted_labels) )\n",
    "   print(   \"   ###\")\n",
    "   print(\"   Accuracy: %0.4f %%\" % (l_accuracy))\n",
    "\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e65a137-2d39-411c-9316-aff11a9b6108",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Sometimes we want to clear history-\n",
    "#\n",
    "\n",
    "l_history.clear()\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebe2eac-169f-4068-b5a4-d70e1e2856a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  To add a blank line to history-\n",
    "#\n",
    "\n",
    "l_history.add(event = \"\", measure = \"\")\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317e2979-c711-419c-91b4-e035bafbe79c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#  Step A1: Iris Data load, encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d1be95-f9df-4435-ab56-add6aa68b8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Read the Iris data into a Pandas Dataframe\n",
    "#\n",
    "#     Features\n",
    "#     1. sepal length in cm\n",
    "#     2. sepal width in cm\n",
    "#     3. petal length in cm\n",
    "#     4. petal width in cm\n",
    "#     5. class: \n",
    "#        Iris-setosa\n",
    "#        Iris-versicolour\n",
    "#        Iris-virginica\n",
    "#\n",
    "#  To convert class into a numeric, we use sklearn.preprocessing.LabelEncoder\n",
    "#  See,\n",
    "#     https://www.turing.com/kb/convert-categorical-data-in-pandas-and-scikit-learn\n",
    "#\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "my_le = LabelEncoder()\n",
    "   #\n",
    "l_folder = \"20_Data\"\n",
    "l_file   = \"11_iris.data.txt\"\n",
    "\n",
    "\n",
    "pd_iris  = pd.read_csv((l_folder + \"/\" + l_file), header = 0, sep = \",\",\n",
    "   names = [\"sl\", \"sw\", \"pl\", \"pw\", \"class\"],\n",
    "   dtype = {\"sl\": \"float\", \"sw\": \"float\", \"pl\": \"float\", \"pw\": \"float\", \"class\": \"string\"} )\n",
    "      #\n",
    "pd_iris[\"class_encoded\"]  =  my_le.fit_transform(pd_iris[\"class\"])\n",
    "   #\n",
    "pd_iris = pd_iris.drop([\"class\"], axis = 1)\n",
    "    \n",
    "    \n",
    "#  Pandas.Dataframe.sample() returns a randomized set of rows, versus\n",
    "#  say head(), which always returns the first n ..\n",
    "#\n",
    "print(tabulate(pd_iris.sample(5), headers='keys', tablefmt='psql', showindex=False))\n",
    "print(\"Number of rows: %d\" % (len(pd_iris)))\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd64b3f-30aa-4680-b983-462d5da51304",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Split data into training and test.\n",
    "#  Convert the data into numpy arrays, since the ml libraries we use later expect that.\n",
    "#\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np_iris = {}\n",
    "   #\n",
    "np_iris[\"train\"], np_iris[\"test\"] = train_test_split(pd_iris.to_numpy(),              #  random_state calls to shuffle the data,\n",
    "   test_size = 0.20, random_state = 40)                                               #    which had arrived sorted\n",
    "                                                                                      #  10% yields way too high of an accuracy\n",
    "                                                                                      #    far below\n",
    "print(\"Number of total rows: %d   Training rows: %d   Test rows: %d\" %\n",
    "  (len(pd_iris), len(np_iris[\"train\"]), len(np_iris[\"test\"])) )\n",
    "\n",
    "print()\n",
    "print(\"Train data:\")\n",
    "print(\"%s\" % (np_iris[\"train\"][0:5]))\n",
    "print()\n",
    "print(\"Test  data:\")\n",
    "print(\"%s\" % (np_iris[\"test\" ][0:5]))\n",
    "print()\n",
    "   #\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62191e70-57c7-49d1-ba47-cca3517b0d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Boxplot, and normalize ..\n",
    "#\n",
    "#  Normalize from,\n",
    "#     https://datascience.stackexchange.com/questions/39142/normalize-matrix-in-python-numpy\n",
    "#\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "   #\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "print(type(np_iris[\"train\"]))\n",
    "\n",
    "plt.boxplot(np_iris[\"train\"])\n",
    "plt.show()\n",
    "\n",
    "   ###\n",
    "\n",
    "def my_normalize(X, x_min, x_max):\n",
    "   nom = (X-X.min(axis=0))*(x_max-x_min)\n",
    "   denom = X.max(axis=0) - X.min(axis=0)\n",
    "   denom[denom==0] = 1\n",
    "   return x_min + nom/denom \n",
    "\n",
    "#  If we normalize the \"class\" column, we lose the categorical nature\n",
    "#  of that data. So, create a deep copy, then just normalize the non-\n",
    "#  class columns.\n",
    "#\n",
    "np_iris[\"train_norm\"] = np.copy(np_iris[\"train\"])\n",
    "np_iris[\"test_norm\" ] = np.copy(np_iris[\"test\" ])\n",
    "   #\n",
    "np_iris[\"train_norm\"][:, :4] = my_normalize(np_iris[\"train_norm\"][:, :4], 0, 1)\n",
    "np_iris[\"test_norm\" ][:, :4] = my_normalize(np_iris[\"test_norm\" ][:, :4], 0, 1)\n",
    "\n",
    "plt.boxplot(np_iris[\"train_norm\"])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(\"--\")\n",
    "\n",
    "#  Legend:\n",
    "#\n",
    "#  Per a given column-\n",
    "#\n",
    "#    . The lower box line, marks 1st quartile. 25% of entries are below this line\n",
    "#      Middle line, 2nd.                       50% above/below\n",
    "#      Top box line, 3rd.                      25% above\n",
    "#\n",
    "#    . The true top and bottom lines-\n",
    "#      There's a formula for this; marks the boundary for entries considered outliers.\n",
    "#\n",
    "#           o            Outliers\n",
    "#           o\n",
    "#\n",
    "#          ---           Q3 + 1.5 QR\n",
    "#           |\n",
    "#           |\n",
    "#           |\n",
    "#           |\n",
    "#       +-------+        Q3      <-----------------+\n",
    "#       |       |                                  |\n",
    "#       |       |                                 1 QR\n",
    "#       +-------+        Median  (Q2)              |\n",
    "#       |       |                                  |\n",
    "#       |       |                                  |\n",
    "#       +-------+        Q1      <-----------------+\n",
    "#           |\n",
    "#           |\n",
    "#          ---           Q1 - 1.5 QR\n",
    "#\n",
    "#           o\n",
    "#           o\n",
    "#           o            Outliers\n",
    "#           o\n",
    "#\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5642d7fa-e89f-47c8-90c4-13e33fc2106c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#  Step A2: Iris Data train, test .. NearestCentroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d15b429-f361-434a-a3c4-f91f48d61824",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "\n",
    "#  Our numpy array has 5 columns, with the last column being the class.\n",
    "#  To review numpy array slicing,\n",
    "#\n",
    "#     To get the first 4 columns use,\n",
    "#        np_iris[\"train\"][:, :4]\n",
    "#     To get the last column use,\n",
    "#        np_iris[\"train\"][:, -1]\n",
    "#\n",
    "\n",
    "do_model(NearestCentroid(), np_iris[\"train\"][:, :4], np_iris[\"train\"][:, -1], np_iris[\"test\"][:, :4], np_iris[\"test\"][:, -1], \"Iris: Centroid\") \n",
    "print()\n",
    "print(tabulate(l_history, headers=[\"Category\", \"Accuracy\"], tablefmt='psql', showindex=False))\n",
    "\n",
    "#  do_model(NearestCentroid(), np_iris[\"train_norm\"][:, :4], np_iris[\"train_norm\"][:, -1], np_iris[\"test_norm\"][:, :4], np_iris[\"test_norm\"][:, -1], \"Iris: Centroid Normalized\") \n",
    "#  print()\n",
    "#  print(tabulate(l_history, headers=[\"Category\", \"Accuracy\"], tablefmt='psql', showindex=False))\n",
    "\n",
    "print(\"--\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c25a3d4-cf32-4503-8c86-e14146b7fa9d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#  Step A3: Iris Data train, test .. kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c1202c-c630-4e27-aea8-4ed425b139ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#  Our numpy array has 5 columns, with the last column being the class.\n",
    "#  To review numpy array slicing,\n",
    "#\n",
    "#     To get the first 4 columns use,\n",
    "#        np_iris[\"train\"][:, :4]\n",
    "#     To get the last column use,\n",
    "#        np_iris[\"train\"][:, -1]\n",
    "#\n",
    "\n",
    "do_model(KNeighborsClassifier(n_neighbors = 3), np_iris[\"train\"][:, :4], np_iris[\"train\"][:, -1], np_iris[\"test\"][:, :4], np_iris[\"test\"][:, -1], \"Iris: kNN=3\" ) \n",
    "print()\n",
    "print(tabulate(l_history, headers=[\"Category\", \"Accuracy\"], tablefmt='psql', showindex=False))\n",
    "\n",
    "#  do_model(KNeighborsClassifier(n_neighbors = 3), np_iris[\"train_norm\"][:, :4], np_iris[\"train_norm\"][:, -1], np_iris[\"test_norm\"][:, :4], np_iris[\"test_norm\"][:, -1], \"Iris: kNN=3 Normalized\" ) \n",
    "#  print()\n",
    "#  print(tabulate(l_history, headers=[\"Category\", \"Accuracy\"], tablefmt='psql', showindex=False))\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5328044-4f54-41b6-a37a-0f96a11e2250",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#  Step A4: Iris Data train, test .. Naive Bayes, Gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c1740a-f656-4d6d-bdf3-cb2f053fef60",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "#  Naive Bayes, Gaussian\n",
    "#\n",
    "#     Gaussian usually does better than the Multinomial below because,\n",
    "#        Gaussian expects continuous values\n",
    "#        Multinomial expects discreet values\n",
    "#\n",
    "#     And our values are continuous\n",
    "#\n",
    "\n",
    "do_model(GaussianNB(), np_iris[\"train\"][:, :4], np_iris[\"train\"][:, -1], np_iris[\"test\"][:, :4], np_iris[\"test\"][:, -1], \"Iris: GaussianNB\" ) \n",
    "print()\n",
    "print(tabulate(l_history, headers=[\"Category\", \"Accuracy\"], tablefmt='psql', showindex=False))\n",
    "\n",
    "#  do_model(GaussianNB(), np_iris[\"train_norm\"][:, :4], np_iris[\"train_norm\"][:, -1], np_iris[\"test_norm\"][:, :4], np_iris[\"test_norm\"][:, -1], \"Iris: GaussianNB Normalized\" ) \n",
    "#  print()\n",
    "#  print(tabulate(l_history, headers=[\"Category\", \"Accuracy\"], tablefmt='psql', showindex=False))\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f815d9-6eb7-4aaa-863c-3c7ce613d127",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#  Step A5: Iris Data train, test .. Naive Bayes, Multinomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de942e0e-31d0-44c1-bdb3-0ee3c5ca3315",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "#  Naive Bayes, Multinomial\n",
    "#\n",
    "\n",
    "do_model(MultinomialNB(), np_iris[\"train\"][:, :4], np_iris[\"train\"][:, -1], np_iris[\"test\"][:, :4], np_iris[\"test\"][:, -1], \"Iris: MultinomialNB\" ) \n",
    "print()\n",
    "print(tabulate(l_history, headers=[\"Category\", \"Accuracy\"], tablefmt='psql', showindex=False))\n",
    "\n",
    "#  do_model(MultinomialNB(), np_iris[\"train_norm\"][:, :4], np_iris[\"train_norm\"][:, -1], np_iris[\"test_norm\"][:, :4], np_iris[\"test_norm\"][:, -1], \"Iris: MultinomialNB Normalized\" ) \n",
    "#  print()\n",
    "#  print(tabulate(l_history, headers=[\"Category\", \"Accuracy\"], tablefmt='psql', showindex=False))\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662897e8-2b96-4d7a-bf24-74aa2da5c006",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#  Step A6: Iris Data train, test .. Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4adb7874-3d2c-4f27-83db-147b953228c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "#  Decision Tree\n",
    "#\n",
    "\n",
    "do_model(DecisionTreeClassifier(), np_iris[\"train\"][:, :4], np_iris[\"train\"][:, -1], np_iris[\"test\"][:, :4], np_iris[\"test\"][:, -1], \"Iris: DecisionTree\" ) \n",
    "print()\n",
    "print(tabulate(l_history, headers=[\"Category\", \"Accuracy\"], tablefmt='psql', showindex=False))\n",
    "\n",
    "#  do_model(DecisionTreeClassifier(), np_iris[\"train_norm\"][:, :4], np_iris[\"train_norm\"][:, -1], np_iris[\"test_norm\"][:, :4], np_iris[\"test_norm\"][:, -1], \"Iris: DecisionTree Normalized\" ) \n",
    "#  print()\n",
    "#  print(tabulate(l_history, headers=[\"Category\", \"Accuracy\"], tablefmt='psql', showindex=False))\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f5d6be-ecfa-4ca1-b518-0b37274c04de",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#  Step A7: Iris Data train, test .. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0a74fa-57c5-44ef-ade3-dd5438751064",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#  Random Forest\n",
    "#\n",
    "\n",
    "#  n_estimators, number of random trees created and trained\n",
    "#\n",
    "\n",
    "do_model(RandomForestClassifier(n_estimators = 5), np_iris[\"train\"][:, :4], np_iris[\"train\"][:, -1], np_iris[\"test\"][:, :4], np_iris[\"test\"][:, -1], \"Iris: RandomForest\" ) \n",
    "print()\n",
    "print(tabulate(l_history, headers=[\"Category\", \"Accuracy\"], tablefmt='psql', showindex=False))\n",
    "\n",
    "#  do_model(RandomForestClassifier(n_estimators = 5), np_iris[\"train_norm\"][:, :4], np_iris[\"train_norm\"][:, -1], np_iris[\"test_norm\"][:, :4], np_iris[\"test_norm\"][:, -1], \"Iris: RandomForest Normalized\" ) \n",
    "#  print()\n",
    "#  print(tabulate(l_history, headers=[\"Category\", \"Accuracy\"], tablefmt='psql', showindex=False))\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7973bf-5a1a-4999-810d-91e52c13bd0b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#  Step A8: Iris Data train, test .. Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2dc0262-190c-455d-b2f9-99056c1bea18",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "#  Support Vector Machine\n",
    "#\n",
    "#  We run this one with a number of configurations ..\n",
    "#\n",
    "#     C      ==  margin constant\n",
    "#     gamma  ==  used by the Gaussian kernel\n",
    "#\n",
    "\n",
    "do_model(SVC(kernel = \"linear\", C = 1.0), np_iris[\"train\"][:, :4], np_iris[\"train\"][:, -1], np_iris[\"test\"][:, :4], np_iris[\"test\"][:, -1], \"Iris: SVC/Linear\" ) \n",
    "print()\n",
    "print(tabulate(l_history, headers=[\"Category\", \"Accuracy\"], tablefmt='psql', showindex=False))\n",
    "print()\n",
    "             \n",
    "#  do_model(SVC(kernel = \"linear\", C = 1.0), np_iris[\"train_norm\"][:, :4], np_iris[\"train_norm\"][:, -1], np_iris[\"test_norm\"][:, :4], np_iris[\"test_norm\"][:, -1], \"Iris: SVC/Linear Normalized\" ) \n",
    "#  print()\n",
    "#  print(tabulate(l_history, headers=[\"Category\", \"Accuracy\"], tablefmt='psql', showindex=False))\n",
    "#  print()\n",
    "\n",
    "             \n",
    "do_model(SVC(kernel = \"rbf\", C = 1.0, gamma = 0.25), np_iris[\"train\"][:, :4], np_iris[\"train\"][:, -1], np_iris[\"test\"][:, :4], np_iris[\"test\"][:, -1], \"Iris: RBF\" ) \n",
    "print()\n",
    "print(tabulate(l_history, headers=[\"Category\", \"Accuracy\"], tablefmt='psql', showindex=False))\n",
    "print()\n",
    "             \n",
    "#  do_model(SVC(kernel = \"rbf\", C = 1.0, gamma = 0.25), np_iris[\"train_norm\"][:, :4], np_iris[\"train_norm\"][:, -1], np_iris[\"test_norm\"][:, :4], np_iris[\"test_norm\"][:, -1], \"Iris: RBF Normalized\" ) \n",
    "#  print()\n",
    "#  print(tabulate(l_history, headers=[\"Category\", \"Accuracy\"], tablefmt='psql', showindex=False))\n",
    "#  print()\n",
    "\n",
    "             \n",
    "do_model(SVC(kernel = \"rbf\", C = 1.0, gamma = 0.001), np_iris[\"train\"][:, :4], np_iris[\"train\"][:, -1], np_iris[\"test\"][:, :4], np_iris[\"test\"][:, -1], \"Iris: RBF 2\" ) \n",
    "print()\n",
    "print(tabulate(l_history, headers=[\"Category\", \"Accuracy\"], tablefmt='psql', showindex=False))\n",
    "print()\n",
    "\n",
    "#  do_model(SVC(kernel = \"rbf\", C = 1.0, gamma = 0.001), np_iris[\"train_norm\"][:, :4], np_iris[\"train_norm\"][:, -1], np_iris[\"test_norm\"][:, :4], np_iris[\"test_norm\"][:, -1], \"Iris: RBF 2 Normalized\" ) \n",
    "#  print()\n",
    "#  print(tabulate(l_history, headers=[\"Category\", \"Accuracy\"], tablefmt='psql', showindex=False))\n",
    "#  print()\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b45c758-b0db-43dd-9aa7-832598413085",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "####################################################################\n",
    "####################################################################\n",
    "####################################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3964740-6c41-4e86-8dd0-0cd5f7101cee",
   "metadata": {},
   "source": [
    "##  Block: All things Breast Cancer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc4cf6a-434a-4fcf-bfdb-edc3308c5252",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#  Step B1:  Breast Cancer Data load, encode, normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafe262a-4fb5-4954-9d6f-c8cbea09d763",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Read the Breast Cancer data into a Pandas Dataframe\n",
    "#\n",
    "#     Features\n",
    "#     1)     ID number\n",
    "#     2)     Diagnosis (M = malignant, B = benign)\n",
    "#     3-32)\n",
    "#       Ten real-valued features are computed for each cell nucleus:\n",
    "#     \n",
    "#     \ta) radius (mean of distances from center to points on the perimeter)\n",
    "#     \tb) texture (standard deviation of gray-scale values)\n",
    "#     \tc) perimeter\n",
    "#     \td) area\n",
    "#     \te) smoothness (local variation in radius lengths)\n",
    "#     \tf) compactness (perimeter^2 / area - 1.0)\n",
    "#     \tg) concavity (severity of concave portions of the contour)\n",
    "#     \th) concave points (number of concave portions of the contour)\n",
    "#     \ti) symmetry \n",
    "#     \tj) fractal dimension (\"coastline approximation\" - 1)\n",
    "#\n",
    "#  Sample data line,\n",
    "#     842302,M,\n",
    "#     17.99,    10.38,    122.8,    1001,    0.1184,    0.2776,    0.3001,    0.1471,    0.2419,    0.07871,         #  10 count\n",
    "#     1.095,    0.9053,   8.589,    153.4,   0.006399,  0.04904,   0.05373,   0.01587,   0.03003,   0.006193,\n",
    "#     25.38,    17.33,    184.6,    2019,    0.1622,    0.6656,    0.7119,    0.2654,    0.4601     ,0.1189\n",
    "#\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "my_le = LabelEncoder()\n",
    "   #\n",
    "l_folder = \"20_Data\"\n",
    "l_file   = \"22_wdbc.data.txt\"\n",
    "\n",
    "\n",
    "pd_bc  = pd.read_csv((l_folder + \"/\" + l_file), header = 0, sep = \",\",\n",
    "   names = [\"id\", \"class\",\n",
    "            \"f01\", \"f02\", \"f03\", \"f04\", \"f05\", \"f06\", \"f07\", \"f08\", \"f09\", \"f10\", \n",
    "            \"f11\", \"f12\", \"f13\", \"f14\", \"f15\", \"f16\", \"f17\", \"f18\", \"f19\", \"f20\", \n",
    "            \"f21\", \"f22\", \"f23\", \"f24\", \"f25\", \"f26\", \"f27\", \"f28\", \"f29\", \"f30\", \n",
    "           ],\n",
    "   dtype = {\"id\": \"int\", \"class\": \"string\",\n",
    "            \"f01\": \"float\", \"f02\": \"float\", \"f03\": \"float\", \"f04\": \"float\", \"f05\": \"float\", \"f06\": \"float\", \"f07\": \"float\", \"f08\": \"float\", \"f09\": \"float\", \"f10\": \"float\", \n",
    "            \"f11\": \"float\", \"f12\": \"float\", \"f13\": \"float\", \"f14\": \"float\", \"f15\": \"float\", \"f16\": \"float\", \"f17\": \"float\", \"f18\": \"float\", \"f19\": \"float\", \"f20\": \"float\", \n",
    "            \"f21\": \"float\", \"f22\": \"float\", \"f23\": \"float\", \"f24\": \"float\", \"f25\": \"float\", \"f26\": \"float\", \"f27\": \"float\", \"f28\": \"float\", \"f29\": \"float\", \"f30\": \"float\", \n",
    "           } )\n",
    "      #\n",
    "pd_bc[\"class_encoded\"]  =  my_le.fit_transform(pd_bc[\"class\"])\n",
    "   #\n",
    "pd_bc = pd_bc.drop([\"class\", \"id\"], axis = 1)\n",
    "    \n",
    "    \n",
    "#  Pandas.Dataframe.sample() returns a randomized count of rows\n",
    "#\n",
    "print(tabulate(pd_bc.sample(5), headers='keys', tablefmt='psql', showindex=False))\n",
    "print(\"Number of rows: %d\" % (len(pd_bc)))\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe05409-32ac-4da0-bd05-6d97b35df9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Split data into training and test.\n",
    "#  Convert the data into numpy arrays, since the ml libraries we use later expect that.\n",
    "#\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np_bc = {}\n",
    "   #\n",
    "np_bc[\"train\"], np_bc[\"test\"] = train_test_split(pd_bc.to_numpy(),                    #  random_state calls to shuffle the data,\n",
    "   test_size = 0.20, random_state = 40)                                               #    which had arrived sorted\n",
    "                                                                                      #  10% yields way too high of an accuracy\n",
    "                                                                                      #    far below\n",
    "print(\"Number of total rows: %d   Training rows: %d   Test rows: %d\" %\n",
    "  (len(pd_bc), len(np_bc[\"train\"]), len(np_bc[\"test\"])) )\n",
    "\n",
    "print()\n",
    "print(\"Train data:\")\n",
    "print(\"%s\" % (np_bc[\"train\"][0:5]))\n",
    "print()\n",
    "print(\"Test  data:\")\n",
    "print(\"%s\" % (np_bc[\"test\" ][0:5]))\n",
    "print()\n",
    "   #\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f885ac6-db00-4fd0-9ef3-c7b5bd130542",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Normalizing the data\n",
    "#\n",
    "\n",
    "def my_normalize(X, x_min, x_max):\n",
    "   nom = (X-X.min(axis=0))*(x_max-x_min)\n",
    "   denom = X.max(axis=0) - X.min(axis=0)\n",
    "   denom[denom==0] = 1\n",
    "   return x_min + nom/denom \n",
    "\n",
    "\n",
    "print(\"Number of columns in matrix: %d\" % (np_bc[\"train\"].shape[1]))\n",
    "      \n",
    "#  If we normalize the \"class\" column, we lose the categorical nature\n",
    "#  of that data. So, create a deep copy, then just normalize the non-\n",
    "#  class columns.\n",
    "#\n",
    "np_bc[\"train_norm\"] = np.copy(np_bc[\"train\"])\n",
    "np_bc[\"test_norm\" ] = np.copy(np_bc[\"test\" ])\n",
    "   #\n",
    "np_bc[\"train_norm\"][:, :30] = my_normalize(np_bc[\"train_norm\"][:, :30], 0, 1)\n",
    "np_bc[\"test_norm\" ][:, :30] = my_normalize(np_bc[\"test_norm\" ][:, :30], 0, 1)\n",
    "\n",
    "plt.boxplot(np_bc[\"train\"     ])\n",
    "plt.show()\n",
    "plt.boxplot(np_bc[\"train_norm\"])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a63d6f-96d6-42c3-9782-8be69c555bef",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#  Step B2:  Breast Cancer Data, run against all models .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf36b4ec-23da-4af1-8cb4-fe5669cc5cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  All of our model libraries were imported above, but ..\n",
    "#\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "#  To add a blank line to history-\n",
    "#\n",
    "l_history.add(event = \"\", measure = \"\")\n",
    "\n",
    "\n",
    "#  Our numpy array has many columns, with the last column being the class.\n",
    "#\n",
    "#  To review numpy array slicing,\n",
    "#\n",
    "#     To get the first 30 columns use,\n",
    "#        np_iris[\"train\"][:, :30]\n",
    "#     To get the last column use,\n",
    "#        np_iris[\"train\"][:, -1]\n",
    "#\n",
    "\n",
    "do_model(NearestCentroid(), np_bc[\"train\"][:, :30], np_bc[\"train\"][:, -1], np_bc[\"test\"][:, :30], np_bc[\"test\"][:, -1], \"Breast Cancer: Centroid\") \n",
    "#  do_model(NearestCentroid(), np_bc[\"train_norm\"][:, :30], np_bc[\"train_norm\"][:, -1], np_bc[\"test_norm\"][:, :30], np_bc[\"test_norm\"][:, -1], \"Breast Cancer: Centroid Normalized\") \n",
    "print()\n",
    "\n",
    "do_model(KNeighborsClassifier(n_neighbors = 3), np_bc[\"train\"][:, :30], np_bc[\"train\"][:, -1], np_bc[\"test\"][:, :30], np_bc[\"test\"][:, -1], \"Breast Cancer: kNN=3\") \n",
    "#  do_model(KNeighborsClassifier(n_neighbors = 3), np_bc[\"train_norm\"][:, :30], np_bc[\"train_norm\"][:, -1], np_bc[\"test_norm\"][:, :30], np_bc[\"test_norm\"][:, -1], \"Breast Cancer: kNN=3 Normalized\") \n",
    "print()\n",
    "\n",
    "do_model(GaussianNB(), np_bc[\"train\"][:, :30], np_bc[\"train\"][:, -1], np_bc[\"test\"][:, :30], np_bc[\"test\"][:, -1], \"Breast Cancer: GaussianNB\") \n",
    "#  do_model(GaussianNB(), np_bc[\"train_norm\"][:, :30], np_bc[\"train_norm\"][:, -1], np_bc[\"test_norm\"][:, :30], np_bc[\"test_norm\"][:, -1], \"Breast Cancer: GaussianNB Normalized\") \n",
    "print()\n",
    "\n",
    "do_model(MultinomialNB(), np_bc[\"train\"][:, :30], np_bc[\"train\"][:, -1], np_bc[\"test\"][:, :30], np_bc[\"test\"][:, -1], \"Breast Cancer: MultinomialNB\") \n",
    "#  do_model(MultinomialNB(), np_bc[\"train_norm\"][:, :30], np_bc[\"train_norm\"][:, -1], np_bc[\"test_norm\"][:, :30], np_bc[\"test_norm\"][:, -1], \"Breast Cancer: MultinomialNB Normalized\") \n",
    "print()\n",
    "\n",
    "do_model(DecisionTreeClassifier(), np_bc[\"train\"][:, :30], np_bc[\"train\"][:, -1], np_bc[\"test\"][:, :30], np_bc[\"test\"][:, -1], \"Breast Cancer: DecisionTree\") \n",
    "#  do_model(DecisionTreeClassifier(), np_bc[\"train_norm\"][:, :30], np_bc[\"train_norm\"][:, -1], np_bc[\"test_norm\"][:, :30], np_bc[\"test_norm\"][:, -1], \"Breast Cancer: DecisionTree Normalized\") \n",
    "print()\n",
    "\n",
    "#  n_estimators, number of random trees created and trained\n",
    "#\n",
    "do_model(RandomForestClassifier(n_estimators = 5), np_bc[\"train\"][:, :30], np_bc[\"train\"][:, -1], np_bc[\"test\"][:, :30], np_bc[\"test\"][:, -1], \"Breast Cancer: Random Forest = 5\") \n",
    "#  do_model(RandomForestClassifier(n_estimators = 5), np_bc[\"train_norm\"][:, :30], np_bc[\"train_norm\"][:, -1], np_bc[\"test_norm\"][:, :30], np_bc[\"test_norm\"][:, -1], \"Breast Cancer: Random Forest = 5 Normalized\") \n",
    "print()\n",
    "\n",
    "   ###\n",
    "\n",
    "#  Support Vector Machine\n",
    "#\n",
    "#  We run this one with a number of configurations ..\n",
    "#\n",
    "#     C      ==  margin constant\n",
    "#     gamma  ==  used by the Gaussian kernel\n",
    "#\n",
    "\n",
    "do_model(SVC(kernel = \"linear\", C = 1.0), np_bc[\"train\"][:, :30], np_bc[\"train\"][:, -1], np_bc[\"test\"][:, :30], np_bc[\"test\"][:, -1], \"Breast Cancer: SVC/Linear\") \n",
    "#  do_model(SVC(kernel = \"linear\", C = 1.0), np_bc[\"train_norm\"][:, :30], np_bc[\"train_norm\"][:, -1], np_bc[\"test_norm\"][:, :30], np_bc[\"test_norm\"][:, -1], \"Breast Cancer: SVC/Linear Normalized\") \n",
    "print()\n",
    "\n",
    "do_model(SVC(kernel = \"rbf\", C = 1.0, gamma = 1.0), np_bc[\"train\"][:, :30], np_bc[\"train\"][:, -1], np_bc[\"test\"][:, :30], np_bc[\"test\"][:, -1], \"Breast Cancer: RBF\") \n",
    "#  do_model(SVC(kernel = \"rbf\", C = 1.0, gamma = 1.0), np_bc[\"train_norm\"][:, :30], np_bc[\"train_norm\"][:, -1], np_bc[\"test_norm\"][:, :30], np_bc[\"test_norm\"][:, -1], \"Breast Cancer: RBF Normalized\") \n",
    "print()\n",
    "\n",
    "do_model(SVC(kernel = \"rbf\", C = 1.0, gamma = 0.001), np_bc[\"train\"][:, :30], np_bc[\"train\"][:, -1], np_bc[\"test\"][:, :30], np_bc[\"test\"][:, -1], \"Breast Cancer: RBF 2\") \n",
    "#  do_model(SVC(kernel = \"rbf\", C = 1.0, gamma = 0.001), np_bc[\"train_norm\"][:, :30], np_bc[\"train_norm\"][:, -1], np_bc[\"test_norm\"][:, :30], np_bc[\"test_norm\"][:, -1], \"Breast Cancer: RBF 2 Normalized\") \n",
    "print()\n",
    "\n",
    "\n",
    "print()\n",
    "print(tabulate(l_history, headers=[\"Category\", \"Accuracy\"], tablefmt='psql', showindex=False))\n",
    "print()\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9ab3c9-9466-440d-bab4-51c4369c11e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "####################################################################\n",
    "####################################################################\n",
    "####################################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25592f04-c344-4a58-8c1d-1d8684656a40",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#  Step C1:  MNist Data load, encode, other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09eb083d-60ba-4c42-b65c-f8f7c0864b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  We will be using Keras, so pip install it inside the Jupyter NoteBook container ..\n",
    "#\n",
    "\n",
    "l_package1 = \"keras\"\n",
    "l_package2 = \"tensorflow\"\n",
    "    \n",
    "def my_func(arg1):\n",
    "    \n",
    "   import sys\n",
    "   import subprocess\n",
    "    \n",
    "   subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", arg1 ])\n",
    "      #\n",
    "   print(\"From each host ..\")\n",
    "      #\n",
    "   return\n",
    "\n",
    "\n",
    "   ##########################################\n",
    "    \n",
    "\n",
    "print(\"Install Python Pip packages on Jupyter container ..\")\n",
    "   #\n",
    "my_return = my_func(l_package1)\n",
    "my_return = my_func(l_package2)\n",
    "print()\n",
    "    \n",
    "\n",
    "#  Use this if installing o nthe KGIP worker nodes ..\n",
    "#\n",
    "#  print(\"Install Python Pip packages on KGIP worker node containers ..\")\n",
    "#     # \n",
    "#  my_return = my_graph.run(lambda g: my_func(l_package))\n",
    "#  print()\n",
    "    \n",
    "print(\"--\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f9a359-02b1-4bb0-aced-216eb7cad1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Intead of loading MNist from disk, we load it from the Keras library ..\n",
    "#\n",
    "\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "\n",
    "\n",
    "np_mnist = {}\n",
    "   #\n",
    "(np_mnist[\"train\"], np_mnist[\"train_label\"]), (np_mnist[\"test\"], np_mnist[\"test_label\"]) = mnist.load_data()\n",
    "\n",
    "\n",
    "#  train and test ccome in as an array [(n), 28, 28] where n == 60000 for train,\n",
    "#  and 100000 for test\n",
    "#\n",
    "#  We need that 28*28 as a vector, so ..\n",
    "#\n",
    "np_mnist[\"train_v\"] = np_mnist[\"train\"].reshape(-1, 28*28)\n",
    "np_mnist[\"test_v\"]  = np_mnist[\"test\" ].reshape(-1, 28*28)\n",
    "\n",
    "\n",
    "print(\"Train shape ................ %s\" % (str(np_mnist[\"train\"].shape)))\n",
    "print(\"Train label shape .......... %s\" % (str(np_mnist[\"train_label\"].shape)))\n",
    "   #\n",
    "print(\"Test  shape ................ %s\" % (str(np_mnist[\"test\"].shape)))\n",
    "print(\"Test  label shape .......... %s\" % (str(np_mnist[\"test_label\"].shape)))\n",
    "   #\n",
    "print(\"Train vector shape ......... %s\" % (str(np_mnist[\"train_v\"].shape)))\n",
    "print(\"Test  vector shape ......... %s\" % (str(np_mnist[\"test_v\" ].shape)))\n",
    "   #\n",
    "print()\n",
    "\n",
    "\n",
    "#  tabulate() displays poorly with this wide data. Straight up print() works well.\n",
    "#\n",
    "# print(tabulate(np_mnist[\"train\"][0:2], headers='keys', tablefmt='psql', showindex=False))\n",
    "print(np_mnist[\"train\"][0:2])\n",
    "print(\"Number of rows: %d\" % (len(np_mnist[\"train\"])))\n",
    "   #\n",
    "print()\n",
    "\n",
    "print(np_mnist[\"train_label\"][0:2])\n",
    "print(\"Number of rows: %d\" % (len(np_mnist[\"train_label\"])))\n",
    "   #\n",
    "print()\n",
    "\n",
    "\n",
    "print(np_mnist[\"test\"][0:2])\n",
    "print(\"Number of rows: %d\" % (len(np_mnist[\"test\"])))\n",
    "   #\n",
    "print()\n",
    "\n",
    "print(np_mnist[\"test_label\"][0:2])\n",
    "print(\"Number of rows: %d\" % (len(np_mnist[\"test_label\"])))\n",
    "   #\n",
    "print()\n",
    "\n",
    "    \n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3148ac2-2ed4-433c-8825-745eb7e269b5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#  Step CN: Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c425b03-8d9c-4208-af6b-9f6115104b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Sample results\n",
    "#\n",
    "#     Train shape ................ (60000, 28, 28)\n",
    "#     Train label shape .......... (60000,)\n",
    "#     Test  shape ................ (10000, 28, 28)\n",
    "#     Test  label shape .......... (10000,)\n",
    "#     Train vector shape ......... (60000, 784)\n",
    "#     Test  vector shape ......... (10000, 784)\n",
    "#     \n",
    "#     [[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   3  18  18  18 126 136 175  26 166 255 247 127   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0  30  36  94 154 170 253 253 253 253 253 225 172 253 242 195  64   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0  49 238 253 253 253 253 253 253 253 253 251  93  82  82  56  39   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0  18 219 253 253 253 253 253 198 182 247 241   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0  80 156 107 253 253 205  11   0  43 154   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0  14   1 154 253  90   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0 139 253 190   2   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0  11 190 253  70   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0  35 241 225 160 108   1   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0  81 240 253 253 119  25   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  45 186 253 253 150  27   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  16  93 252 253 187   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 249 253 249  64   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  46 130 183 253 253 207   2   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0  39 148 229 253 253 253 250 182   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0  24 114 221 253 253 253 253 201  78   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0  23  66 213 253 253 253 253 198  81   2   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0  18 171 219 253 253 253 253 195  80   9   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0  55 172 226 253 253 253 253 244 133  11   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0 136 253 253 253 212 135 132  16   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]]\n",
    "#     \n",
    "#      [[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  51 159 253 159  50   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  48 238 252 252 252 237   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0  54 227 253 252 239 233 252  57   6   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0  10  60 224 252 253 252 202  84 252 253 122   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0 163 252 252 252 253 252 252  96 189 253 167   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0  51 238 253 253 190 114 253 228  47  79 255 168   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0  48 238 252 252 179  12  75 121  21   0   0 253 243  50   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0  38 165 253 233 208  84   0   0   0   0   0   0 253 252 165   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   7 178 252 240  71  19  28   0   0   0   0   0   0 253 252 195   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0  57 252 252  63   0   0   0   0   0   0   0   0   0 253 252 195   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0 198 253 190   0   0   0   0   0   0   0   0   0   0 255 253 196   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0  76 246 252 112   0   0   0   0   0   0   0   0   0   0 253 252 148   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0  85 252 230  25   0   0   0   0   0   0   0   0   7 135 253 186  12   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0  85 252 223   0   0   0   0   0   0   0   0   7 131 252 225  71   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0  85 252 145   0   0   0   0   0   0   0  48 165 252 173   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0  86 253 225   0   0   0   0   0   0 114 238 253 162   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0  85 252 249 146  48  29  85 178 225 253 223 167  56   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0  85 252 252 252 229 215 252 252 252 196 130   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0  28 199 252 252 253 252 252 233 145   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0  25 128 252 253 252 141  37   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]]]\n",
    "#     Number of rows: 60000\n",
    "#     \n",
    "#     [5 0]\n",
    "#     Number of rows: 60000\n",
    "#     \n",
    "#     [[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0  84 185 159 151  60  36   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0 222 254 254 254 254 241 198 198 198 198 198 198 198 198 170  52   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0  67 114  72 114 163 227 254 225 254 254 254 250 229 254 254 140   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0  17  66  14  67  67  67  59  21 236 254 106   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  83 253 209  18   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  22 233 255  83   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 129 254 238  44   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  59 249 254  62   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 133 254 187   5   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   9 205 248  58   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 126 254 182   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  75 251 240  57   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0  19 221 254 166   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   3 203 254 219  35   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0  38 254 254  77   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0  31 224 254 115   1   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0 133 254 254  52   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0  61 242 254 254  52   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0 121 254 254 219  40   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0 121 254 207  18   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]]\n",
    "#     \n",
    "#      [[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0 116 125 171 255 255 150  93   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0 169 253 253 253 253 253 253 218  30   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0 169 253 253 253 213 142 176 253 253 122   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0  52 250 253 210  32  12   0   6 206 253 140   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0  77 251 210  25   0   0   0 122 248 253  65   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0  31  18   0   0   0   0 209 253 253  65   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0 117 247 253 198  10   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0  76 247 253 231  63   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0 128 253 253 144   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0 176 246 253 159  12   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0  25 234 253 233  35   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0 198 253 253 141   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0  78 248 253 189  12   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0  19 200 253 253 141   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0 134 253 253 173  12   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0 248 253 253  25   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0 248 253 253  43  20  20  20  20   5   0   5  20  20  37 150 150 150 147  10   0]\n",
    "#       [  0   0   0   0   0   0   0   0 248 253 253 253 253 253 253 253 168 143 166 253 253 253 253 253 253 253 123   0]\n",
    "#       [  0   0   0   0   0   0   0   0 174 253 253 253 253 253 253 253 253 253 253 253 249 247 247 169 117 117  57   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0 118 123 123 123 166 253 253 253 155 123 123  41   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
    "#       [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]]]\n",
    "#     Number of rows: 10000\n",
    "#     \n",
    "#     [7 2]\n",
    "#     Number of rows: 10000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060a3191-9af2-4937-9221-3499953b67a1",
   "metadata": {
    "tags": []
   },
   "source": [
    "#  Step CN: End of results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6cda4d-95a6-46ac-8871-aa7826962680",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#  Step C2: MNist train, test .. (All)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6bcf95-6f46-448e-a618-4bf0fb15e098",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Sometimes we want to clear history-\n",
    "#\n",
    "\n",
    "l_history.clear()\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97564456-56d7-4f55-b4d8-94ec6ddde2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  All of our model libraries were imported above, but ..\n",
    "#\n",
    "\n",
    "#  Here we run given ML routines against the MNist data set\n",
    "#\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "#  Adding these to the above\n",
    "#\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import decomposition\n",
    "\n",
    "import time\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e883ec4-3c77-43cd-8d71-588f58f12503",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  To add a blank line to history-\n",
    "#\n",
    "l_history.add(event = \"\", measure = \"\")\n",
    "\n",
    "\n",
    "do_model(NearestCentroid(), np_mnist[\"train_v\"], np_mnist[\"train_label\"], np_mnist[\"test_v\"], np_mnist[\"test_label\"], \"MNist: Centroid\") \n",
    "print()\n",
    "\n",
    "do_model(KNeighborsClassifier(n_neighbors =  3), np_mnist[\"train_v\"], np_mnist[\"train_label\"], np_mnist[\"test_v\"], np_mnist[\"test_label\"], \"MNist: kNN=3\" ) \n",
    "do_model(KNeighborsClassifier(n_neighbors =  7), np_mnist[\"train_v\"], np_mnist[\"train_label\"], np_mnist[\"test_v\"], np_mnist[\"test_label\"], \"MNist: kNN=7\") \n",
    "print()\n",
    "\n",
    "do_model(GaussianNB(), np_mnist[\"train_v\"], np_mnist[\"train_label\"], np_mnist[\"test_v\"], np_mnist[\"test_label\"], \"MNist: GaussianNB\") \n",
    "print()\n",
    "\n",
    "do_model(MultinomialNB(), np_mnist[\"train_v\"], np_mnist[\"train_label\"], np_mnist[\"test_v\"], np_mnist[\"test_label\"], \"MNist: MultinomialNB\") \n",
    "print()\n",
    "\n",
    "do_model(DecisionTreeClassifier(), np_mnist[\"train_v\"], np_mnist[\"train_label\"], np_mnist[\"test_v\"], np_mnist[\"test_label\"], \"MNist: DecisionTree\") \n",
    "print()\n",
    "\n",
    "#  n_estimators, number of random trees created and trained\n",
    "#\n",
    "do_model(RandomForestClassifier(n_estimators = 5   ), np_mnist[\"train_v\"], np_mnist[\"train_label\"], np_mnist[\"test_v\"], np_mnist[\"test_label\"], \"MNist: Random Forest = 5   \") \n",
    "do_model(RandomForestClassifier(n_estimators = 50  ), np_mnist[\"train_v\"], np_mnist[\"train_label\"], np_mnist[\"test_v\"], np_mnist[\"test_label\"], \"MNist: Random Forest = 50  \") \n",
    "do_model(RandomForestClassifier(n_estimators = 500 ), np_mnist[\"train_v\"], np_mnist[\"train_label\"], np_mnist[\"test_v\"], np_mnist[\"test_label\"], \"MNist: Random Forest = 500 \") \n",
    "do_model(RandomForestClassifier(n_estimators = 5000), np_mnist[\"train_v\"], np_mnist[\"train_label\"], np_mnist[\"test_v\"], np_mnist[\"test_label\"], \"MNist: Random Forest = 5000\") \n",
    "\n",
    "print()\n",
    "\n",
    "   ###\n",
    "\n",
    "#  Support Vector Machine\n",
    "#\n",
    "#  We run this one with a number of configurations ..\n",
    "#\n",
    "#     C      ==  margin constant\n",
    "#     gamma  ==  used by the Gaussian kernel\n",
    "#\n",
    "\n",
    "#  As configured, these throw warnings, never settle ..\n",
    "#\n",
    "\n",
    "#  do_model(LinearSVC(C = 0.01), np_mnist[\"train_v\"], np_mnist[\"train_label\"], np_mnist[\"test_v\"], np_mnist[\"test_label\"], \"MNist: LinearSVC c=0.01   \") \n",
    "#  print()\n",
    "#  do_model(LinearSVC(C = 0.1 ), np_mnist[\"train_v\"], np_mnist[\"train_label\"], np_mnist[\"test_v\"], np_mnist[\"test_label\"], \"MNist: LinearSVC c=0.1    \") \n",
    "#  print()\n",
    "#  do_model(LinearSVC(C = 1.0 ), np_mnist[\"train_v\"], np_mnist[\"train_label\"], np_mnist[\"test_v\"], np_mnist[\"test_label\"], \"MNist: LinearSVC c=1.0    \") \n",
    "#  print()\n",
    "#  do_model(LinearSVC(C = 10.0), np_mnist[\"train_v\"], np_mnist[\"train_label\"], np_mnist[\"test_v\"], np_mnist[\"test_label\"], \"MNist: LinearSVC c=10.0   \") \n",
    "#  print()\n",
    "\n",
    "print()\n",
    "print(tabulate(l_history, headers=[\"Category\", \"Accuracy\"], tablefmt='psql', showindex=False))\n",
    "print()\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fac6b7c-b45d-49fa-95ba-e5cafed4af26",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#  Step CN: Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1b1c44-a03b-4965-9f81-c838e23136ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Sample results\n",
    "#\n",
    "#     MNist: Centroid ...\n",
    "#        Actual    labels from test......... [7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4 9 6 6 5 4 0 7 4 0 1 ... 5 2 4 9 4 3 6 4 1 7 2 6 5 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6]\n",
    "#        Predicted labels from test......... [7 2 1 0 4 1 4 9 2 9 0 2 9 0 1 5 9 7 3 4 7 6 4 5 4 0 7 4 0 1 ... 3 2 4 9 4 2 6 4 1 7 0 6 6 0 1 8 8 4 5 6 7 8 4 0 1 2 3 4 5 6]\n",
    "#        ###\n",
    "#        Accuracy: 82.0300 %\n",
    "#     \n",
    "#     MNist: kNN=3 ...\n",
    "#        Actual    labels from test......... [7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4 9 6 6 5 4 0 7 4 0 1 ... 5 2 4 9 4 3 6 4 1 7 2 6 5 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6]\n",
    "#        Predicted labels from test......... [7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4 9 6 6 5 4 0 7 4 0 1 ... 5 2 4 9 4 3 6 4 1 7 2 6 3 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6]\n",
    "#        ###\n",
    "#        Accuracy: 97.0500 %\n",
    "#     MNist: kNN=7 ...\n",
    "#        Actual    labels from test......... [7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4 9 6 6 5 4 0 7 4 0 1 ... 5 2 4 9 4 3 6 4 1 7 2 6 5 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6]\n",
    "#        Predicted labels from test......... [7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4 9 6 6 5 4 0 7 4 0 1 ... 5 2 4 9 4 3 6 4 1 7 2 6 6 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6]\n",
    "#        ###\n",
    "#        Accuracy: 96.9400 %\n",
    "#     \n",
    "#     MNist: GaussianNB ...\n",
    "#        Actual    labels from test......... [7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4 9 6 6 5 4 0 7 4 0 1 ... 5 2 4 9 4 3 6 4 1 7 2 6 5 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6]\n",
    "#        Predicted labels from test......... [9 2 1 0 9 1 8 9 4 9 0 6 9 0 1 0 9 7 2 9 9 6 6 8 9 0 7 9 0 1 ... 6 0 8 9 8 8 6 9 1 9 3 6 6 0 1 9 8 9 8 6 9 8 9 0 1 8 8 9 8 6]\n",
    "#        ###\n",
    "#        Accuracy: 55.5800 %\n",
    "#     \n",
    "#     MNist: MultinomialNB ...\n",
    "#        Actual    labels from test......... [7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4 9 6 6 5 4 0 7 4 0 1 ... 5 2 4 9 4 3 6 4 1 7 2 6 5 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6]\n",
    "#        Predicted labels from test......... [7 2 1 0 4 1 4 9 4 9 0 6 9 0 1 3 9 7 3 4 9 6 6 5 4 0 7 4 0 1 ... 2 2 4 9 4 2 6 4 1 7 2 6 6 0 1 8 8 4 5 6 7 8 9 0 1 2 3 9 8 6]\n",
    "#        ###\n",
    "#        Accuracy: 83.6500 %\n",
    "#     \n",
    "#     MNist: DecisionTree ...\n",
    "#        Actual    labels from test......... [7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4 9 6 6 5 4 0 7 4 0 1 ... 5 2 4 9 4 3 6 4 1 7 2 6 5 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6]\n",
    "#        Predicted labels from test......... [7 2 1 0 4 1 4 7 6 9 0 6 9 0 1 5 9 7 6 4 9 6 6 5 4 0 7 4 0 1 ... 5 2 4 9 4 3 8 4 1 7 5 6 8 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6]\n",
    "#        ###\n",
    "#        Accuracy: 87.6700 %\n",
    "#     \n",
    "#     MNist: Random Forest = 5    ...\n",
    "#        Actual    labels from test......... [7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4 9 6 6 5 4 0 7 4 0 1 ... 5 2 4 9 4 3 6 4 1 7 2 6 5 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6]\n",
    "#        Predicted labels from test......... [7 2 1 0 4 1 4 9 4 9 0 6 9 0 1 5 9 7 3 4 9 6 6 5 4 0 7 4 0 1 ... 5 2 4 9 4 2 6 4 1 7 8 6 6 0 1 2 3 4 5 6 7 3 9 0 1 2 3 4 5 6]\n",
    "#        ###\n",
    "#        Accuracy: 91.9100 %\n",
    "#     MNist: Random Forest = 50   ...\n",
    "#        Actual    labels from test......... [7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4 9 6 6 5 4 0 7 4 0 1 ... 5 2 4 9 4 3 6 4 1 7 2 6 5 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6]\n",
    "#        Predicted labels from test......... [7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4 9 6 6 5 4 0 7 4 0 1 ... 5 2 4 9 4 2 6 4 1 7 3 6 3 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6]\n",
    "#        ###\n",
    "#        Accuracy: 96.7000 %\n",
    "#     MNist: Random Forest = 500  ...\n",
    "#        Actual    labels from test......... [7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4 9 6 6 5 4 0 7 4 0 1 ... 5 2 4 9 4 3 6 4 1 7 2 6 5 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6]\n",
    "#        Predicted labels from test......... [7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4 9 6 6 5 4 0 7 4 0 1 ... 5 2 4 9 4 2 6 4 1 7 2 6 6 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6]\n",
    "#        ###\n",
    "#        Accuracy: 97.0400 %\n",
    "#     MNist: Random Forest = 5000 ...\n",
    "#        Actual    labels from test......... [7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4 9 6 6 5 4 0 7 4 0 1 ... 5 2 4 9 4 3 6 4 1 7 2 6 5 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6]\n",
    "#        Predicted labels from test......... [7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4 9 6 6 5 4 0 7 4 0 1 ... 3 2 4 9 4 2 6 4 1 7 2 6 6 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6]\n",
    "#        ###\n",
    "#        Accuracy: 97.1800 %\n",
    "#     \n",
    "#     /opt/conda/lib/python3.8/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
    "#       warnings.warn(\n",
    "#     \n",
    "#     MNist: LinearSVC c=0.01    ...\n",
    "#        Actual    labels from test......... [7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4 9 6 6 5 4 0 7 4 0 1 ... 5 2 4 9 4 3 6 4 1 7 2 6 5 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6]\n",
    "#        Predicted labels from test......... [7 2 1 0 4 1 4 9 6 9 0 6 9 0 1 5 9 7 2 4 9 6 6 5 4 0 7 4 0 1 ... 3 2 4 9 4 3 6 4 1 7 3 6 6 0 1 2 3 4 5 6 7 3 9 0 1 2 3 4 5 6]\n",
    "#        ###\n",
    "#        Accuracy: 87.1200 %\n",
    "#     \n",
    "#     /opt/conda/lib/python3.8/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
    "#       warnings.warn(\n",
    "#     \n",
    "#     MNist: LinearSVC c=0.1     ...\n",
    "#        Actual    labels from test......... [7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4 9 6 6 5 4 0 7 4 0 1 ... 5 2 4 9 4 3 6 4 1 7 2 6 5 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6]\n",
    "#        Predicted labels from test......... [7 2 1 0 4 1 4 1 6 4 0 6 9 0 1 5 9 7 2 4 9 6 6 5 4 0 7 4 0 1 ... 3 2 4 4 4 2 6 4 1 7 3 6 6 0 1 2 3 4 5 6 7 3 4 0 1 2 3 4 5 6]\n",
    "#        ###\n",
    "#        Accuracy: 86.4700 %\n",
    "#     \n",
    "#     /opt/conda/lib/python3.8/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
    "#       warnings.warn(\n",
    "#     \n",
    "#     MNist: LinearSVC c=1.0     ...\n",
    "#        Actual    labels from test......... [7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4 9 6 6 5 4 0 7 4 0 1 ... 5 2 4 9 4 3 6 4 1 7 2 6 5 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6]\n",
    "#        Predicted labels from test......... [7 2 1 0 4 1 4 9 4 9 0 6 9 0 1 5 9 7 3 4 9 6 6 5 4 0 7 4 0 1 ... 3 2 4 9 4 3 6 4 1 7 3 6 2 0 1 2 3 4 5 6 7 3 9 0 1 2 3 4 5 6]\n",
    "#        ###\n",
    "#        Accuracy: 83.9900 %\n",
    "#     \n",
    "#     MNist: LinearSVC c=10.0    ...\n",
    "#        Actual    labels from test......... [7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4 9 6 6 5 4 0 7 4 0 1 ... 5 2 4 9 4 3 6 4 1 7 2 6 5 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6]\n",
    "#        Predicted labels from test......... [7 2 1 0 4 1 8 9 6 9 0 6 9 0 1 5 9 7 3 4 9 6 6 5 4 0 7 4 0 1 ... 3 2 4 9 4 3 6 4 1 7 3 6 6 0 1 8 8 4 5 6 7 8 9 0 1 8 3 5 5 6]\n",
    "#        ###\n",
    "#        Accuracy: 83.6900 %\n",
    "#     \n",
    "#     \n",
    "#     +-----------------------------+------------+\n",
    "#     | Category                    | Accuracy   |\n",
    "#     |-----------------------------+------------|\n",
    "#     |                             |            |\n",
    "#     | MNist: Centroid             | 82.03      |\n",
    "#     | MNist: kNN=3                | 97.05      |\n",
    "#     | MNist: kNN=7                | 96.94      |\n",
    "#     | MNist: GaussianNB           | 55.58      |\n",
    "#     | MNist: MultinomialNB        | 83.65      |\n",
    "#     | MNist: DecisionTree         | 87.67      |\n",
    "#     | MNist: Random Forest = 5    | 91.91      |\n",
    "#     | MNist: Random Forest = 50   | 96.7       |\n",
    "#     | MNist: Random Forest = 500  | 97.04      |\n",
    "#     | MNist: Random Forest = 5000 | 97.18      |\n",
    "#     | MNist: LinearSVC c=0.01     | 87.12      |\n",
    "#     | MNist: LinearSVC c=0.1      | 86.47      |\n",
    "#     | MNist: LinearSVC c=1.0      | 83.99      |\n",
    "#     | MNist: LinearSVC c=10.0     | 83.69      |\n",
    "#     +-----------------------------+------------+\n",
    "#     \n",
    "#     --\n",
    "#     \n",
    "#     /opt/conda/lib/python3.8/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
    "#       warnings.warn(\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8612d30f-3774-4620-a843-7c04c7a6095f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Regarding this,\n",
    "#\n",
    "#     /opt/conda/lib/python3.8/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning:\n",
    "#        Liblinear failed to converge, increase the number of iterations.\n",
    "#        warnings.warn(\n",
    "#\n",
    "#  From,\n",
    "#     https://stackoverflow.com/questions/52670012/convergencewarning-liblinear-failed-to-converge-increase-the-number-of-iterati\n",
    "#\n",
    "#      Normally when an optimization algorithm does not converge, it is usually because the problem is not well-conditioned,\n",
    "#      perhaps due to a poor scaling of the decision variables. There are a few things you can try.\n",
    "#      \n",
    "#          Normalize your training data so that the problem hopefully becomes more well conditioned, which in turn can speed up\n",
    "#          convergence. One possibility is to scale your data to 0 mean, unit standard deviation using Scikit-Learn's StandardScaler\n",
    "#          for an example.\n",
    "#\n",
    "#          Note that you have to apply the StandardScaler fitted on the training data to the test data. Also, if you have discrete\n",
    "#          features, make sure they are transformed properly so that scaling them makes sense.\n",
    "#\n",
    "#          Related to 1), make sure the other arguments such as regularization weight, C, is set appropriately. C has to be > 0.\n",
    "#          Typically one would try various values of C in a logarithmic scale (1e-5, 1e-4, 1e-3, ..., 1, 10, 100, ...) before\n",
    "#          finetuning it at finer granularity within a particular interval. These days, it probably make more sense to tune\n",
    "#          parameters using, for e.g., Bayesian Optimization using a package such as Scikit-Optimize.\n",
    "#\n",
    "#          Set max_iter to a larger value. The default is 1000. This should be your last resort. If the optimization process does\n",
    "#          not converge within the first 1000 iterations, having it converge by setting a larger max_iter typically masks other\n",
    "#          problems such as those described in 1) and 2). It might even indicate that you have some in appropriate features or\n",
    "#          strong correlations in the features. Debug those first before taking this easy way out.\n",
    "#\n",
    "#          Set dual = True if number of features > number of examples and vice versa. This solves the SVM optimization problem using\n",
    "#          the dual formulation. Thanks @Nino van Hooff for pointing this out, and @JamesKo for spotting my mistake.\n",
    "#\n",
    "#          Use a different solver, for e.g., the L-BFGS solver if you are using Logistic Regression. See @5ervant's answer.\n",
    "#      \n",
    "#      Note: One should not ignore this warning.\n",
    "#      \n",
    "#      This warning came about because\n",
    "#      \n",
    "#          Solving the linear SVM is just solving a quadratic optimization problem. The solver is typically an iterative algorithm\n",
    "#          that keeps a running estimate of the solution (i.e., the weight and bias for the SVM). It stops running when the solution\n",
    "#          corresponds to an objective value that is optimal for this convex optimization problem, or when it hits the maximum number\n",
    "#          of iterations set.\n",
    "#      \n",
    "#          If the algorithm does not converge, then the current estimate of the SVM's parameters are not guaranteed to be any good, \n",
    "#          hence the predictions can also be complete garbage.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b3b1a5-ff23-4af4-bc41-4798e0c58487",
   "metadata": {},
   "source": [
    "#  Step CN: End of results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013d986c-991a-472c-b2a0-62ac654473fc",
   "metadata": {},
   "source": [
    "#  Step C3: Compare MNist test when data is randomized .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff57e71-9dc7-4e7c-9e6c-eb447e07867a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#  Effect of randomness, moving the bits around inside each image\n",
    "#\n",
    "\n",
    "#  The following variables are in scope ..\n",
    "#\n",
    "#     np_mnist[\"train\"] \n",
    "#     np_mnist[\"train_label\"]\n",
    "#     np_mnist[\"test\"]\n",
    "#     np_mnist[\"test_label\"]\n",
    "#     np_mnist[\"train_v\"]           #  vectors of the two data sets above\n",
    "#     np_mnist[\"test_v\"] \n",
    "#\n",
    "\n",
    "#  Here we want to copy the two \"v\" arrays and randomize them\n",
    "#\n",
    "np_mnist[\"train_v_s\"] = np.copy(np_mnist[\"train_v\"])\n",
    "np_mnist[\"test_v_s\" ] = np.copy(np_mnist[\"test_v\" ])\n",
    "   #\n",
    "for i in range(np_mnist[\"train_v_s\"].shape[0]):\n",
    "   np.random.shuffle(np_mnist[\"train_v_s\"][i, :])\n",
    "for i in range(np_mnist[\"test_v_s\" ].shape[0]):\n",
    "   np.random.shuffle(np_mnist[\"test_v_s\" ][i, :])\n",
    "\n",
    "\n",
    "#  Looking at the non-scrambled, and yes-scrambled data\n",
    "#\n",
    "#  Currently the data lives as a vector. To look at it, copy\n",
    "#  it back to a 28*28 numpy array. We only need this for two\n",
    "#  rows we wish to view, and we choose to use test.\n",
    "#\n",
    "np_mnist[\"test_s\"] = np.zeros((2, np_mnist[\"test\"].shape[1], np_mnist[\"test\"].shape[2]), dtype = int)\n",
    "   #\n",
    "for i in range(np_mnist[\"test_s\"].shape[0]):\n",
    "   np_mnist[\"test_s\"][i,:,:] = np_mnist[\"test_v_s\"][i].reshape(28, 28)\n",
    "\n",
    "#  And the actual print\n",
    "#\n",
    "#  Non-randomized bits\n",
    "#\n",
    "print(np_mnist[\"test_label\"][0:2])\n",
    "print(np_mnist[\"test\"][0:2])\n",
    "print(\"Number of rows: %d\" % (len(np_mnist[\"test\"])))\n",
    "   #\n",
    "print()\n",
    "\n",
    "#  Problems with print formatting. These lines help\n",
    "#\n",
    "np.set_printoptions()\n",
    "np.set_printoptions(edgeitems = 30, linewidth = 100000,  formatter = dict(int = lambda x: \"%3i\" % x))\n",
    "\n",
    "#  Randomized bits\n",
    "#\n",
    "print(np_mnist[\"test_label\"][0:2])\n",
    "print(np_mnist[\"test_s\"][0:2])\n",
    "print(\"Number of rows: %d\" % (len(np_mnist[\"test_s\"])))\n",
    "   #\n",
    "print()\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e9b208e4-2d65-4efa-9ed1-f4f51be6bf35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "#  Creating a bar chart; Are these the same values ?\n",
    "#\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "l_hs = np.hstack(np_mnist[\"test_v_s\"][0])\n",
    "_ = plt.hist(l_hs, bins='auto') \n",
    "   #\n",
    "plt.title(\"784 possible values: Image pre-randomization\")\n",
    "plt.xlabel('RGB Value')\n",
    "plt.ylabel('Count of Said Value')\n",
    "   #\n",
    "plt.show()\n",
    "\n",
    "l_hs = np.hstack(np_mnist[\"test_v_s\"][0])\n",
    "_ = plt.hist(l_hs, bins='auto') \n",
    "   #\n",
    "plt.title(\"784 possible values: Image post-randomization\")\n",
    "plt.xlabel('RGB Value')\n",
    "plt.ylabel('Count of Said Value')\n",
    "   #\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92dc972a-abf1-433d-8ecb-d58699a6fbe1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c40de98e-9e12-4ee5-bc98-b3ab683ade35",
   "metadata": {
    "tags": []
   },
   "source": [
    "#  Step CN: Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd68398d-130c-4359-8e90-1c28f076a60d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1983d12-36a3-42fd-83dc-3fb41fe464d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Sample results\n",
    "#\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa52d68-ea0e-4304-852b-97e2f0e0495a",
   "metadata": {},
   "source": [
    "#  Step CN: End of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1eae7a2-b06a-435c-9930-13fed78717b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6316f92-89a4-440c-9861-090b6f6e8fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  The results below were on images that were not scrambled.\n",
    "#\n",
    "#     +-----------------------------+-------------------+\n",
    "#     | Category                    | Accuracy          |\n",
    "#     |-----------------------------+-------------------|\n",
    "#     | MNist: Centroid             | 82.03             |\n",
    "#     | MNist: kNN=3                | 97.05             |\n",
    "#     | MNist: kNN=7                | 96.94             |\n",
    "#     | MNist: GaussianNB           | 55.58             |\n",
    "#     | MNist: MultinomialNB        | 83.65             |\n",
    "#     | MNist: DecisionTree         | 87.72             |\n",
    "#     | MNist: Random Forest = 5    | 92.36999999999999 |\n",
    "#     | MNist: Random Forest = 50   | 96.67999999999999 |\n",
    "#     | MNist: Random Forest = 500  | 97.15             |\n",
    "#     | MNist: Random Forest = 5000 | 97.17             |\n",
    "#     |                             |                   |\n",
    "#     +-----------------------------+-------------------+\n",
    "\n",
    "#  Rerun now o nthe scrambled images\n",
    "#\n",
    "\n",
    "#  To add a blank line to history-\n",
    "#\n",
    "l_history.add(event = \"\", measure = \"\")\n",
    "\n",
    "\n",
    "do_model(NearestCentroid(), np_mnist[\"train_scramble_v\"], np_mnist[\"train_label\"], np_mnist[\"test_scramble_v\"], np_mnist[\"test_label\"], \"MNist: Centroid, Scramble\") \n",
    "print()\n",
    "\n",
    "#  do_model(KNeighborsClassifier(n_neighbors =  3), np_mnist[\"train_scramble_v\"], np_mnist[\"train_label\"], np_mnist[\"test_scramble_v\"], np_mnist[\"test_label\"], \"MNist: kNN=3, Scramble\" ) \n",
    "#  do_model(KNeighborsClassifier(n_neighbors =  7), np_mnist[\"train_scramble_v\"], np_mnist[\"train_label\"], np_mnist[\"test_scramble_v\"], np_mnist[\"test_label\"], \"MNist: kNN=7, Scramble\") \n",
    "#  print()\n",
    "\n",
    "#  do_model(GaussianNB(), np_mnist[\"train_scramble_v\"], np_mnist[\"train_label\"], np_mnist[\"test_scramble_v\"], np_mnist[\"test_label\"], \"MNist: GaussianNB, Scramble\") \n",
    "#  print()\n",
    "\n",
    "#  do_model(MultinomialNB(), np_mnist[\"train_scramble_v\"], np_mnist[\"train_label\"], np_mnist[\"test_scramble_v\"], np_mnist[\"test_label\"], \"MNist: MultinomialNB, Scramble\") \n",
    "#  print()\n",
    "\n",
    "#  do_model(DecisionTreeClassifier(), np_mnist[\"train_scramble_v\"], np_mnist[\"train_label\"], np_mnist[\"test_scramble_v\"], np_mnist[\"test_label\"], \"MNist: DecisionTree, Scramble\") \n",
    "#  print()\n",
    "\n",
    "#  n_estimators, number of random trees created and trained\n",
    "#\n",
    "#  do_model(RandomForestClassifier(n_estimators = 5   ), np_mnist[\"train_scramble_v\"], np_mnist[\"train_label\"], np_mnist[\"test_scramble_v\"], np_mnist[\"test_label\"], \"MNist: Random Forest = 5   , Scramble\") \n",
    "#  do_model(RandomForestClassifier(n_estimators = 50  ), np_mnist[\"train_scramble_v\"], np_mnist[\"train_label\"], np_mnist[\"test_scramble_v\"], np_mnist[\"test_label\"], \"MNist: Random Forest = 50  , Scramble\") \n",
    "#  do_model(RandomForestClassifier(n_estimators = 500 ), np_mnist[\"train_scramble_v\"], np_mnist[\"train_label\"], np_mnist[\"test_scramble_v\"], np_mnist[\"test_label\"], \"MNist: Random Forest = 500 , Scramble\") \n",
    "#  do_model(RandomForestClassifier(n_estimators = 5000), np_mnist[\"train_scramble_v\"], np_mnist[\"train_label\"], np_mnist[\"test_scramble_v\"], np_mnist[\"test_label\"], \"MNist: Random Forest = 5000, Scramble\") \n",
    "\n",
    "print()\n",
    "\n",
    "   ###\n",
    "\n",
    "print()\n",
    "print(tabulate(l_history, headers=[\"Category\", \"Accuracy\"], tablefmt='psql', showindex=False))\n",
    "print()\n",
    "\n",
    "print(\"--\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf646e34-43c1-48a0-b369-7aa59aed6fc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420ad074-e31f-44ff-844a-2c77217929f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95f9af4-927b-42cd-aa56-d7ba85422a35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2506248-ba2a-4b82-82fc-afbb6b909e9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee49c01-e2a6-43c5-82c7-e3c76815d2b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5a84dc94-53e2-42f8-b92f-7593a268b81a",
   "metadata": {},
   "source": [
    "#  Step CN: Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c16a43-f6f7-4e67-b7c1-2ba2cdbe9590",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a96567-70df-4093-b310-f88ca3a644de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7758e3d2-8156-4b6f-bb84-baf0de7cb434",
   "metadata": {},
   "source": [
    "#  Step CN: End of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b89fa9-100e-4b6f-b728-e281d8b4d725",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3976e37-8702-4f53-bf97-e28a958675f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9499d30f-d8c8-46d2-9cef-22fd45a841b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a397bde-64f0-4ec9-a728-d37e4c51ecea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
