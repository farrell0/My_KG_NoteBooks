{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064cde52-a7e1-486e-a7c4-3ce3aaaa01da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  This NoteBook contains code to run classic ML routines against a \n",
    "#  number of familiar data sets ..\n",
    "#\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d433bb83-5f71-406e-ab1c-1b0ef02583f1",
   "metadata": {
    "tags": []
   },
   "source": [
    "#  Step 00: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba7589c-6f1e-4f5c-a37b-21f8944c2499",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Largely code to control how print statements and related work\n",
    "#\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%xmode Minimal\n",
    "\n",
    "\n",
    "#  Setting display options \n",
    "#\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.width\", 640)\n",
    "   #\n",
    "import numpy as np\n",
    "np.set_printoptions(edgeitems = 30, linewidth = 100000, \n",
    "   formatter = dict(float = lambda x: \"%.3g\" % x))\n",
    "\n",
    "#  Sets horizontal scroll for wide outputs\n",
    "#\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"\"))\n",
    "\n",
    "   ###\n",
    "    \n",
    "from tabulate import tabulate\n",
    "#\n",
    "#  How to use tabulate-\n",
    "#\n",
    "#  l_result = [{ \"col1\": 20, \"col2\": 30}]\n",
    "#  #\n",
    "#  print(tabulate(l_result, headers='keys', tablefmt='psql', showindex=False))\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a747a0-4aec-4cd2-bf38-8849deee7ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Helper functions for what we want to do below-\n",
    "#\n",
    "#  **  You must run this cell to do much of anything in this NoteBook\n",
    "\n",
    "#  We use these objects to store the history of results; display only\n",
    "#\n",
    "class HistoryIterator:\n",
    "   def __init__(self, history):\n",
    "       self._history = history\n",
    "       self._index = 0\n",
    "\n",
    "   def __next__(self):\n",
    "       if (self._index < len(self._history._events)):\n",
    "           result = (self._history._events[self._index][\"event\"] , self._history._events[self._index][\"measure\"])\n",
    "           self._index +=1\n",
    "           return result\n",
    "       raise StopIteration\n",
    "\n",
    "class History:\n",
    "   def __init__(self):\n",
    "      self._events = list()\n",
    "\n",
    "   def clear(self):\n",
    "      self._events = list()\n",
    "    \n",
    "   def add(self, event, measure):\n",
    "      self._events.append({\"event\": event, \"measure\": measure})\n",
    "\n",
    "   def __iter__(self):\n",
    "      return HistoryIterator(self)\n",
    "\n",
    "\n",
    "l_history = History()\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e65a137-2d39-411c-9316-aff11a9b6108",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Sometimes we want to clear history-\n",
    "#\n",
    "\n",
    "l_history.clear()\n",
    "\n",
    "\n",
    "#  To add a blank line to history-\n",
    "#\n",
    "\n",
    "l_history.add(event = \"\", measure = \"\")\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8909c6ac-eef4-42ea-af33-cfda0b6d5740",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  All of our model libraries are imported below, but ..\n",
    "#\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317e2979-c711-419c-91b4-e035bafbe79c",
   "metadata": {
    "tags": []
   },
   "source": [
    "#  Step A1: Iris Data load, encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d1be95-f9df-4435-ab56-add6aa68b8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Read the Iris data into a Pandas Dataframe\n",
    "#\n",
    "#     Features\n",
    "#     1. sepal length in cm\n",
    "#     2. sepal width in cm\n",
    "#     3. petal length in cm\n",
    "#     4. petal width in cm\n",
    "#     5. class: \n",
    "#        Iris-setosa\n",
    "#        Iris-versicolour\n",
    "#        Iris-virginica\n",
    "#\n",
    "#  To convert class into a numeric, we use sklearn.preprocessing.LabelEncoder\n",
    "#  See,\n",
    "#     https://www.turing.com/kb/convert-categorical-data-in-pandas-and-scikit-learn\n",
    "#\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "my_le = LabelEncoder()\n",
    "   #\n",
    "l_folder = \"20_Data\"\n",
    "l_file   = \"11_iris.data.txt\"\n",
    "\n",
    "\n",
    "pd_iris  = pd.read_csv((l_folder + \"/\" + l_file), header = 0, sep = \",\",\n",
    "   names = [\"sl\", \"sw\", \"pl\", \"pw\", \"class\"],\n",
    "   dtype = {\"sl\": \"float\", \"sw\": \"float\", \"pl\": \"float\", \"pw\": \"float\", \"class\": \"string\"} )\n",
    "      #\n",
    "pd_iris[\"class_encoded\"]  =  my_le.fit_transform(pd_iris[\"class\"])\n",
    "   #\n",
    "pd_iris = pd_iris.drop([\"class\"], axis = 1)\n",
    "    \n",
    "    \n",
    "#  Pandas.Dataframe.sample() returns a randomized set of rows, versus\n",
    "#  say head(), which always returns the first n ..\n",
    "#\n",
    "print(tabulate(pd_iris.sample(5), headers='keys', tablefmt='psql', showindex=False))\n",
    "print(\"Number of rows: %d\" % (len(pd_iris)))\n",
    "\n",
    "print(\"--\")\n",
    "\n",
    "#  Sample output,\n",
    "#\n",
    "#     +------+------+------+------+-----------------+\n",
    "#     |   sl |   sw |   pl |   pw |   class_encoded |\n",
    "#     |------+------+------+------+-----------------|\n",
    "#     |  5.5 |  2.4 |  3.8 |  1.1 |               1 |\n",
    "#     |  6.4 |  3.2 |  4.5 |  1.5 |               1 |\n",
    "#     |  6.8 |  3.2 |  5.9 |  2.3 |               2 |\n",
    "#     |  6.7 |  3.3 |  5.7 |  2.1 |               2 |\n",
    "#     |  5.5 |  2.6 |  4.4 |  1.2 |               1 |\n",
    "#     +------+------+------+------+-----------------+\n",
    "#     Number of rows: 149\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd64b3f-30aa-4680-b983-462d5da51304",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Split data into training and test.\n",
    "#  Convert the data into numpy arrays, since the ml code we use later expect that.\n",
    "#  We only want two of the classes, 1 and 2.\n",
    "#  And we only want columns 2 and 3.\n",
    "#\n",
    "#     Why this data ?  It was harder to predict; see the plot below.\n",
    "#\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np_iris = {}\n",
    "   #\n",
    "np_iris[\"train\"], np_iris[\"test\"] = train_test_split(pd_iris.to_numpy(),              #  random_state calls to shuffle the data,\n",
    "   test_size = 0.20, random_state = 40)                                               #    which had arrived sorted\n",
    "\n",
    "\n",
    "#  Filter out given labels\n",
    "#\n",
    "np_iris[\"train_f\"] = np_iris[\"train\"][ ( np_iris[\"train\"][:, -1] > 0) & ( np_iris[\"train\"][:, -1] < 3) ]\n",
    "np_iris[\"test_f\" ] = np_iris[\"test\" ][ ( np_iris[\"test\" ][:, -1] > 0) & ( np_iris[\"test\" ][:, -1] < 3) ]\n",
    "\n",
    "\n",
    "#  Slicing only given columns\n",
    "#\n",
    "np_iris[\"train_fs\"] = np_iris[\"train_f\"][:, [2 ,3, 4]]\n",
    "np_iris[\"test_fs\" ] = np_iris[\"test_f\" ][:, [2, 3, 4]] \n",
    "\n",
    "\n",
    "#  Outputs for confirmation\n",
    "#\n",
    "print(\"Number of total                 rows... %d   Training rows: %d   Test rows: %d\" %\n",
    "  (len(pd_iris), len(np_iris[\"train\"]), len(np_iris[\"test\"])) )\n",
    "print()\n",
    "print(\"Number of total filtered/sliced rows... %d   Training rows: %d   Test rows: %d\" %\n",
    "  ( len(np_iris[\"train_fs\"]) + len(np_iris[\"test_fs\"]),\n",
    "    len(np_iris[\"train_fs\"]), len(np_iris[\"test_fs\"]) ) ) \n",
    "print()\n",
    "\n",
    "print(\"Train data:\")\n",
    "print(\"%s\" % (np_iris[\"train_fs\"][0:5]))\n",
    "print()\n",
    "print(\"Test  data:\")\n",
    "print(\"%s\" % (np_iris[\"test_fs\" ][0:5]))\n",
    "print()\n",
    "   #\n",
    "print(\"--\")\n",
    "\n",
    "#  Sample output\n",
    "#\n",
    "#     Number of total                 rows... 149   Training rows: 119   Test rows: 30\n",
    "#     \n",
    "#     Number of total filtered/sliced rows... 100   Training rows: 78   Test rows: 22\n",
    "#     \n",
    "#     Train data:\n",
    "#     [[4.7 1.4 1]\n",
    "#      [5.8 1.6 2]\n",
    "#      [5.1 2.4 2]\n",
    "#      [4.4 1.3 1]\n",
    "#      [5.6 2.1 2]]\n",
    "#     \n",
    "#     Test  data:\n",
    "#     [[4.1 1 1]\n",
    "#      [5.5 1.8 2]\n",
    "#      [6.9 2.3 2]\n",
    "#      [4.4 1.2 1]\n",
    "#      [5.6 2.4 2]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62191e70-57c7-49d1-ba47-cca3517b0d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Plot of just two features from the two class data set\n",
    "#\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.scatter(np_iris[\"train_fs\"][np_iris[\"train_fs\"][:, -1]==1][:, 0], np_iris[\"train_fs\"][np_iris[\"train_fs\"][:, -1]==1][:, 1], c = \"red\"  , label = \"Class 1\", marker = \".\")\n",
    "plt.scatter(np_iris[\"train_fs\"][np_iris[\"train_fs\"][:, -1]==2][:, 0], np_iris[\"train_fs\"][np_iris[\"train_fs\"][:, -1]==2][:, 1], c = \"blue\" , label = \"Class 2\", marker = \",\")\n",
    "    \n",
    "plt.legend()\n",
    "   #\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eee2512-59dc-41ed-8cf2-96505468b1e7",
   "metadata": {},
   "source": [
    "<div> \n",
    "<img src=\"./01_Images/09_Results_Chart.png\" alt=\"Drawing\" style=\"width: 800px;\"/>  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93cb4fa2-1927-4534-8ea4-1dde87fceddd",
   "metadata": {},
   "source": [
    "#  Step A2: Iris Data train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a59cef88-7f7a-4579-922c-fc7fa9960059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.69138617\n",
      "Iteration 2, loss = 0.69135742\n",
      "Iteration 3, loss = 0.69132963\n",
      "Iteration 4, loss = 0.69130284\n",
      "Iteration 5, loss = 0.69127703\n",
      "Iteration 6, loss = 0.69125216\n",
      "Iteration 7, loss = 0.69122818\n",
      "Iteration 8, loss = 0.69120507\n",
      "Iteration 9, loss = 0.69118279\n",
      "Iteration 10, loss = 0.69116135\n",
      "Iteration 11, loss = 0.69114071\n",
      "Iteration 12, loss = 0.69112086\n",
      "Iteration 13, loss = 0.69110179\n",
      "Iteration 14, loss = 0.69108347\n",
      "Iteration 15, loss = 0.69106586\n",
      "Iteration 16, loss = 0.69104894\n",
      "Iteration 17, loss = 0.69103266\n",
      "Iteration 18, loss = 0.69101699\n",
      "Iteration 19, loss = 0.69100189\n",
      "Iteration 20, loss = 0.69098729\n",
      "Iteration 21, loss = 0.69097316\n",
      "Iteration 22, loss = 0.69095944\n",
      "Iteration 23, loss = 0.69094608\n",
      "Iteration 24, loss = 0.69093303\n",
      "Iteration 25, loss = 0.69092023\n",
      "Iteration 26, loss = 0.69090763\n",
      "Iteration 27, loss = 0.69089518\n",
      "Iteration 28, loss = 0.69088284\n",
      "Iteration 29, loss = 0.69087056\n",
      "Iteration 30, loss = 0.69085830\n",
      "Iteration 31, loss = 0.69084603\n",
      "Iteration 32, loss = 0.69083372\n",
      "Iteration 33, loss = 0.69082135\n",
      "Iteration 34, loss = 0.69080889\n",
      "Iteration 35, loss = 0.69079632\n",
      "Iteration 36, loss = 0.69078364\n",
      "Iteration 37, loss = 0.69077084\n",
      "Iteration 38, loss = 0.69075791\n",
      "Iteration 39, loss = 0.69074486\n",
      "Iteration 40, loss = 0.69073167\n",
      "Iteration 41, loss = 0.69071837\n",
      "Iteration 42, loss = 0.69070494\n",
      "Iteration 43, loss = 0.69069140\n",
      "Iteration 44, loss = 0.69067774\n",
      "Iteration 45, loss = 0.69066399\n",
      "Iteration 46, loss = 0.69065013\n",
      "Iteration 47, loss = 0.69063618\n",
      "Iteration 48, loss = 0.69062214\n",
      "Iteration 49, loss = 0.69060802\n",
      "Iteration 50, loss = 0.69059381\n",
      "Iteration 51, loss = 0.69057952\n",
      "Iteration 52, loss = 0.69056515\n",
      "Iteration 53, loss = 0.69055070\n",
      "Iteration 54, loss = 0.69053616\n",
      "Iteration 55, loss = 0.69052155\n",
      "Iteration 56, loss = 0.69050685\n",
      "Iteration 57, loss = 0.69049206\n",
      "Iteration 58, loss = 0.69047719\n",
      "Iteration 59, loss = 0.69046222\n",
      "Iteration 60, loss = 0.69044716\n",
      "Iteration 61, loss = 0.69043201\n",
      "Iteration 62, loss = 0.69041675\n",
      "Iteration 63, loss = 0.69040140\n",
      "Iteration 64, loss = 0.69038594\n",
      "Iteration 65, loss = 0.69037038\n",
      "Iteration 66, loss = 0.69035472\n",
      "Iteration 67, loss = 0.69033895\n",
      "Iteration 68, loss = 0.69032307\n",
      "Iteration 69, loss = 0.69030708\n",
      "Iteration 70, loss = 0.69029099\n",
      "Iteration 71, loss = 0.69027479\n",
      "Iteration 72, loss = 0.69025848\n",
      "Iteration 73, loss = 0.69024207\n",
      "Iteration 74, loss = 0.69022554\n",
      "Iteration 75, loss = 0.69020891\n",
      "Iteration 76, loss = 0.69019216\n",
      "Iteration 77, loss = 0.69017531\n",
      "Iteration 78, loss = 0.69015834\n",
      "Iteration 79, loss = 0.69014127\n",
      "Iteration 80, loss = 0.69012408\n",
      "Iteration 81, loss = 0.69010679\n",
      "Iteration 82, loss = 0.69008938\n",
      "Iteration 83, loss = 0.69007185\n",
      "Iteration 84, loss = 0.69005422\n",
      "Iteration 85, loss = 0.69003646\n",
      "Iteration 86, loss = 0.69001859\n",
      "Iteration 87, loss = 0.69000060\n",
      "Iteration 88, loss = 0.68998249\n",
      "Iteration 89, loss = 0.68996427\n",
      "Iteration 90, loss = 0.68994592\n",
      "Iteration 91, loss = 0.68992744\n",
      "Iteration 92, loss = 0.68990885\n",
      "Iteration 93, loss = 0.68989013\n",
      "Iteration 94, loss = 0.68987128\n",
      "Iteration 95, loss = 0.68985231\n",
      "Iteration 96, loss = 0.68983321\n",
      "Iteration 97, loss = 0.68981398\n",
      "Iteration 98, loss = 0.68979462\n",
      "Iteration 99, loss = 0.68977513\n",
      "Iteration 100, loss = 0.68975550\n",
      "Iteration 101, loss = 0.68973575\n",
      "Iteration 102, loss = 0.68971585\n",
      "Iteration 103, loss = 0.68969583\n",
      "Iteration 104, loss = 0.68967566\n",
      "Iteration 105, loss = 0.68965536\n",
      "Iteration 106, loss = 0.68963492\n",
      "Iteration 107, loss = 0.68961433\n",
      "Iteration 108, loss = 0.68959361\n",
      "Iteration 109, loss = 0.68957274\n",
      "Iteration 110, loss = 0.68955173\n",
      "Iteration 111, loss = 0.68953057\n",
      "Iteration 112, loss = 0.68950926\n",
      "Iteration 113, loss = 0.68948781\n",
      "Iteration 114, loss = 0.68946620\n",
      "Iteration 115, loss = 0.68944445\n",
      "Iteration 116, loss = 0.68942254\n",
      "Iteration 117, loss = 0.68940047\n",
      "Iteration 118, loss = 0.68937825\n",
      "Iteration 119, loss = 0.68935587\n",
      "Iteration 120, loss = 0.68933333\n",
      "Iteration 121, loss = 0.68931063\n",
      "Iteration 122, loss = 0.68928776\n",
      "Iteration 123, loss = 0.68926473\n",
      "Iteration 124, loss = 0.68924153\n",
      "Iteration 125, loss = 0.68921817\n",
      "Iteration 126, loss = 0.68919463\n",
      "Iteration 127, loss = 0.68917092\n",
      "Iteration 128, loss = 0.68914704\n",
      "Iteration 129, loss = 0.68912298\n",
      "Iteration 130, loss = 0.68909874\n",
      "Iteration 131, loss = 0.68907432\n",
      "Iteration 132, loss = 0.68904972\n",
      "Iteration 133, loss = 0.68902494\n",
      "Iteration 134, loss = 0.68899997\n",
      "Iteration 135, loss = 0.68897481\n",
      "Iteration 136, loss = 0.68894946\n",
      "Iteration 137, loss = 0.68892392\n",
      "Iteration 138, loss = 0.68889818\n",
      "Iteration 139, loss = 0.68887225\n",
      "Iteration 140, loss = 0.68884612\n",
      "Iteration 141, loss = 0.68881979\n",
      "Iteration 142, loss = 0.68879325\n",
      "Iteration 143, loss = 0.68876651\n",
      "Iteration 144, loss = 0.68873957\n",
      "Iteration 145, loss = 0.68871241\n",
      "Iteration 146, loss = 0.68868504\n",
      "Iteration 147, loss = 0.68865746\n",
      "Iteration 148, loss = 0.68862966\n",
      "Iteration 149, loss = 0.68860164\n",
      "Iteration 150, loss = 0.68857341\n",
      "Iteration 151, loss = 0.68854495\n",
      "Iteration 152, loss = 0.68851627\n",
      "Iteration 153, loss = 0.68848736\n",
      "Iteration 154, loss = 0.68845822\n",
      "Iteration 155, loss = 0.68842885\n",
      "Iteration 156, loss = 0.68839925\n",
      "Iteration 157, loss = 0.68836941\n",
      "Iteration 158, loss = 0.68833934\n",
      "Iteration 159, loss = 0.68830902\n",
      "Iteration 160, loss = 0.68827847\n",
      "Iteration 161, loss = 0.68824767\n",
      "Iteration 162, loss = 0.68821662\n",
      "Iteration 163, loss = 0.68818532\n",
      "Iteration 164, loss = 0.68815378\n",
      "Iteration 165, loss = 0.68812198\n",
      "Iteration 166, loss = 0.68808993\n",
      "Iteration 167, loss = 0.68805762\n",
      "Iteration 168, loss = 0.68802504\n",
      "Iteration 169, loss = 0.68799221\n",
      "Iteration 170, loss = 0.68795912\n",
      "Iteration 171, loss = 0.68792575\n",
      "Iteration 172, loss = 0.68789212\n",
      "Iteration 173, loss = 0.68785822\n",
      "Iteration 174, loss = 0.68782405\n",
      "Iteration 175, loss = 0.68778960\n",
      "Iteration 176, loss = 0.68775487\n",
      "Iteration 177, loss = 0.68771986\n",
      "Iteration 178, loss = 0.68768458\n",
      "Iteration 179, loss = 0.68764900\n",
      "Iteration 180, loss = 0.68761314\n",
      "Iteration 181, loss = 0.68757700\n",
      "Iteration 182, loss = 0.68754056\n",
      "Iteration 183, loss = 0.68750382\n",
      "Iteration 184, loss = 0.68746680\n",
      "Iteration 185, loss = 0.68742947\n",
      "Iteration 186, loss = 0.68739185\n",
      "Iteration 187, loss = 0.68735392\n",
      "Iteration 188, loss = 0.68731569\n",
      "Iteration 189, loss = 0.68727715\n",
      "Iteration 190, loss = 0.68723830\n",
      "Iteration 191, loss = 0.68719914\n",
      "Iteration 192, loss = 0.68715966\n",
      "Iteration 193, loss = 0.68711987\n",
      "Iteration 194, loss = 0.68707976\n",
      "Iteration 195, loss = 0.68703933\n",
      "Iteration 196, loss = 0.68699858\n",
      "Iteration 197, loss = 0.68695750\n",
      "Iteration 198, loss = 0.68691609\n",
      "Iteration 199, loss = 0.68687436\n",
      "Iteration 200, loss = 0.68683229\n",
      "Iteration 201, loss = 0.68678989\n",
      "Iteration 202, loss = 0.68674715\n",
      "Iteration 203, loss = 0.68670407\n",
      "Iteration 204, loss = 0.68666065\n",
      "Iteration 205, loss = 0.68661689\n",
      "Iteration 206, loss = 0.68657278\n",
      "Iteration 207, loss = 0.68652833\n",
      "Iteration 208, loss = 0.68648352\n",
      "Iteration 209, loss = 0.68643836\n",
      "Iteration 210, loss = 0.68639285\n",
      "Iteration 211, loss = 0.68634698\n",
      "Iteration 212, loss = 0.68630075\n",
      "Iteration 213, loss = 0.68625416\n",
      "Iteration 214, loss = 0.68620720\n",
      "Iteration 215, loss = 0.68615988\n",
      "Iteration 216, loss = 0.68611219\n",
      "Iteration 217, loss = 0.68606413\n",
      "Iteration 218, loss = 0.68601570\n",
      "Iteration 219, loss = 0.68596690\n",
      "Iteration 220, loss = 0.68591771\n",
      "Iteration 221, loss = 0.68586815\n",
      "Iteration 222, loss = 0.68581821\n",
      "Iteration 223, loss = 0.68576788\n",
      "Iteration 224, loss = 0.68571717\n",
      "Iteration 225, loss = 0.68566607\n",
      "Iteration 226, loss = 0.68561459\n",
      "Iteration 227, loss = 0.68556270\n",
      "Iteration 228, loss = 0.68551043\n",
      "Iteration 229, loss = 0.68545776\n",
      "Iteration 230, loss = 0.68540469\n",
      "Iteration 231, loss = 0.68535122\n",
      "Iteration 232, loss = 0.68529735\n",
      "Iteration 233, loss = 0.68524307\n",
      "Iteration 234, loss = 0.68518839\n",
      "Iteration 235, loss = 0.68513330\n",
      "Iteration 236, loss = 0.68507780\n",
      "Iteration 237, loss = 0.68502189\n",
      "Iteration 238, loss = 0.68496556\n",
      "Iteration 239, loss = 0.68490881\n",
      "Iteration 240, loss = 0.68485164\n",
      "Iteration 241, loss = 0.68479406\n",
      "Iteration 242, loss = 0.68473605\n",
      "Iteration 243, loss = 0.68467762\n",
      "Iteration 244, loss = 0.68461876\n",
      "Iteration 245, loss = 0.68455947\n",
      "Iteration 246, loss = 0.68449974\n",
      "Iteration 247, loss = 0.68443959\n",
      "Iteration 248, loss = 0.68437900\n",
      "Iteration 249, loss = 0.68431798\n",
      "Iteration 250, loss = 0.68425651\n",
      "Iteration 251, loss = 0.68419461\n",
      "Iteration 252, loss = 0.68413226\n",
      "Iteration 253, loss = 0.68406947\n",
      "Iteration 254, loss = 0.68400623\n",
      "Iteration 255, loss = 0.68394254\n",
      "Iteration 256, loss = 0.68387840\n",
      "Iteration 257, loss = 0.68381381\n",
      "Iteration 258, loss = 0.68374877\n",
      "Iteration 259, loss = 0.68368326\n",
      "Iteration 260, loss = 0.68361730\n",
      "Iteration 261, loss = 0.68355088\n",
      "Iteration 262, loss = 0.68348400\n",
      "Iteration 263, loss = 0.68341666\n",
      "Iteration 264, loss = 0.68334884\n",
      "Iteration 265, loss = 0.68328057\n",
      "Iteration 266, loss = 0.68321182\n",
      "Iteration 267, loss = 0.68314260\n",
      "Iteration 268, loss = 0.68307290\n",
      "Iteration 269, loss = 0.68300273\n",
      "Iteration 270, loss = 0.68293209\n",
      "Iteration 271, loss = 0.68286097\n",
      "Iteration 272, loss = 0.68278936\n",
      "Iteration 273, loss = 0.68271727\n",
      "Iteration 274, loss = 0.68264470\n",
      "Iteration 275, loss = 0.68257164\n",
      "Iteration 276, loss = 0.68249810\n",
      "Iteration 277, loss = 0.68242406\n",
      "Iteration 278, loss = 0.68234953\n",
      "Iteration 279, loss = 0.68227451\n",
      "Iteration 280, loss = 0.68219900\n",
      "Iteration 281, loss = 0.68212299\n",
      "Iteration 282, loss = 0.68204647\n",
      "Iteration 283, loss = 0.68196946\n",
      "Iteration 284, loss = 0.68189195\n",
      "Iteration 285, loss = 0.68181393\n",
      "Iteration 286, loss = 0.68173540\n",
      "Iteration 287, loss = 0.68165637\n",
      "Iteration 288, loss = 0.68157683\n",
      "Iteration 289, loss = 0.68149678\n",
      "Iteration 290, loss = 0.68141621\n",
      "Iteration 291, loss = 0.68133513\n",
      "Iteration 292, loss = 0.68125353\n",
      "Iteration 293, loss = 0.68117141\n",
      "Iteration 294, loss = 0.68108878\n",
      "Iteration 295, loss = 0.68100562\n",
      "Iteration 296, loss = 0.68092194\n",
      "Iteration 297, loss = 0.68083773\n",
      "Iteration 298, loss = 0.68075300\n",
      "Iteration 299, loss = 0.68066774\n",
      "Iteration 300, loss = 0.68058195\n",
      "Iteration 301, loss = 0.68049562\n",
      "Iteration 302, loss = 0.68040876\n",
      "Iteration 303, loss = 0.68032137\n",
      "Iteration 304, loss = 0.68023344\n",
      "Iteration 305, loss = 0.68014497\n",
      "Iteration 306, loss = 0.68005596\n",
      "Iteration 307, loss = 0.67996640\n",
      "Iteration 308, loss = 0.67987631\n",
      "Iteration 309, loss = 0.67978567\n",
      "Iteration 310, loss = 0.67969448\n",
      "Iteration 311, loss = 0.67960274\n",
      "Iteration 312, loss = 0.67951045\n",
      "Iteration 313, loss = 0.67941761\n",
      "Iteration 314, loss = 0.67932421\n",
      "Iteration 315, loss = 0.67923026\n",
      "Iteration 316, loss = 0.67913575\n",
      "Iteration 317, loss = 0.67904068\n",
      "Iteration 318, loss = 0.67894505\n",
      "Iteration 319, loss = 0.67884886\n",
      "Iteration 320, loss = 0.67875211\n",
      "Iteration 321, loss = 0.67865479\n",
      "Iteration 322, loss = 0.67855690\n",
      "Iteration 323, loss = 0.67845844\n",
      "Iteration 324, loss = 0.67835942\n",
      "Iteration 325, loss = 0.67825982\n",
      "Iteration 326, loss = 0.67815965\n",
      "Iteration 327, loss = 0.67805890\n",
      "Iteration 328, loss = 0.67795758\n",
      "Iteration 329, loss = 0.67785568\n",
      "Iteration 330, loss = 0.67775320\n",
      "Iteration 331, loss = 0.67765013\n",
      "Iteration 332, loss = 0.67754649\n",
      "Iteration 333, loss = 0.67744226\n",
      "Iteration 334, loss = 0.67733744\n",
      "Iteration 335, loss = 0.67723204\n",
      "Iteration 336, loss = 0.67712604\n",
      "Iteration 337, loss = 0.67701946\n",
      "Iteration 338, loss = 0.67691228\n",
      "Iteration 339, loss = 0.67680451\n",
      "Iteration 340, loss = 0.67669615\n",
      "Iteration 341, loss = 0.67658718\n",
      "Iteration 342, loss = 0.67647762\n",
      "Iteration 343, loss = 0.67636746\n",
      "Iteration 344, loss = 0.67625670\n",
      "Iteration 345, loss = 0.67614534\n",
      "Iteration 346, loss = 0.67603337\n",
      "Iteration 347, loss = 0.67592079\n",
      "Iteration 348, loss = 0.67580761\n",
      "Iteration 349, loss = 0.67569382\n",
      "Iteration 350, loss = 0.67557942\n",
      "Iteration 351, loss = 0.67546441\n",
      "Iteration 352, loss = 0.67534878\n",
      "Iteration 353, loss = 0.67523254\n",
      "Iteration 354, loss = 0.67511568\n",
      "Iteration 355, loss = 0.67499821\n",
      "Iteration 356, loss = 0.67488012\n",
      "Iteration 357, loss = 0.67476140\n",
      "Iteration 358, loss = 0.67464207\n",
      "Iteration 359, loss = 0.67452211\n",
      "Iteration 360, loss = 0.67440153\n",
      "Iteration 361, loss = 0.67428033\n",
      "Iteration 362, loss = 0.67415849\n",
      "Iteration 363, loss = 0.67403603\n",
      "Iteration 364, loss = 0.67391294\n",
      "Iteration 365, loss = 0.67378921\n",
      "Iteration 366, loss = 0.67366486\n",
      "Iteration 367, loss = 0.67353987\n",
      "Iteration 368, loss = 0.67341424\n",
      "Iteration 369, loss = 0.67328798\n",
      "Iteration 370, loss = 0.67316108\n",
      "Iteration 371, loss = 0.67303354\n",
      "Iteration 372, loss = 0.67290536\n",
      "Iteration 373, loss = 0.67277654\n",
      "Iteration 374, loss = 0.67264708\n",
      "Iteration 375, loss = 0.67251697\n",
      "Iteration 376, loss = 0.67238622\n",
      "Iteration 377, loss = 0.67225482\n",
      "Iteration 378, loss = 0.67212277\n",
      "Iteration 379, loss = 0.67199007\n",
      "Iteration 380, loss = 0.67185673\n",
      "Iteration 381, loss = 0.67172273\n",
      "Iteration 382, loss = 0.67158808\n",
      "Iteration 383, loss = 0.67145277\n",
      "Iteration 384, loss = 0.67131681\n",
      "Iteration 385, loss = 0.67118019\n",
      "Iteration 386, loss = 0.67104292\n",
      "Iteration 387, loss = 0.67090498\n",
      "Iteration 388, loss = 0.67076639\n",
      "Iteration 389, loss = 0.67062714\n",
      "Iteration 390, loss = 0.67048722\n",
      "Iteration 391, loss = 0.67034664\n",
      "Iteration 392, loss = 0.67020540\n",
      "Iteration 393, loss = 0.67006349\n",
      "Iteration 394, loss = 0.66992091\n",
      "Iteration 395, loss = 0.66977767\n",
      "Iteration 396, loss = 0.66963376\n",
      "Iteration 397, loss = 0.66948918\n",
      "Iteration 398, loss = 0.66934392\n",
      "Iteration 399, loss = 0.66919800\n",
      "Iteration 400, loss = 0.66905140\n",
      "Iteration 401, loss = 0.66890413\n",
      "Iteration 402, loss = 0.66875619\n",
      "Iteration 403, loss = 0.66860756\n",
      "Iteration 404, loss = 0.66845827\n",
      "Iteration 405, loss = 0.66830829\n",
      "Iteration 406, loss = 0.66815764\n",
      "Iteration 407, loss = 0.66800630\n",
      "Iteration 408, loss = 0.66785429\n",
      "Iteration 409, loss = 0.66770159\n",
      "Iteration 410, loss = 0.66754821\n",
      "Iteration 411, loss = 0.66739415\n",
      "Iteration 412, loss = 0.66723940\n",
      "Iteration 413, loss = 0.66708397\n",
      "Iteration 414, loss = 0.66692785\n",
      "Iteration 415, loss = 0.66677105\n",
      "Iteration 416, loss = 0.66661355\n",
      "Iteration 417, loss = 0.66645537\n",
      "Iteration 418, loss = 0.66629650\n",
      "Iteration 419, loss = 0.66613694\n",
      "Iteration 420, loss = 0.66597669\n",
      "Iteration 421, loss = 0.66581574\n",
      "Iteration 422, loss = 0.66565411\n",
      "Iteration 423, loss = 0.66549178\n",
      "Iteration 424, loss = 0.66532875\n",
      "Iteration 425, loss = 0.66516503\n",
      "Iteration 426, loss = 0.66500062\n",
      "Iteration 427, loss = 0.66483551\n",
      "Iteration 428, loss = 0.66466970\n",
      "Iteration 429, loss = 0.66450319\n",
      "Iteration 430, loss = 0.66433599\n",
      "Iteration 431, loss = 0.66416808\n",
      "Iteration 432, loss = 0.66399948\n",
      "Iteration 433, loss = 0.66383017\n",
      "Iteration 434, loss = 0.66366017\n",
      "Iteration 435, loss = 0.66348946\n",
      "Iteration 436, loss = 0.66331805\n",
      "Iteration 437, loss = 0.66314594\n",
      "Iteration 438, loss = 0.66297312\n",
      "Iteration 439, loss = 0.66279960\n",
      "Iteration 440, loss = 0.66262538\n",
      "Iteration 441, loss = 0.66245045\n",
      "Iteration 442, loss = 0.66227481\n",
      "Iteration 443, loss = 0.66209847\n",
      "Iteration 444, loss = 0.66192142\n",
      "Iteration 445, loss = 0.66174367\n",
      "Iteration 446, loss = 0.66156520\n",
      "Iteration 447, loss = 0.66138603\n",
      "Iteration 448, loss = 0.66120615\n",
      "Iteration 449, loss = 0.66102556\n",
      "Iteration 450, loss = 0.66084426\n",
      "Iteration 451, loss = 0.66066225\n",
      "Iteration 452, loss = 0.66047953\n",
      "Iteration 453, loss = 0.66029610\n",
      "Iteration 454, loss = 0.66011196\n",
      "Iteration 455, loss = 0.65992711\n",
      "Iteration 456, loss = 0.65974154\n",
      "Iteration 457, loss = 0.65955527\n",
      "Iteration 458, loss = 0.65936828\n",
      "Iteration 459, loss = 0.65918058\n",
      "Iteration 460, loss = 0.65899216\n",
      "Iteration 461, loss = 0.65880303\n",
      "Iteration 462, loss = 0.65861319\n",
      "Iteration 463, loss = 0.65842263\n",
      "Iteration 464, loss = 0.65823136\n",
      "Iteration 465, loss = 0.65803937\n",
      "Iteration 466, loss = 0.65784667\n",
      "Iteration 467, loss = 0.65765326\n",
      "Iteration 468, loss = 0.65745913\n",
      "Iteration 469, loss = 0.65726429\n",
      "Iteration 470, loss = 0.65706872\n",
      "Iteration 471, loss = 0.65687245\n",
      "Iteration 472, loss = 0.65667546\n",
      "Iteration 473, loss = 0.65647775\n",
      "Iteration 474, loss = 0.65627933\n",
      "Iteration 475, loss = 0.65608019\n",
      "Iteration 476, loss = 0.65588033\n",
      "Iteration 477, loss = 0.65567976\n",
      "Iteration 478, loss = 0.65547847\n",
      "Iteration 479, loss = 0.65527647\n",
      "Iteration 480, loss = 0.65507375\n",
      "Iteration 481, loss = 0.65487031\n",
      "Iteration 482, loss = 0.65466616\n",
      "Iteration 483, loss = 0.65446129\n",
      "Iteration 484, loss = 0.65425571\n",
      "Iteration 485, loss = 0.65404941\n",
      "Iteration 486, loss = 0.65384239\n",
      "Iteration 487, loss = 0.65363466\n",
      "Iteration 488, loss = 0.65342621\n",
      "Iteration 489, loss = 0.65321705\n",
      "Iteration 490, loss = 0.65300717\n",
      "Iteration 491, loss = 0.65279657\n",
      "Iteration 492, loss = 0.65258526\n",
      "Iteration 493, loss = 0.65237323\n",
      "Iteration 494, loss = 0.65216049\n",
      "Iteration 495, loss = 0.65194704\n",
      "Iteration 496, loss = 0.65173287\n",
      "Iteration 497, loss = 0.65151798\n",
      "Iteration 498, loss = 0.65130238\n",
      "Iteration 499, loss = 0.65108607\n",
      "Iteration 500, loss = 0.65086905\n",
      "Iteration 501, loss = 0.65065131\n",
      "Iteration 502, loss = 0.65043285\n",
      "Iteration 503, loss = 0.65021369\n",
      "Iteration 504, loss = 0.64999381\n",
      "Iteration 505, loss = 0.64977322\n",
      "Iteration 506, loss = 0.64955192\n",
      "Iteration 507, loss = 0.64932991\n",
      "Iteration 508, loss = 0.64910718\n",
      "Iteration 509, loss = 0.64888375\n",
      "Iteration 510, loss = 0.64865960\n",
      "Iteration 511, loss = 0.64843475\n",
      "Iteration 512, loss = 0.64820918\n",
      "Iteration 513, loss = 0.64798291\n",
      "Iteration 514, loss = 0.64775593\n",
      "Iteration 515, loss = 0.64752824\n",
      "Iteration 516, loss = 0.64729984\n",
      "Iteration 517, loss = 0.64707074\n",
      "Iteration 518, loss = 0.64684093\n",
      "Iteration 519, loss = 0.64661041\n",
      "Iteration 520, loss = 0.64637919\n",
      "Iteration 521, loss = 0.64614727\n",
      "Iteration 522, loss = 0.64591464\n",
      "Iteration 523, loss = 0.64568130\n",
      "Iteration 524, loss = 0.64544726\n",
      "Iteration 525, loss = 0.64521253\n",
      "Iteration 526, loss = 0.64497708\n",
      "Iteration 527, loss = 0.64474094\n",
      "Iteration 528, loss = 0.64450410\n",
      "Iteration 529, loss = 0.64426656\n",
      "Iteration 530, loss = 0.64402832\n",
      "Iteration 531, loss = 0.64378938\n",
      "Iteration 532, loss = 0.64354974\n",
      "Iteration 533, loss = 0.64330940\n",
      "Iteration 534, loss = 0.64306837\n",
      "Iteration 535, loss = 0.64282665\n",
      "Iteration 536, loss = 0.64258423\n",
      "Iteration 537, loss = 0.64234111\n",
      "Iteration 538, loss = 0.64209730\n",
      "Iteration 539, loss = 0.64185280\n",
      "Iteration 540, loss = 0.64160761\n",
      "Iteration 541, loss = 0.64136173\n",
      "Iteration 542, loss = 0.64111516\n",
      "Iteration 543, loss = 0.64086790\n",
      "Iteration 544, loss = 0.64061995\n",
      "Iteration 545, loss = 0.64037132\n",
      "Iteration 546, loss = 0.64012200\n",
      "Iteration 547, loss = 0.63987199\n",
      "Iteration 548, loss = 0.63962130\n",
      "Iteration 549, loss = 0.63936992\n",
      "Iteration 550, loss = 0.63911787\n",
      "Iteration 551, loss = 0.63886513\n",
      "Iteration 552, loss = 0.63861171\n",
      "Iteration 553, loss = 0.63835762\n",
      "Iteration 554, loss = 0.63810284\n",
      "Iteration 555, loss = 0.63784739\n",
      "Iteration 556, loss = 0.63759126\n",
      "Iteration 557, loss = 0.63733445\n",
      "Iteration 558, loss = 0.63707697\n",
      "Iteration 559, loss = 0.63681882\n",
      "Iteration 560, loss = 0.63656000\n",
      "Iteration 561, loss = 0.63630051\n",
      "Iteration 562, loss = 0.63604034\n",
      "Iteration 563, loss = 0.63577951\n",
      "Iteration 564, loss = 0.63551801\n",
      "Iteration 565, loss = 0.63525584\n",
      "Iteration 566, loss = 0.63499301\n",
      "Iteration 567, loss = 0.63472952\n",
      "Iteration 568, loss = 0.63446536\n",
      "Iteration 569, loss = 0.63420054\n",
      "Iteration 570, loss = 0.63393506\n",
      "Iteration 571, loss = 0.63366892\n",
      "Iteration 572, loss = 0.63340213\n",
      "Iteration 573, loss = 0.63313468\n",
      "Iteration 574, loss = 0.63286657\n",
      "Iteration 575, loss = 0.63259781\n",
      "Iteration 576, loss = 0.63232840\n",
      "Iteration 577, loss = 0.63205833\n",
      "Iteration 578, loss = 0.63178762\n",
      "Iteration 579, loss = 0.63151625\n",
      "Iteration 580, loss = 0.63124424\n",
      "Iteration 581, loss = 0.63097159\n",
      "Iteration 582, loss = 0.63069829\n",
      "Iteration 583, loss = 0.63042434\n",
      "Iteration 584, loss = 0.63014976\n",
      "Iteration 585, loss = 0.62987453\n",
      "Iteration 586, loss = 0.62959867\n",
      "Iteration 587, loss = 0.62932217\n",
      "Iteration 588, loss = 0.62904503\n",
      "Iteration 589, loss = 0.62876726\n",
      "Iteration 590, loss = 0.62848886\n",
      "Iteration 591, loss = 0.62820982\n",
      "Iteration 592, loss = 0.62793016\n",
      "Iteration 593, loss = 0.62764986\n",
      "Iteration 594, loss = 0.62736894\n",
      "Iteration 595, loss = 0.62708739\n",
      "Iteration 596, loss = 0.62680522\n",
      "Iteration 597, loss = 0.62652243\n",
      "Iteration 598, loss = 0.62623902\n",
      "Iteration 599, loss = 0.62595499\n",
      "Iteration 600, loss = 0.62567034\n",
      "Iteration 601, loss = 0.62538507\n",
      "Iteration 602, loss = 0.62509919\n",
      "Iteration 603, loss = 0.62481270\n",
      "Iteration 604, loss = 0.62452560\n",
      "Iteration 605, loss = 0.62423788\n",
      "Iteration 606, loss = 0.62394956\n",
      "Iteration 607, loss = 0.62366064\n",
      "Iteration 608, loss = 0.62337111\n",
      "Iteration 609, loss = 0.62308097\n",
      "Iteration 610, loss = 0.62279024\n",
      "Iteration 611, loss = 0.62249891\n",
      "Iteration 612, loss = 0.62220698\n",
      "Iteration 613, loss = 0.62191445\n",
      "Iteration 614, loss = 0.62162133\n",
      "Iteration 615, loss = 0.62132762\n",
      "Iteration 616, loss = 0.62103331\n",
      "Iteration 617, loss = 0.62073842\n",
      "Iteration 618, loss = 0.62044294\n",
      "Iteration 619, loss = 0.62014688\n",
      "Iteration 620, loss = 0.61985023\n",
      "Iteration 621, loss = 0.61955300\n",
      "Iteration 622, loss = 0.61925519\n",
      "Iteration 623, loss = 0.61895681\n",
      "Iteration 624, loss = 0.61865785\n",
      "Iteration 625, loss = 0.61835831\n",
      "Iteration 626, loss = 0.61805820\n",
      "Iteration 627, loss = 0.61775752\n",
      "Iteration 628, loss = 0.61745627\n",
      "Iteration 629, loss = 0.61715446\n",
      "Iteration 630, loss = 0.61685208\n",
      "Iteration 631, loss = 0.61654914\n",
      "Iteration 632, loss = 0.61624564\n",
      "Iteration 633, loss = 0.61594158\n",
      "Iteration 634, loss = 0.61563696\n",
      "Iteration 635, loss = 0.61533178\n",
      "Iteration 636, loss = 0.61502606\n",
      "Iteration 637, loss = 0.61471978\n",
      "Iteration 638, loss = 0.61441295\n",
      "Iteration 639, loss = 0.61410558\n",
      "Iteration 640, loss = 0.61379766\n",
      "Iteration 641, loss = 0.61348920\n",
      "Iteration 642, loss = 0.61318019\n",
      "Iteration 643, loss = 0.61287065\n",
      "Iteration 644, loss = 0.61256057\n",
      "Iteration 645, loss = 0.61224996\n",
      "Iteration 646, loss = 0.61193881\n",
      "Iteration 647, loss = 0.61162713\n",
      "Iteration 648, loss = 0.61131492\n",
      "Iteration 649, loss = 0.61100219\n",
      "Iteration 650, loss = 0.61068893\n",
      "Iteration 651, loss = 0.61037515\n",
      "Iteration 652, loss = 0.61006085\n",
      "Iteration 653, loss = 0.60974602\n",
      "Iteration 654, loss = 0.60943069\n",
      "Iteration 655, loss = 0.60911484\n",
      "Iteration 656, loss = 0.60879847\n",
      "Iteration 657, loss = 0.60848160\n",
      "Iteration 658, loss = 0.60816422\n",
      "Iteration 659, loss = 0.60784633\n",
      "Iteration 660, loss = 0.60752794\n",
      "Iteration 661, loss = 0.60720905\n",
      "Iteration 662, loss = 0.60688966\n",
      "Iteration 663, loss = 0.60656977\n",
      "Iteration 664, loss = 0.60624938\n",
      "Iteration 665, loss = 0.60592851\n",
      "Iteration 666, loss = 0.60560714\n",
      "Iteration 667, loss = 0.60528529\n",
      "Iteration 668, loss = 0.60496295\n",
      "Iteration 669, loss = 0.60464012\n",
      "Iteration 670, loss = 0.60431681\n",
      "Iteration 671, loss = 0.60399303\n",
      "Iteration 672, loss = 0.60366876\n",
      "Iteration 673, loss = 0.60334403\n",
      "Iteration 674, loss = 0.60301882\n",
      "Iteration 675, loss = 0.60269313\n",
      "Iteration 676, loss = 0.60236699\n",
      "Iteration 677, loss = 0.60204037\n",
      "Iteration 678, loss = 0.60171329\n",
      "Iteration 679, loss = 0.60138575\n",
      "Iteration 680, loss = 0.60105775\n",
      "Iteration 681, loss = 0.60072929\n",
      "Iteration 682, loss = 0.60040038\n",
      "Iteration 683, loss = 0.60007102\n",
      "Iteration 684, loss = 0.59974121\n",
      "Iteration 685, loss = 0.59941094\n",
      "Iteration 686, loss = 0.59908024\n",
      "Iteration 687, loss = 0.59874909\n",
      "Iteration 688, loss = 0.59841750\n",
      "Iteration 689, loss = 0.59808547\n",
      "Iteration 690, loss = 0.59775301\n",
      "Iteration 691, loss = 0.59742011\n",
      "Iteration 692, loss = 0.59708678\n",
      "Iteration 693, loss = 0.59675302\n",
      "Iteration 694, loss = 0.59641883\n",
      "Iteration 695, loss = 0.59608423\n",
      "Iteration 696, loss = 0.59574919\n",
      "Iteration 697, loss = 0.59541374\n",
      "Iteration 698, loss = 0.59507787\n",
      "Iteration 699, loss = 0.59474159\n",
      "Iteration 700, loss = 0.59440490\n",
      "Iteration 701, loss = 0.59406779\n",
      "Iteration 702, loss = 0.59373028\n",
      "Iteration 703, loss = 0.59339236\n",
      "Iteration 704, loss = 0.59305404\n",
      "Iteration 705, loss = 0.59271531\n",
      "Iteration 706, loss = 0.59237619\n",
      "Iteration 707, loss = 0.59203668\n",
      "Iteration 708, loss = 0.59169677\n",
      "Iteration 709, loss = 0.59135647\n",
      "Iteration 710, loss = 0.59101578\n",
      "Iteration 711, loss = 0.59067470\n",
      "Iteration 712, loss = 0.59033325\n",
      "Iteration 713, loss = 0.58999141\n",
      "Iteration 714, loss = 0.58964919\n",
      "Iteration 715, loss = 0.58930659\n",
      "Iteration 716, loss = 0.58896362\n",
      "Iteration 717, loss = 0.58862028\n",
      "Iteration 718, loss = 0.58827657\n",
      "Iteration 719, loss = 0.58793250\n",
      "Iteration 720, loss = 0.58758806\n",
      "Iteration 721, loss = 0.58724325\n",
      "Iteration 722, loss = 0.58689809\n",
      "Iteration 723, loss = 0.58655257\n",
      "Iteration 724, loss = 0.58620670\n",
      "Iteration 725, loss = 0.58586048\n",
      "Iteration 726, loss = 0.58551390\n",
      "Iteration 727, loss = 0.58516698\n",
      "Iteration 728, loss = 0.58481971\n",
      "Iteration 729, loss = 0.58447211\n",
      "Iteration 730, loss = 0.58412416\n",
      "Iteration 731, loss = 0.58377587\n",
      "Iteration 732, loss = 0.58342725\n",
      "Iteration 733, loss = 0.58307830\n",
      "Iteration 734, loss = 0.58272902\n",
      "Iteration 735, loss = 0.58237941\n",
      "Iteration 736, loss = 0.58202948\n",
      "Iteration 737, loss = 0.58167922\n",
      "Iteration 738, loss = 0.58132864\n",
      "Iteration 739, loss = 0.58097775\n",
      "Iteration 740, loss = 0.58062654\n",
      "Iteration 741, loss = 0.58027502\n",
      "Iteration 742, loss = 0.57992319\n",
      "Iteration 743, loss = 0.57957105\n",
      "Iteration 744, loss = 0.57921860\n",
      "Iteration 745, loss = 0.57886586\n",
      "Iteration 746, loss = 0.57851281\n",
      "Iteration 747, loss = 0.57815946\n",
      "Iteration 748, loss = 0.57780583\n",
      "Iteration 749, loss = 0.57745189\n",
      "Iteration 750, loss = 0.57709767\n",
      "Iteration 751, loss = 0.57674316\n",
      "Iteration 752, loss = 0.57638836\n",
      "Iteration 753, loss = 0.57603329\n",
      "Iteration 754, loss = 0.57567793\n",
      "Iteration 755, loss = 0.57532229\n",
      "Iteration 756, loss = 0.57496638\n",
      "Iteration 757, loss = 0.57461020\n",
      "Iteration 758, loss = 0.57425374\n",
      "Iteration 759, loss = 0.57389702\n",
      "Iteration 760, loss = 0.57354003\n",
      "Iteration 761, loss = 0.57318278\n",
      "Iteration 762, loss = 0.57282527\n",
      "Iteration 763, loss = 0.57246750\n",
      "Iteration 764, loss = 0.57210948\n",
      "Iteration 765, loss = 0.57175120\n",
      "Iteration 766, loss = 0.57139267\n",
      "Iteration 767, loss = 0.57103389\n",
      "Iteration 768, loss = 0.57067487\n",
      "Iteration 769, loss = 0.57031561\n",
      "Iteration 770, loss = 0.56995610\n",
      "Iteration 771, loss = 0.56959636\n",
      "Iteration 772, loss = 0.56923638\n",
      "Iteration 773, loss = 0.56887617\n",
      "Iteration 774, loss = 0.56851572\n",
      "Iteration 775, loss = 0.56815505\n",
      "Iteration 776, loss = 0.56779415\n",
      "Iteration 777, loss = 0.56743303\n",
      "Iteration 778, loss = 0.56707169\n",
      "Iteration 779, loss = 0.56671013\n",
      "Iteration 780, loss = 0.56634836\n",
      "Iteration 781, loss = 0.56598637\n",
      "Iteration 782, loss = 0.56562417\n",
      "Iteration 783, loss = 0.56526176\n",
      "Iteration 784, loss = 0.56489914\n",
      "Iteration 785, loss = 0.56453633\n",
      "Iteration 786, loss = 0.56417331\n",
      "Iteration 787, loss = 0.56381009\n",
      "Iteration 788, loss = 0.56344667\n",
      "Iteration 789, loss = 0.56308306\n",
      "Iteration 790, loss = 0.56271926\n",
      "Iteration 791, loss = 0.56235527\n",
      "Iteration 792, loss = 0.56199110\n",
      "Iteration 793, loss = 0.56162673\n",
      "Iteration 794, loss = 0.56126219\n",
      "Iteration 795, loss = 0.56089747\n",
      "Iteration 796, loss = 0.56053257\n",
      "Iteration 797, loss = 0.56016750\n",
      "Iteration 798, loss = 0.55980225\n",
      "Iteration 799, loss = 0.55943684\n",
      "Iteration 800, loss = 0.55907126\n",
      "Iteration 801, loss = 0.55870551\n",
      "Iteration 802, loss = 0.55833960\n",
      "Iteration 803, loss = 0.55797353\n",
      "Iteration 804, loss = 0.55760730\n",
      "Iteration 805, loss = 0.55724092\n",
      "Iteration 806, loss = 0.55687439\n",
      "Iteration 807, loss = 0.55650770\n",
      "Iteration 808, loss = 0.55614087\n",
      "Iteration 809, loss = 0.55577389\n",
      "Iteration 810, loss = 0.55540676\n",
      "Iteration 811, loss = 0.55503950\n",
      "Iteration 812, loss = 0.55467210\n",
      "Iteration 813, loss = 0.55430456\n",
      "Iteration 814, loss = 0.55393688\n",
      "Iteration 815, loss = 0.55356908\n",
      "Iteration 816, loss = 0.55320115\n",
      "Iteration 817, loss = 0.55283309\n",
      "Iteration 818, loss = 0.55246490\n",
      "Iteration 819, loss = 0.55209659\n",
      "Iteration 820, loss = 0.55172817\n",
      "Iteration 821, loss = 0.55135962\n",
      "Iteration 822, loss = 0.55099096\n",
      "Iteration 823, loss = 0.55062219\n",
      "Iteration 824, loss = 0.55025331\n",
      "Iteration 825, loss = 0.54988432\n",
      "Iteration 826, loss = 0.54951522\n",
      "Iteration 827, loss = 0.54914602\n",
      "Iteration 828, loss = 0.54877672\n",
      "Iteration 829, loss = 0.54840732\n",
      "Iteration 830, loss = 0.54803782\n",
      "Iteration 831, loss = 0.54766823\n",
      "Iteration 832, loss = 0.54729855\n",
      "Iteration 833, loss = 0.54692877\n",
      "Iteration 834, loss = 0.54655891\n",
      "Iteration 835, loss = 0.54618897\n",
      "Iteration 836, loss = 0.54581894\n",
      "Iteration 837, loss = 0.54544883\n",
      "Iteration 838, loss = 0.54507864\n",
      "Iteration 839, loss = 0.54470838\n",
      "Iteration 840, loss = 0.54433804\n",
      "Iteration 841, loss = 0.54396763\n",
      "Iteration 842, loss = 0.54359715\n",
      "Iteration 843, loss = 0.54322660\n",
      "Iteration 844, loss = 0.54285599\n",
      "Iteration 845, loss = 0.54248532\n",
      "Iteration 846, loss = 0.54211458\n",
      "Iteration 847, loss = 0.54174379\n",
      "Iteration 848, loss = 0.54137294\n",
      "Iteration 849, loss = 0.54100204\n",
      "Iteration 850, loss = 0.54063108\n",
      "Iteration 851, loss = 0.54026008\n",
      "Iteration 852, loss = 0.53988902\n",
      "Iteration 853, loss = 0.53951793\n",
      "Iteration 854, loss = 0.53914679\n",
      "Iteration 855, loss = 0.53877561\n",
      "Iteration 856, loss = 0.53840439\n",
      "Iteration 857, loss = 0.53803313\n",
      "Iteration 858, loss = 0.53766184\n",
      "Iteration 859, loss = 0.53729052\n",
      "Iteration 860, loss = 0.53691916\n",
      "Iteration 861, loss = 0.53654778\n",
      "Iteration 862, loss = 0.53617638\n",
      "Iteration 863, loss = 0.53580495\n",
      "Iteration 864, loss = 0.53543349\n",
      "Iteration 865, loss = 0.53506202\n",
      "Iteration 866, loss = 0.53469053\n",
      "Iteration 867, loss = 0.53431903\n",
      "Iteration 868, loss = 0.53394751\n",
      "Iteration 869, loss = 0.53357598\n",
      "Iteration 870, loss = 0.53320445\n",
      "Iteration 871, loss = 0.53283290\n",
      "Iteration 872, loss = 0.53246135\n",
      "Iteration 873, loss = 0.53208980\n",
      "Iteration 874, loss = 0.53171825\n",
      "Iteration 875, loss = 0.53134670\n",
      "Iteration 876, loss = 0.53097515\n",
      "Iteration 877, loss = 0.53060361\n",
      "Iteration 878, loss = 0.53023207\n",
      "Iteration 879, loss = 0.52986055\n",
      "Iteration 880, loss = 0.52948903\n",
      "Iteration 881, loss = 0.52911753\n",
      "Iteration 882, loss = 0.52874605\n",
      "Iteration 883, loss = 0.52837458\n",
      "Iteration 884, loss = 0.52800313\n",
      "Iteration 885, loss = 0.52763170\n",
      "Iteration 886, loss = 0.52726030\n",
      "Iteration 887, loss = 0.52688892\n",
      "Iteration 888, loss = 0.52651757\n",
      "Iteration 889, loss = 0.52614625\n",
      "Iteration 890, loss = 0.52577496\n",
      "Iteration 891, loss = 0.52540370\n",
      "Iteration 892, loss = 0.52503248\n",
      "Iteration 893, loss = 0.52466130\n",
      "Iteration 894, loss = 0.52429015\n",
      "Iteration 895, loss = 0.52391905\n",
      "Iteration 896, loss = 0.52354799\n",
      "Iteration 897, loss = 0.52317697\n",
      "Iteration 898, loss = 0.52280600\n",
      "Iteration 899, loss = 0.52243508\n",
      "Iteration 900, loss = 0.52206421\n",
      "Iteration 901, loss = 0.52169339\n",
      "Iteration 902, loss = 0.52132263\n",
      "Iteration 903, loss = 0.52095192\n",
      "Iteration 904, loss = 0.52058127\n",
      "Iteration 905, loss = 0.52021068\n",
      "Iteration 906, loss = 0.51984016\n",
      "Iteration 907, loss = 0.51946969\n",
      "Iteration 908, loss = 0.51909929\n",
      "Iteration 909, loss = 0.51872896\n",
      "Iteration 910, loss = 0.51835870\n",
      "Iteration 911, loss = 0.51798850\n",
      "Iteration 912, loss = 0.51761839\n",
      "Iteration 913, loss = 0.51724834\n",
      "Iteration 914, loss = 0.51687837\n",
      "Iteration 915, loss = 0.51650848\n",
      "Iteration 916, loss = 0.51613867\n",
      "Iteration 917, loss = 0.51576894\n",
      "Iteration 918, loss = 0.51539930\n",
      "Iteration 919, loss = 0.51502973\n",
      "Iteration 920, loss = 0.51466026\n",
      "Iteration 921, loss = 0.51429088\n",
      "Iteration 922, loss = 0.51392158\n",
      "Iteration 923, loss = 0.51355238\n",
      "Iteration 924, loss = 0.51318327\n",
      "Iteration 925, loss = 0.51281425\n",
      "Iteration 926, loss = 0.51244534\n",
      "Iteration 927, loss = 0.51207652\n",
      "Iteration 928, loss = 0.51170780\n",
      "Iteration 929, loss = 0.51133919\n",
      "Iteration 930, loss = 0.51097068\n",
      "Iteration 931, loss = 0.51060227\n",
      "Iteration 932, loss = 0.51023397\n",
      "Iteration 933, loss = 0.50986578\n",
      "Iteration 934, loss = 0.50949771\n",
      "Iteration 935, loss = 0.50912974\n",
      "Iteration 936, loss = 0.50876189\n",
      "Iteration 937, loss = 0.50839415\n",
      "Iteration 938, loss = 0.50802653\n",
      "Iteration 939, loss = 0.50765903\n",
      "Iteration 940, loss = 0.50729165\n",
      "Iteration 941, loss = 0.50692439\n",
      "Iteration 942, loss = 0.50655726\n",
      "Iteration 943, loss = 0.50619025\n",
      "Iteration 944, loss = 0.50582336\n",
      "Iteration 945, loss = 0.50545661\n",
      "Iteration 946, loss = 0.50508998\n",
      "Iteration 947, loss = 0.50472349\n",
      "Iteration 948, loss = 0.50435713\n",
      "Iteration 949, loss = 0.50399090\n",
      "Iteration 950, loss = 0.50362481\n",
      "Iteration 951, loss = 0.50325886\n",
      "Iteration 952, loss = 0.50289304\n",
      "Iteration 953, loss = 0.50252737\n",
      "Iteration 954, loss = 0.50216184\n",
      "Iteration 955, loss = 0.50179645\n",
      "Iteration 956, loss = 0.50143121\n",
      "Iteration 957, loss = 0.50106611\n",
      "Iteration 958, loss = 0.50070117\n",
      "Iteration 959, loss = 0.50033637\n",
      "Iteration 960, loss = 0.49997172\n",
      "Iteration 961, loss = 0.49960722\n",
      "Iteration 962, loss = 0.49924288\n",
      "Iteration 963, loss = 0.49887870\n",
      "Iteration 964, loss = 0.49851467\n",
      "Iteration 965, loss = 0.49815080\n",
      "Iteration 966, loss = 0.49778709\n",
      "Iteration 967, loss = 0.49742354\n",
      "Iteration 968, loss = 0.49706015\n",
      "Iteration 969, loss = 0.49669692\n",
      "Iteration 970, loss = 0.49633386\n",
      "Iteration 971, loss = 0.49597097\n",
      "Iteration 972, loss = 0.49560825\n",
      "Iteration 973, loss = 0.49524569\n",
      "Iteration 974, loss = 0.49488331\n",
      "Iteration 975, loss = 0.49452109\n",
      "Iteration 976, loss = 0.49415905\n",
      "Iteration 977, loss = 0.49379719\n",
      "Iteration 978, loss = 0.49343550\n",
      "Iteration 979, loss = 0.49307399\n",
      "Iteration 980, loss = 0.49271266\n",
      "Iteration 981, loss = 0.49235150\n",
      "Iteration 982, loss = 0.49199053\n",
      "Iteration 983, loss = 0.49162974\n",
      "Iteration 984, loss = 0.49126914\n",
      "Iteration 985, loss = 0.49090872\n",
      "Iteration 986, loss = 0.49054848\n",
      "Iteration 987, loss = 0.49018844\n",
      "Iteration 988, loss = 0.48982858\n",
      "Iteration 989, loss = 0.48946891\n",
      "Iteration 990, loss = 0.48910944\n",
      "Iteration 991, loss = 0.48875015\n",
      "Iteration 992, loss = 0.48839106\n",
      "Iteration 993, loss = 0.48803217\n",
      "Iteration 994, loss = 0.48767347\n",
      "Iteration 995, loss = 0.48731497\n",
      "Iteration 996, loss = 0.48695667\n",
      "Iteration 997, loss = 0.48659857\n",
      "Iteration 998, loss = 0.48624067\n",
      "Iteration 999, loss = 0.48588297\n",
      "Iteration 1000, loss = 0.48552548\n",
      "Iteration 1001, loss = 0.48516819\n",
      "Iteration 1002, loss = 0.48481110\n",
      "Iteration 1003, loss = 0.48445422\n",
      "Iteration 1004, loss = 0.48409756\n",
      "Iteration 1005, loss = 0.48374110\n",
      "Iteration 1006, loss = 0.48338485\n",
      "Iteration 1007, loss = 0.48302881\n",
      "Iteration 1008, loss = 0.48267298\n",
      "Iteration 1009, loss = 0.48231737\n",
      "Iteration 1010, loss = 0.48196197\n",
      "Iteration 1011, loss = 0.48160679\n",
      "Iteration 1012, loss = 0.48125183\n",
      "Iteration 1013, loss = 0.48089708\n",
      "Iteration 1014, loss = 0.48054255\n",
      "Iteration 1015, loss = 0.48018825\n",
      "Iteration 1016, loss = 0.47983416\n",
      "Iteration 1017, loss = 0.47948030\n",
      "Iteration 1018, loss = 0.47912666\n",
      "Iteration 1019, loss = 0.47877324\n",
      "Iteration 1020, loss = 0.47842005\n",
      "Iteration 1021, loss = 0.47806709\n",
      "Iteration 1022, loss = 0.47771435\n",
      "Iteration 1023, loss = 0.47736184\n",
      "Iteration 1024, loss = 0.47700957\n",
      "Iteration 1025, loss = 0.47665752\n",
      "Iteration 1026, loss = 0.47630570\n",
      "Iteration 1027, loss = 0.47595412\n",
      "Iteration 1028, loss = 0.47560277\n",
      "Iteration 1029, loss = 0.47525165\n",
      "Iteration 1030, loss = 0.47490077\n",
      "Iteration 1031, loss = 0.47455013\n",
      "Iteration 1032, loss = 0.47419972\n",
      "Iteration 1033, loss = 0.47384955\n",
      "Iteration 1034, loss = 0.47349962\n",
      "Iteration 1035, loss = 0.47314993\n",
      "Iteration 1036, loss = 0.47280048\n",
      "Iteration 1037, loss = 0.47245128\n",
      "Iteration 1038, loss = 0.47210231\n",
      "Iteration 1039, loss = 0.47175359\n",
      "Iteration 1040, loss = 0.47140512\n",
      "Iteration 1041, loss = 0.47105689\n",
      "Iteration 1042, loss = 0.47070891\n",
      "Iteration 1043, loss = 0.47036117\n",
      "Iteration 1044, loss = 0.47001368\n",
      "Iteration 1045, loss = 0.46966644\n",
      "Iteration 1046, loss = 0.46931946\n",
      "Iteration 1047, loss = 0.46897272\n",
      "Iteration 1048, loss = 0.46862623\n",
      "Iteration 1049, loss = 0.46828000\n",
      "Iteration 1050, loss = 0.46793402\n",
      "Iteration 1051, loss = 0.46758829\n",
      "Iteration 1052, loss = 0.46724282\n",
      "Iteration 1053, loss = 0.46689760\n",
      "Iteration 1054, loss = 0.46655264\n",
      "Iteration 1055, loss = 0.46620794\n",
      "Iteration 1056, loss = 0.46586350\n",
      "Iteration 1057, loss = 0.46551931\n",
      "Iteration 1058, loss = 0.46517539\n",
      "Iteration 1059, loss = 0.46483173\n",
      "Iteration 1060, loss = 0.46448832\n",
      "Iteration 1061, loss = 0.46414518\n",
      "Iteration 1062, loss = 0.46380230\n",
      "Iteration 1063, loss = 0.46345969\n",
      "Iteration 1064, loss = 0.46311734\n",
      "Iteration 1065, loss = 0.46277525\n",
      "Iteration 1066, loss = 0.46243343\n",
      "Iteration 1067, loss = 0.46209188\n",
      "Iteration 1068, loss = 0.46175059\n",
      "Iteration 1069, loss = 0.46140958\n",
      "Iteration 1070, loss = 0.46106883\n",
      "Iteration 1071, loss = 0.46072835\n",
      "Iteration 1072, loss = 0.46038814\n",
      "Iteration 1073, loss = 0.46004820\n",
      "Iteration 1074, loss = 0.45970853\n",
      "Iteration 1075, loss = 0.45936913\n",
      "Iteration 1076, loss = 0.45903001\n",
      "Iteration 1077, loss = 0.45869116\n",
      "Iteration 1078, loss = 0.45835259\n",
      "Iteration 1079, loss = 0.45801429\n",
      "Iteration 1080, loss = 0.45767626\n",
      "Iteration 1081, loss = 0.45733851\n",
      "Iteration 1082, loss = 0.45700104\n",
      "Iteration 1083, loss = 0.45666384\n",
      "Iteration 1084, loss = 0.45632693\n",
      "Iteration 1085, loss = 0.45599029\n",
      "Iteration 1086, loss = 0.45565393\n",
      "Iteration 1087, loss = 0.45531785\n",
      "Iteration 1088, loss = 0.45498205\n",
      "Iteration 1089, loss = 0.45464653\n",
      "Iteration 1090, loss = 0.45431129\n",
      "Iteration 1091, loss = 0.45397634\n",
      "Iteration 1092, loss = 0.45364167\n",
      "Iteration 1093, loss = 0.45330728\n",
      "Iteration 1094, loss = 0.45297317\n",
      "Iteration 1095, loss = 0.45263935\n",
      "Iteration 1096, loss = 0.45230582\n",
      "Iteration 1097, loss = 0.45197257\n",
      "Iteration 1098, loss = 0.45163960\n",
      "Iteration 1099, loss = 0.45130692\n",
      "Iteration 1100, loss = 0.45097453\n",
      "Iteration 1101, loss = 0.45064243\n",
      "Iteration 1102, loss = 0.45031062\n",
      "Iteration 1103, loss = 0.44997909\n",
      "Iteration 1104, loss = 0.44964786\n",
      "Iteration 1105, loss = 0.44931691\n",
      "Iteration 1106, loss = 0.44898625\n",
      "Iteration 1107, loss = 0.44865589\n",
      "Iteration 1108, loss = 0.44832581\n",
      "Iteration 1109, loss = 0.44799603\n",
      "Iteration 1110, loss = 0.44766654\n",
      "Iteration 1111, loss = 0.44733734\n",
      "Iteration 1112, loss = 0.44700844\n",
      "Iteration 1113, loss = 0.44667983\n",
      "Iteration 1114, loss = 0.44635151\n",
      "Iteration 1115, loss = 0.44602349\n",
      "Iteration 1116, loss = 0.44569576\n",
      "Iteration 1117, loss = 0.44536833\n",
      "Iteration 1118, loss = 0.44504119\n",
      "Iteration 1119, loss = 0.44471435\n",
      "Iteration 1120, loss = 0.44438781\n",
      "Iteration 1121, loss = 0.44406156\n",
      "Iteration 1122, loss = 0.44373561\n",
      "Iteration 1123, loss = 0.44340996\n",
      "Iteration 1124, loss = 0.44308461\n",
      "Iteration 1125, loss = 0.44275956\n",
      "Iteration 1126, loss = 0.44243480\n",
      "Iteration 1127, loss = 0.44211035\n",
      "Iteration 1128, loss = 0.44178619\n",
      "Iteration 1129, loss = 0.44146234\n",
      "Iteration 1130, loss = 0.44113879\n",
      "Iteration 1131, loss = 0.44081553\n",
      "Iteration 1132, loss = 0.44049258\n",
      "Iteration 1133, loss = 0.44016993\n",
      "Iteration 1134, loss = 0.43984759\n",
      "Iteration 1135, loss = 0.43952554\n",
      "Iteration 1136, loss = 0.43920380\n",
      "Iteration 1137, loss = 0.43888236\n",
      "Iteration 1138, loss = 0.43856123\n",
      "Iteration 1139, loss = 0.43824040\n",
      "Iteration 1140, loss = 0.43791987\n",
      "Iteration 1141, loss = 0.43759965\n",
      "Iteration 1142, loss = 0.43727973\n",
      "Iteration 1143, loss = 0.43696012\n",
      "Iteration 1144, loss = 0.43664081\n",
      "Iteration 1145, loss = 0.43632181\n",
      "Iteration 1146, loss = 0.43600311\n",
      "Iteration 1147, loss = 0.43568473\n",
      "Iteration 1148, loss = 0.43536665\n",
      "Iteration 1149, loss = 0.43504887\n",
      "Iteration 1150, loss = 0.43473140\n",
      "Iteration 1151, loss = 0.43441424\n",
      "Iteration 1152, loss = 0.43409739\n",
      "Iteration 1153, loss = 0.43378085\n",
      "Iteration 1154, loss = 0.43346461\n",
      "Iteration 1155, loss = 0.43314869\n",
      "Iteration 1156, loss = 0.43283307\n",
      "Iteration 1157, loss = 0.43251776\n",
      "Iteration 1158, loss = 0.43220276\n",
      "Iteration 1159, loss = 0.43188807\n",
      "Iteration 1160, loss = 0.43157369\n",
      "Iteration 1161, loss = 0.43125962\n",
      "Iteration 1162, loss = 0.43094586\n",
      "Iteration 1163, loss = 0.43063241\n",
      "Iteration 1164, loss = 0.43031928\n",
      "Iteration 1165, loss = 0.43000645\n",
      "Iteration 1166, loss = 0.42969393\n",
      "Iteration 1167, loss = 0.42938173\n",
      "Iteration 1168, loss = 0.42906984\n",
      "Iteration 1169, loss = 0.42875826\n",
      "Iteration 1170, loss = 0.42844699\n",
      "Iteration 1171, loss = 0.42813603\n",
      "Iteration 1172, loss = 0.42782539\n",
      "Iteration 1173, loss = 0.42751505\n",
      "Iteration 1174, loss = 0.42720504\n",
      "Iteration 1175, loss = 0.42689533\n",
      "Iteration 1176, loss = 0.42658594\n",
      "Iteration 1177, loss = 0.42627686\n",
      "Iteration 1178, loss = 0.42596809\n",
      "Iteration 1179, loss = 0.42565964\n",
      "Iteration 1180, loss = 0.42535150\n",
      "Iteration 1181, loss = 0.42504368\n",
      "Iteration 1182, loss = 0.42473617\n",
      "Iteration 1183, loss = 0.42442897\n",
      "Iteration 1184, loss = 0.42412209\n",
      "Iteration 1185, loss = 0.42381552\n",
      "Iteration 1186, loss = 0.42350927\n",
      "Iteration 1187, loss = 0.42320333\n",
      "Iteration 1188, loss = 0.42289771\n",
      "Iteration 1189, loss = 0.42259240\n",
      "Iteration 1190, loss = 0.42228741\n",
      "Iteration 1191, loss = 0.42198273\n",
      "Iteration 1192, loss = 0.42167837\n",
      "Iteration 1193, loss = 0.42137432\n",
      "Iteration 1194, loss = 0.42107059\n",
      "Iteration 1195, loss = 0.42076718\n",
      "Iteration 1196, loss = 0.42046408\n",
      "Iteration 1197, loss = 0.42016129\n",
      "Iteration 1198, loss = 0.41985883\n",
      "Iteration 1199, loss = 0.41955667\n",
      "Iteration 1200, loss = 0.41925484\n",
      "Iteration 1201, loss = 0.41895332\n",
      "Iteration 1202, loss = 0.41865212\n",
      "Iteration 1203, loss = 0.41835123\n",
      "Iteration 1204, loss = 0.41805066\n",
      "Iteration 1205, loss = 0.41775041\n",
      "Iteration 1206, loss = 0.41745047\n",
      "Iteration 1207, loss = 0.41715085\n",
      "Iteration 1208, loss = 0.41685155\n",
      "Iteration 1209, loss = 0.41655256\n",
      "Iteration 1210, loss = 0.41625389\n",
      "Iteration 1211, loss = 0.41595554\n",
      "Iteration 1212, loss = 0.41565750\n",
      "Iteration 1213, loss = 0.41535978\n",
      "Iteration 1214, loss = 0.41506238\n",
      "Iteration 1215, loss = 0.41476529\n",
      "Iteration 1216, loss = 0.41446852\n",
      "Iteration 1217, loss = 0.41417207\n",
      "Iteration 1218, loss = 0.41387594\n",
      "Iteration 1219, loss = 0.41358012\n",
      "Iteration 1220, loss = 0.41328462\n",
      "Iteration 1221, loss = 0.41298944\n",
      "Iteration 1222, loss = 0.41269457\n",
      "Iteration 1223, loss = 0.41240002\n",
      "Iteration 1224, loss = 0.41210579\n",
      "Iteration 1225, loss = 0.41181188\n",
      "Iteration 1226, loss = 0.41151828\n",
      "Iteration 1227, loss = 0.41122500\n",
      "Iteration 1228, loss = 0.41093203\n",
      "Iteration 1229, loss = 0.41063939\n",
      "Iteration 1230, loss = 0.41034706\n",
      "Iteration 1231, loss = 0.41005505\n",
      "Iteration 1232, loss = 0.40976335\n",
      "Iteration 1233, loss = 0.40947197\n",
      "Iteration 1234, loss = 0.40918091\n",
      "Iteration 1235, loss = 0.40889017\n",
      "Iteration 1236, loss = 0.40859974\n",
      "Iteration 1237, loss = 0.40830963\n",
      "Iteration 1238, loss = 0.40801983\n",
      "Iteration 1239, loss = 0.40773036\n",
      "Iteration 1240, loss = 0.40744120\n",
      "Iteration 1241, loss = 0.40715235\n",
      "Iteration 1242, loss = 0.40686383\n",
      "Iteration 1243, loss = 0.40657562\n",
      "Iteration 1244, loss = 0.40628772\n",
      "Iteration 1245, loss = 0.40600014\n",
      "Iteration 1246, loss = 0.40571288\n",
      "Iteration 1247, loss = 0.40542594\n",
      "Iteration 1248, loss = 0.40513931\n",
      "Iteration 1249, loss = 0.40485300\n",
      "Iteration 1250, loss = 0.40456700\n",
      "Iteration 1251, loss = 0.40428133\n",
      "Iteration 1252, loss = 0.40399597\n",
      "Iteration 1253, loss = 0.40371094\n",
      "Iteration 1254, loss = 0.40342626\n",
      "Iteration 1255, loss = 0.40314199\n",
      "Iteration 1256, loss = 0.40285803\n",
      "Iteration 1257, loss = 0.40257408\n",
      "Iteration 1258, loss = 0.40229041\n",
      "Iteration 1259, loss = 0.40200742\n",
      "Iteration 1260, loss = 0.40172459\n",
      "Iteration 1261, loss = 0.40144189\n",
      "Iteration 1262, loss = 0.40115980\n",
      "Iteration 1263, loss = 0.40087789\n",
      "Iteration 1264, loss = 0.40059621\n",
      "Iteration 1265, loss = 0.40031505\n",
      "Iteration 1266, loss = 0.40003404\n",
      "Iteration 1267, loss = 0.39975338\n",
      "Iteration 1268, loss = 0.39947312\n",
      "Iteration 1269, loss = 0.39919304\n",
      "Iteration 1270, loss = 0.39891338\n",
      "Iteration 1271, loss = 0.39863401\n",
      "Iteration 1272, loss = 0.39835490\n",
      "Iteration 1273, loss = 0.39807620\n",
      "Iteration 1274, loss = 0.39779772\n",
      "Iteration 1275, loss = 0.39751961\n",
      "Iteration 1276, loss = 0.39724181\n",
      "Iteration 1277, loss = 0.39696428\n",
      "Iteration 1278, loss = 0.39668712\n",
      "Iteration 1279, loss = 0.39641024\n",
      "Iteration 1280, loss = 0.39613368\n",
      "Iteration 1281, loss = 0.39585745\n",
      "Iteration 1282, loss = 0.39558149\n",
      "Iteration 1283, loss = 0.39530588\n",
      "Iteration 1284, loss = 0.39503057\n",
      "Iteration 1285, loss = 0.39475557\n",
      "Iteration 1286, loss = 0.39448090\n",
      "Iteration 1287, loss = 0.39420651\n",
      "Iteration 1288, loss = 0.39393245\n",
      "Iteration 1289, loss = 0.39365871\n",
      "Iteration 1290, loss = 0.39338526\n",
      "Iteration 1291, loss = 0.39311214\n",
      "Iteration 1292, loss = 0.39283932\n",
      "Iteration 1293, loss = 0.39256681\n",
      "Iteration 1294, loss = 0.39229462\n",
      "Iteration 1295, loss = 0.39202273\n",
      "Iteration 1296, loss = 0.39175116\n",
      "Iteration 1297, loss = 0.39147989\n",
      "Iteration 1298, loss = 0.39120893\n",
      "Iteration 1299, loss = 0.39093829\n",
      "Iteration 1300, loss = 0.39066795\n",
      "Iteration 1301, loss = 0.39039793\n",
      "Iteration 1302, loss = 0.39012821\n",
      "Iteration 1303, loss = 0.38985880\n",
      "Iteration 1304, loss = 0.38958970\n",
      "Iteration 1305, loss = 0.38932090\n",
      "Iteration 1306, loss = 0.38905242\n",
      "Iteration 1307, loss = 0.38878425\n",
      "Iteration 1308, loss = 0.38851638\n",
      "Iteration 1309, loss = 0.38824882\n",
      "Iteration 1310, loss = 0.38798156\n",
      "Iteration 1311, loss = 0.38771462\n",
      "Iteration 1312, loss = 0.38744798\n",
      "Iteration 1313, loss = 0.38718165\n",
      "Iteration 1314, loss = 0.38691562\n",
      "Iteration 1315, loss = 0.38664990\n",
      "Iteration 1316, loss = 0.38638449\n",
      "Iteration 1317, loss = 0.38611938\n",
      "Iteration 1318, loss = 0.38585458\n",
      "Iteration 1319, loss = 0.38559008\n",
      "Iteration 1320, loss = 0.38532589\n",
      "Iteration 1321, loss = 0.38506201\n",
      "Iteration 1322, loss = 0.38479843\n",
      "Iteration 1323, loss = 0.38453515\n",
      "Iteration 1324, loss = 0.38427218\n",
      "Iteration 1325, loss = 0.38400951\n",
      "Iteration 1326, loss = 0.38374715\n",
      "Iteration 1327, loss = 0.38348509\n",
      "Iteration 1328, loss = 0.38322333\n",
      "Iteration 1329, loss = 0.38296188\n",
      "Iteration 1330, loss = 0.38270073\n",
      "Iteration 1331, loss = 0.38243988\n",
      "Iteration 1332, loss = 0.38217933\n",
      "Iteration 1333, loss = 0.38191909\n",
      "Iteration 1334, loss = 0.38165915\n",
      "Iteration 1335, loss = 0.38139951\n",
      "Iteration 1336, loss = 0.38114017\n",
      "Iteration 1337, loss = 0.38088114\n",
      "Iteration 1338, loss = 0.38062240\n",
      "Iteration 1339, loss = 0.38036396\n",
      "Iteration 1340, loss = 0.38010583\n",
      "Iteration 1341, loss = 0.37984799\n",
      "Iteration 1342, loss = 0.37959046\n",
      "Iteration 1343, loss = 0.37933323\n",
      "Iteration 1344, loss = 0.37907629\n",
      "Iteration 1345, loss = 0.37881965\n",
      "Iteration 1346, loss = 0.37856332\n",
      "Iteration 1347, loss = 0.37830728\n",
      "Iteration 1348, loss = 0.37805154\n",
      "Iteration 1349, loss = 0.37779609\n",
      "Iteration 1350, loss = 0.37754095\n",
      "Iteration 1351, loss = 0.37728610\n",
      "Iteration 1352, loss = 0.37703155\n",
      "Iteration 1353, loss = 0.37677730\n",
      "Iteration 1354, loss = 0.37652334\n",
      "Iteration 1355, loss = 0.37626968\n",
      "Iteration 1356, loss = 0.37601632\n",
      "Iteration 1357, loss = 0.37576325\n",
      "Iteration 1358, loss = 0.37551048\n",
      "Iteration 1359, loss = 0.37525800\n",
      "Iteration 1360, loss = 0.37500582\n",
      "Iteration 1361, loss = 0.37475393\n",
      "Iteration 1362, loss = 0.37450234\n",
      "Iteration 1363, loss = 0.37425104\n",
      "Iteration 1364, loss = 0.37400004\n",
      "Iteration 1365, loss = 0.37374933\n",
      "Iteration 1366, loss = 0.37349891\n",
      "Iteration 1367, loss = 0.37324878\n",
      "Iteration 1368, loss = 0.37299895\n",
      "Iteration 1369, loss = 0.37274941\n",
      "Iteration 1370, loss = 0.37250016\n",
      "Iteration 1371, loss = 0.37225121\n",
      "Iteration 1372, loss = 0.37200254\n",
      "Iteration 1373, loss = 0.37175417\n",
      "Iteration 1374, loss = 0.37150609\n",
      "Iteration 1375, loss = 0.37125830\n",
      "Iteration 1376, loss = 0.37101080\n",
      "Iteration 1377, loss = 0.37076359\n",
      "Iteration 1378, loss = 0.37051667\n",
      "Iteration 1379, loss = 0.37027004\n",
      "Iteration 1380, loss = 0.37002369\n",
      "Iteration 1381, loss = 0.36977764\n",
      "Iteration 1382, loss = 0.36953188\n",
      "Iteration 1383, loss = 0.36928640\n",
      "Iteration 1384, loss = 0.36904121\n",
      "Iteration 1385, loss = 0.36879631\n",
      "Iteration 1386, loss = 0.36855169\n",
      "Iteration 1387, loss = 0.36830737\n",
      "Iteration 1388, loss = 0.36806333\n",
      "Iteration 1389, loss = 0.36781957\n",
      "Iteration 1390, loss = 0.36757611\n",
      "Iteration 1391, loss = 0.36733292\n",
      "Iteration 1392, loss = 0.36709003\n",
      "Iteration 1393, loss = 0.36684742\n",
      "Iteration 1394, loss = 0.36660509\n",
      "Iteration 1395, loss = 0.36636305\n",
      "Iteration 1396, loss = 0.36612129\n",
      "Iteration 1397, loss = 0.36587981\n",
      "Iteration 1398, loss = 0.36563862\n",
      "Iteration 1399, loss = 0.36539771\n",
      "Iteration 1400, loss = 0.36515709\n",
      "Iteration 1401, loss = 0.36491675\n",
      "Iteration 1402, loss = 0.36467669\n",
      "Iteration 1403, loss = 0.36443691\n",
      "Iteration 1404, loss = 0.36419741\n",
      "Iteration 1405, loss = 0.36395820\n",
      "Iteration 1406, loss = 0.36371926\n",
      "Iteration 1407, loss = 0.36348061\n",
      "Iteration 1408, loss = 0.36324223\n",
      "Iteration 1409, loss = 0.36300414\n",
      "Iteration 1410, loss = 0.36276632\n",
      "Iteration 1411, loss = 0.36252879\n",
      "Iteration 1412, loss = 0.36229153\n",
      "Iteration 1413, loss = 0.36205455\n",
      "Iteration 1414, loss = 0.36181785\n",
      "Iteration 1415, loss = 0.36158143\n",
      "Iteration 1416, loss = 0.36134529\n",
      "Iteration 1417, loss = 0.36110942\n",
      "Iteration 1418, loss = 0.36087383\n",
      "Iteration 1419, loss = 0.36063852\n",
      "Iteration 1420, loss = 0.36040348\n",
      "Iteration 1421, loss = 0.36016872\n",
      "Iteration 1422, loss = 0.35993423\n",
      "Iteration 1423, loss = 0.35970002\n",
      "Iteration 1424, loss = 0.35946608\n",
      "Iteration 1425, loss = 0.35923242\n",
      "Iteration 1426, loss = 0.35899903\n",
      "Iteration 1427, loss = 0.35876592\n",
      "Iteration 1428, loss = 0.35853308\n",
      "Iteration 1429, loss = 0.35830051\n",
      "Iteration 1430, loss = 0.35806822\n",
      "Iteration 1431, loss = 0.35783620\n",
      "Iteration 1432, loss = 0.35760445\n",
      "Iteration 1433, loss = 0.35737297\n",
      "Iteration 1434, loss = 0.35714176\n",
      "Iteration 1435, loss = 0.35691083\n",
      "Iteration 1436, loss = 0.35668016\n",
      "Iteration 1437, loss = 0.35644976\n",
      "Iteration 1438, loss = 0.35621964\n",
      "Iteration 1439, loss = 0.35598978\n",
      "Iteration 1440, loss = 0.35576020\n",
      "Iteration 1441, loss = 0.35553088\n",
      "Iteration 1442, loss = 0.35530183\n",
      "Iteration 1443, loss = 0.35507305\n",
      "Iteration 1444, loss = 0.35484454\n",
      "Iteration 1445, loss = 0.35461629\n",
      "Iteration 1446, loss = 0.35438832\n",
      "Iteration 1447, loss = 0.35416061\n",
      "Iteration 1448, loss = 0.35393316\n",
      "Iteration 1449, loss = 0.35370598\n",
      "Iteration 1450, loss = 0.35347907\n",
      "Iteration 1451, loss = 0.35325242\n",
      "Iteration 1452, loss = 0.35302604\n",
      "Iteration 1453, loss = 0.35279992\n",
      "Iteration 1454, loss = 0.35257407\n",
      "Iteration 1455, loss = 0.35234848\n",
      "Iteration 1456, loss = 0.35212315\n",
      "Iteration 1457, loss = 0.35189809\n",
      "Iteration 1458, loss = 0.35167329\n",
      "Iteration 1459, loss = 0.35144875\n",
      "Iteration 1460, loss = 0.35122448\n",
      "Iteration 1461, loss = 0.35100046\n",
      "Iteration 1462, loss = 0.35077671\n",
      "Iteration 1463, loss = 0.35055322\n",
      "Iteration 1464, loss = 0.35032999\n",
      "Iteration 1465, loss = 0.35010702\n",
      "Iteration 1466, loss = 0.34988431\n",
      "Iteration 1467, loss = 0.34966186\n",
      "Iteration 1468, loss = 0.34943967\n",
      "Iteration 1469, loss = 0.34921773\n",
      "Iteration 1470, loss = 0.34899606\n",
      "Iteration 1471, loss = 0.34877464\n",
      "Iteration 1472, loss = 0.34855349\n",
      "Iteration 1473, loss = 0.34833258\n",
      "Iteration 1474, loss = 0.34811194\n",
      "Iteration 1475, loss = 0.34789155\n",
      "Iteration 1476, loss = 0.34767142\n",
      "Iteration 1477, loss = 0.34745155\n",
      "Iteration 1478, loss = 0.34723193\n",
      "Iteration 1479, loss = 0.34701256\n",
      "Iteration 1480, loss = 0.34679346\n",
      "Iteration 1481, loss = 0.34657460\n",
      "Iteration 1482, loss = 0.34635600\n",
      "Iteration 1483, loss = 0.34613765\n",
      "Iteration 1484, loss = 0.34591956\n",
      "Iteration 1485, loss = 0.34570172\n",
      "Iteration 1486, loss = 0.34548413\n",
      "Iteration 1487, loss = 0.34526679\n",
      "Iteration 1488, loss = 0.34504971\n",
      "Iteration 1489, loss = 0.34483288\n",
      "Iteration 1490, loss = 0.34461630\n",
      "Iteration 1491, loss = 0.34439997\n",
      "Iteration 1492, loss = 0.34418389\n",
      "Iteration 1493, loss = 0.34396806\n",
      "Iteration 1494, loss = 0.34375248\n",
      "Iteration 1495, loss = 0.34353714\n",
      "Iteration 1496, loss = 0.34332206\n",
      "Iteration 1497, loss = 0.34310723\n",
      "Iteration 1498, loss = 0.34289264\n",
      "Iteration 1499, loss = 0.34267830\n",
      "Iteration 1500, loss = 0.34246421\n",
      "Iteration 1501, loss = 0.34225037\n",
      "Iteration 1502, loss = 0.34203677\n",
      "Iteration 1503, loss = 0.34182342\n",
      "Iteration 1504, loss = 0.34161032\n",
      "Iteration 1505, loss = 0.34139746\n",
      "Iteration 1506, loss = 0.34118485\n",
      "Iteration 1507, loss = 0.34097248\n",
      "Iteration 1508, loss = 0.34076035\n",
      "Iteration 1509, loss = 0.34054847\n",
      "Iteration 1510, loss = 0.34033684\n",
      "Iteration 1511, loss = 0.34012544\n",
      "Iteration 1512, loss = 0.33991429\n",
      "Iteration 1513, loss = 0.33970338\n",
      "Iteration 1514, loss = 0.33949272\n",
      "Iteration 1515, loss = 0.33928229\n",
      "Iteration 1516, loss = 0.33907211\n",
      "Iteration 1517, loss = 0.33886217\n",
      "Iteration 1518, loss = 0.33865247\n",
      "Iteration 1519, loss = 0.33844301\n",
      "Iteration 1520, loss = 0.33823379\n",
      "Iteration 1521, loss = 0.33802481\n",
      "Iteration 1522, loss = 0.33781606\n",
      "Iteration 1523, loss = 0.33760756\n",
      "Iteration 1524, loss = 0.33739930\n",
      "Iteration 1525, loss = 0.33719127\n",
      "Iteration 1526, loss = 0.33698348\n",
      "Iteration 1527, loss = 0.33677593\n",
      "Iteration 1528, loss = 0.33656861\n",
      "Iteration 1529, loss = 0.33636154\n",
      "Iteration 1530, loss = 0.33615469\n",
      "Iteration 1531, loss = 0.33594809\n",
      "Iteration 1532, loss = 0.33574172\n",
      "Iteration 1533, loss = 0.33553558\n",
      "Iteration 1534, loss = 0.33532968\n",
      "Iteration 1535, loss = 0.33512401\n",
      "Iteration 1536, loss = 0.33491858\n",
      "Iteration 1537, loss = 0.33471338\n",
      "Iteration 1538, loss = 0.33450841\n",
      "Iteration 1539, loss = 0.33430368\n",
      "Iteration 1540, loss = 0.33409918\n",
      "Iteration 1541, loss = 0.33389491\n",
      "Iteration 1542, loss = 0.33369087\n",
      "Iteration 1543, loss = 0.33348707\n",
      "Iteration 1544, loss = 0.33328349\n",
      "Iteration 1545, loss = 0.33308015\n",
      "Iteration 1546, loss = 0.33287703\n",
      "Iteration 1547, loss = 0.33267415\n",
      "Iteration 1548, loss = 0.33247149\n",
      "Iteration 1549, loss = 0.33226907\n",
      "Iteration 1550, loss = 0.33206687\n",
      "Iteration 1551, loss = 0.33186490\n",
      "Iteration 1552, loss = 0.33166316\n",
      "Iteration 1553, loss = 0.33146164\n",
      "Iteration 1554, loss = 0.33126036\n",
      "Iteration 1555, loss = 0.33105930\n",
      "Iteration 1556, loss = 0.33085846\n",
      "Iteration 1557, loss = 0.33065786\n",
      "Iteration 1558, loss = 0.33045747\n",
      "Iteration 1559, loss = 0.33025732\n",
      "Iteration 1560, loss = 0.33005739\n",
      "Iteration 1561, loss = 0.32985768\n",
      "Iteration 1562, loss = 0.32965820\n",
      "Iteration 1563, loss = 0.32945894\n",
      "Iteration 1564, loss = 0.32925990\n",
      "Iteration 1565, loss = 0.32906109\n",
      "Iteration 1566, loss = 0.32886250\n",
      "Iteration 1567, loss = 0.32866413\n",
      "Iteration 1568, loss = 0.32846598\n",
      "Iteration 1569, loss = 0.32826806\n",
      "Iteration 1570, loss = 0.32807036\n",
      "Iteration 1571, loss = 0.32787287\n",
      "Iteration 1572, loss = 0.32767561\n",
      "Iteration 1573, loss = 0.32747857\n",
      "Iteration 1574, loss = 0.32728175\n",
      "Iteration 1575, loss = 0.32708514\n",
      "Iteration 1576, loss = 0.32688876\n",
      "Iteration 1577, loss = 0.32669259\n",
      "Iteration 1578, loss = 0.32649665\n",
      "Iteration 1579, loss = 0.32630092\n",
      "Iteration 1580, loss = 0.32610540\n",
      "Iteration 1581, loss = 0.32591011\n",
      "Iteration 1582, loss = 0.32571503\n",
      "Iteration 1583, loss = 0.32552017\n",
      "Iteration 1584, loss = 0.32532552\n",
      "Iteration 1585, loss = 0.32513109\n",
      "Iteration 1586, loss = 0.32493688\n",
      "Iteration 1587, loss = 0.32474288\n",
      "Iteration 1588, loss = 0.32454909\n",
      "Iteration 1589, loss = 0.32435552\n",
      "Iteration 1590, loss = 0.32416216\n",
      "Iteration 1591, loss = 0.32396901\n",
      "Iteration 1592, loss = 0.32377608\n",
      "Iteration 1593, loss = 0.32358336\n",
      "Iteration 1594, loss = 0.32339085\n",
      "Iteration 1595, loss = 0.32319856\n",
      "Iteration 1596, loss = 0.32300647\n",
      "Iteration 1597, loss = 0.32281460\n",
      "Iteration 1598, loss = 0.32262294\n",
      "Iteration 1599, loss = 0.32243149\n",
      "Iteration 1600, loss = 0.32224025\n",
      "Iteration 1601, loss = 0.32204921\n",
      "Iteration 1602, loss = 0.32185839\n",
      "Iteration 1603, loss = 0.32166778\n",
      "Iteration 1604, loss = 0.32147737\n",
      "Iteration 1605, loss = 0.32128717\n",
      "Iteration 1606, loss = 0.32109718\n",
      "Iteration 1607, loss = 0.32090740\n",
      "Iteration 1608, loss = 0.32071783\n",
      "Iteration 1609, loss = 0.32052846\n",
      "Iteration 1610, loss = 0.32033930\n",
      "Iteration 1611, loss = 0.32015034\n",
      "Iteration 1612, loss = 0.31996159\n",
      "Iteration 1613, loss = 0.31977304\n",
      "Iteration 1614, loss = 0.31958470\n",
      "Iteration 1615, loss = 0.31939657\n",
      "Iteration 1616, loss = 0.31920864\n",
      "Iteration 1617, loss = 0.31902091\n",
      "Iteration 1618, loss = 0.31883338\n",
      "Iteration 1619, loss = 0.31864606\n",
      "Iteration 1620, loss = 0.31845894\n",
      "Iteration 1621, loss = 0.31827203\n",
      "Iteration 1622, loss = 0.31808531\n",
      "Iteration 1623, loss = 0.31789880\n",
      "Iteration 1624, loss = 0.31771249\n",
      "Iteration 1625, loss = 0.31752638\n",
      "Iteration 1626, loss = 0.31734047\n",
      "Iteration 1627, loss = 0.31715476\n",
      "Iteration 1628, loss = 0.31696925\n",
      "Iteration 1629, loss = 0.31678394\n",
      "Iteration 1630, loss = 0.31659882\n",
      "Iteration 1631, loss = 0.31641391\n",
      "Iteration 1632, loss = 0.31622920\n",
      "Iteration 1633, loss = 0.31604468\n",
      "Iteration 1634, loss = 0.31586036\n",
      "Iteration 1635, loss = 0.31567624\n",
      "Iteration 1636, loss = 0.31549231\n",
      "Iteration 1637, loss = 0.31530859\n",
      "Iteration 1638, loss = 0.31512505\n",
      "Iteration 1639, loss = 0.31494172\n",
      "Iteration 1640, loss = 0.31475858\n",
      "Iteration 1641, loss = 0.31457563\n",
      "Iteration 1642, loss = 0.31439288\n",
      "Iteration 1643, loss = 0.31421032\n",
      "Iteration 1644, loss = 0.31402796\n",
      "Iteration 1645, loss = 0.31384579\n",
      "Iteration 1646, loss = 0.31366381\n",
      "Iteration 1647, loss = 0.31348203\n",
      "Iteration 1648, loss = 0.31330044\n",
      "Iteration 1649, loss = 0.31311905\n",
      "Iteration 1650, loss = 0.31293784\n",
      "Iteration 1651, loss = 0.31275684\n",
      "Iteration 1652, loss = 0.31257604\n",
      "Iteration 1653, loss = 0.31239546\n",
      "Iteration 1654, loss = 0.31221513\n",
      "Iteration 1655, loss = 0.31203501\n",
      "Iteration 1656, loss = 0.31185499\n",
      "Iteration 1657, loss = 0.31167491\n",
      "Iteration 1658, loss = 0.31149506\n",
      "Iteration 1659, loss = 0.31131566\n",
      "Iteration 1660, loss = 0.31113647\n",
      "Iteration 1661, loss = 0.31095726\n",
      "Iteration 1662, loss = 0.31077822\n",
      "Iteration 1663, loss = 0.31059957\n",
      "Iteration 1664, loss = 0.31042106\n",
      "Iteration 1665, loss = 0.31024260\n",
      "Iteration 1666, loss = 0.31006440\n",
      "Iteration 1667, loss = 0.30988648\n",
      "Iteration 1668, loss = 0.30970864\n",
      "Iteration 1669, loss = 0.30953096\n",
      "Iteration 1670, loss = 0.30935356\n",
      "Iteration 1671, loss = 0.30917633\n",
      "Iteration 1672, loss = 0.30899921\n",
      "Iteration 1673, loss = 0.30882233\n",
      "Iteration 1674, loss = 0.30864566\n",
      "Iteration 1675, loss = 0.30846911\n",
      "Iteration 1676, loss = 0.30829276\n",
      "Iteration 1677, loss = 0.30811663\n",
      "Iteration 1678, loss = 0.30794066\n",
      "Iteration 1679, loss = 0.30776485\n",
      "Iteration 1680, loss = 0.30758925\n",
      "Iteration 1681, loss = 0.30741383\n",
      "Iteration 1682, loss = 0.30723857\n",
      "Iteration 1683, loss = 0.30706351\n",
      "Iteration 1684, loss = 0.30688863\n",
      "Iteration 1685, loss = 0.30671392\n",
      "Iteration 1686, loss = 0.30653939\n",
      "Iteration 1687, loss = 0.30636505\n",
      "Iteration 1688, loss = 0.30619088\n",
      "Iteration 1689, loss = 0.30601688\n",
      "Iteration 1690, loss = 0.30584307\n",
      "Iteration 1691, loss = 0.30566944\n",
      "Iteration 1692, loss = 0.30549598\n",
      "Iteration 1693, loss = 0.30532270\n",
      "Iteration 1694, loss = 0.30514960\n",
      "Iteration 1695, loss = 0.30497667\n",
      "Iteration 1696, loss = 0.30480392\n",
      "Iteration 1697, loss = 0.30463135\n",
      "Iteration 1698, loss = 0.30445895\n",
      "Iteration 1699, loss = 0.30428672\n",
      "Iteration 1700, loss = 0.30411468\n",
      "Iteration 1701, loss = 0.30394280\n",
      "Iteration 1702, loss = 0.30377110\n",
      "Iteration 1703, loss = 0.30359957\n",
      "Iteration 1704, loss = 0.30342822\n",
      "Iteration 1705, loss = 0.30325704\n",
      "Iteration 1706, loss = 0.30308603\n",
      "Iteration 1707, loss = 0.30291520\n",
      "Iteration 1708, loss = 0.30274453\n",
      "Iteration 1709, loss = 0.30257404\n",
      "Iteration 1710, loss = 0.30240372\n",
      "Iteration 1711, loss = 0.30223357\n",
      "Iteration 1712, loss = 0.30206359\n",
      "Iteration 1713, loss = 0.30189378\n",
      "Iteration 1714, loss = 0.30172414\n",
      "Iteration 1715, loss = 0.30155468\n",
      "Iteration 1716, loss = 0.30138538\n",
      "Iteration 1717, loss = 0.30121625\n",
      "Iteration 1718, loss = 0.30104728\n",
      "Iteration 1719, loss = 0.30087849\n",
      "Iteration 1720, loss = 0.30070987\n",
      "Iteration 1721, loss = 0.30054141\n",
      "Iteration 1722, loss = 0.30037312\n",
      "Iteration 1723, loss = 0.30020500\n",
      "Iteration 1724, loss = 0.30003704\n",
      "Iteration 1725, loss = 0.29986925\n",
      "Iteration 1726, loss = 0.29970163\n",
      "Iteration 1727, loss = 0.29953417\n",
      "Iteration 1728, loss = 0.29936688\n",
      "Iteration 1729, loss = 0.29919975\n",
      "Iteration 1730, loss = 0.29903279\n",
      "Iteration 1731, loss = 0.29886600\n",
      "Iteration 1732, loss = 0.29869936\n",
      "Iteration 1733, loss = 0.29853289\n",
      "Iteration 1734, loss = 0.29836659\n",
      "Iteration 1735, loss = 0.29820045\n",
      "Iteration 1736, loss = 0.29803447\n",
      "Iteration 1737, loss = 0.29786865\n",
      "Iteration 1738, loss = 0.29770300\n",
      "Iteration 1739, loss = 0.29753750\n",
      "Iteration 1740, loss = 0.29737217\n",
      "Iteration 1741, loss = 0.29720701\n",
      "Iteration 1742, loss = 0.29704200\n",
      "Iteration 1743, loss = 0.29687715\n",
      "Iteration 1744, loss = 0.29671246\n",
      "Iteration 1745, loss = 0.29654794\n",
      "Iteration 1746, loss = 0.29638357\n",
      "Iteration 1747, loss = 0.29621936\n",
      "Iteration 1748, loss = 0.29605531\n",
      "Iteration 1749, loss = 0.29589143\n",
      "Iteration 1750, loss = 0.29572769\n",
      "Iteration 1751, loss = 0.29556412\n",
      "Iteration 1752, loss = 0.29540071\n",
      "Iteration 1753, loss = 0.29523745\n",
      "Iteration 1754, loss = 0.29507435\n",
      "Iteration 1755, loss = 0.29491141\n",
      "Iteration 1756, loss = 0.29474862\n",
      "Iteration 1757, loss = 0.29458599\n",
      "Iteration 1758, loss = 0.29442352\n",
      "Iteration 1759, loss = 0.29426120\n",
      "Iteration 1760, loss = 0.29409904\n",
      "Iteration 1761, loss = 0.29393703\n",
      "Iteration 1762, loss = 0.29377518\n",
      "Iteration 1763, loss = 0.29361348\n",
      "Iteration 1764, loss = 0.29345194\n",
      "Iteration 1765, loss = 0.29329055\n",
      "Iteration 1766, loss = 0.29312932\n",
      "Iteration 1767, loss = 0.29296824\n",
      "Iteration 1768, loss = 0.29280731\n",
      "Iteration 1769, loss = 0.29264653\n",
      "Iteration 1770, loss = 0.29248591\n",
      "Iteration 1771, loss = 0.29232543\n",
      "Iteration 1772, loss = 0.29216511\n",
      "Iteration 1773, loss = 0.29200495\n",
      "Iteration 1774, loss = 0.29184493\n",
      "Iteration 1775, loss = 0.29168506\n",
      "Iteration 1776, loss = 0.29152535\n",
      "Iteration 1777, loss = 0.29136578\n",
      "Iteration 1778, loss = 0.29120637\n",
      "Iteration 1779, loss = 0.29104710\n",
      "Iteration 1780, loss = 0.29088799\n",
      "Iteration 1781, loss = 0.29072902\n",
      "Iteration 1782, loss = 0.29057020\n",
      "Iteration 1783, loss = 0.29041154\n",
      "Iteration 1784, loss = 0.29025302\n",
      "Iteration 1785, loss = 0.29009464\n",
      "Iteration 1786, loss = 0.28993642\n",
      "Iteration 1787, loss = 0.28977834\n",
      "Iteration 1788, loss = 0.28962041\n",
      "Iteration 1789, loss = 0.28946263\n",
      "Iteration 1790, loss = 0.28930499\n",
      "Iteration 1791, loss = 0.28914750\n",
      "Iteration 1792, loss = 0.28899016\n",
      "Iteration 1793, loss = 0.28883296\n",
      "Iteration 1794, loss = 0.28867590\n",
      "Iteration 1795, loss = 0.28851900\n",
      "Iteration 1796, loss = 0.28836223\n",
      "Iteration 1797, loss = 0.28820561\n",
      "Iteration 1798, loss = 0.28804914\n",
      "Iteration 1799, loss = 0.28789281\n",
      "Iteration 1800, loss = 0.28773662\n",
      "Iteration 1801, loss = 0.28758058\n",
      "Iteration 1802, loss = 0.28742468\n",
      "Iteration 1803, loss = 0.28726892\n",
      "Iteration 1804, loss = 0.28711331\n",
      "Iteration 1805, loss = 0.28695783\n",
      "Iteration 1806, loss = 0.28680250\n",
      "Iteration 1807, loss = 0.28664732\n",
      "Iteration 1808, loss = 0.28649227\n",
      "Iteration 1809, loss = 0.28633736\n",
      "Iteration 1810, loss = 0.28618260\n",
      "Iteration 1811, loss = 0.28602797\n",
      "Iteration 1812, loss = 0.28587349\n",
      "Iteration 1813, loss = 0.28571915\n",
      "Iteration 1814, loss = 0.28556494\n",
      "Iteration 1815, loss = 0.28541088\n",
      "Iteration 1816, loss = 0.28525695\n",
      "Iteration 1817, loss = 0.28510316\n",
      "Iteration 1818, loss = 0.28494952\n",
      "Iteration 1819, loss = 0.28479601\n",
      "Iteration 1820, loss = 0.28464264\n",
      "Iteration 1821, loss = 0.28448940\n",
      "Iteration 1822, loss = 0.28433631\n",
      "Iteration 1823, loss = 0.28418335\n",
      "Iteration 1824, loss = 0.28403053\n",
      "Iteration 1825, loss = 0.28387784\n",
      "Iteration 1826, loss = 0.28372530\n",
      "Iteration 1827, loss = 0.28357288\n",
      "Iteration 1828, loss = 0.28342061\n",
      "Iteration 1829, loss = 0.28326847\n",
      "Iteration 1830, loss = 0.28311646\n",
      "Iteration 1831, loss = 0.28296459\n",
      "Iteration 1832, loss = 0.28281286\n",
      "Iteration 1833, loss = 0.28266126\n",
      "Iteration 1834, loss = 0.28250980\n",
      "Iteration 1835, loss = 0.28235846\n",
      "Iteration 1836, loss = 0.28220727\n",
      "Iteration 1837, loss = 0.28205620\n",
      "Iteration 1838, loss = 0.28190527\n",
      "Iteration 1839, loss = 0.28175448\n",
      "Iteration 1840, loss = 0.28160381\n",
      "Iteration 1841, loss = 0.28145328\n",
      "Iteration 1842, loss = 0.28130288\n",
      "Iteration 1843, loss = 0.28115261\n",
      "Iteration 1844, loss = 0.28100248\n",
      "Iteration 1845, loss = 0.28085247\n",
      "Iteration 1846, loss = 0.28070260\n",
      "Iteration 1847, loss = 0.28055286\n",
      "Iteration 1848, loss = 0.28040325\n",
      "Iteration 1849, loss = 0.28025377\n",
      "Iteration 1850, loss = 0.28010442\n",
      "Iteration 1851, loss = 0.27995520\n",
      "Iteration 1852, loss = 0.27980611\n",
      "Iteration 1853, loss = 0.27965715\n",
      "Iteration 1854, loss = 0.27950831\n",
      "Iteration 1855, loss = 0.27935961\n",
      "Iteration 1856, loss = 0.27921104\n",
      "Iteration 1857, loss = 0.27906259\n",
      "Iteration 1858, loss = 0.27891427\n",
      "Iteration 1859, loss = 0.27876609\n",
      "Iteration 1860, loss = 0.27861802\n",
      "Iteration 1861, loss = 0.27847009\n",
      "Iteration 1862, loss = 0.27832228\n",
      "Iteration 1863, loss = 0.27817460\n",
      "Iteration 1864, loss = 0.27802705\n",
      "Iteration 1865, loss = 0.27787962\n",
      "Iteration 1866, loss = 0.27773232\n",
      "Iteration 1867, loss = 0.27758515\n",
      "Iteration 1868, loss = 0.27743810\n",
      "Iteration 1869, loss = 0.27729118\n",
      "Iteration 1870, loss = 0.27714438\n",
      "Iteration 1871, loss = 0.27699771\n",
      "Iteration 1872, loss = 0.27685116\n",
      "Iteration 1873, loss = 0.27670474\n",
      "Iteration 1874, loss = 0.27655844\n",
      "Iteration 1875, loss = 0.27641226\n",
      "Iteration 1876, loss = 0.27626621\n",
      "Iteration 1877, loss = 0.27612028\n",
      "Iteration 1878, loss = 0.27597448\n",
      "Iteration 1879, loss = 0.27582880\n",
      "Iteration 1880, loss = 0.27568324\n",
      "Iteration 1881, loss = 0.27553780\n",
      "Iteration 1882, loss = 0.27539249\n",
      "Iteration 1883, loss = 0.27524730\n",
      "Iteration 1884, loss = 0.27510223\n",
      "Iteration 1885, loss = 0.27495728\n",
      "Iteration 1886, loss = 0.27481246\n",
      "Iteration 1887, loss = 0.27466775\n",
      "Iteration 1888, loss = 0.27452317\n",
      "Iteration 1889, loss = 0.27437871\n",
      "Iteration 1890, loss = 0.27423436\n",
      "Iteration 1891, loss = 0.27409014\n",
      "Iteration 1892, loss = 0.27394604\n",
      "Iteration 1893, loss = 0.27380206\n",
      "Iteration 1894, loss = 0.27365820\n",
      "Iteration 1895, loss = 0.27351445\n",
      "Iteration 1896, loss = 0.27337083\n",
      "Iteration 1897, loss = 0.27322732\n",
      "Iteration 1898, loss = 0.27308394\n",
      "Iteration 1899, loss = 0.27294067\n",
      "Iteration 1900, loss = 0.27279752\n",
      "Iteration 1901, loss = 0.27265449\n",
      "Iteration 1902, loss = 0.27251157\n",
      "Iteration 1903, loss = 0.27236878\n",
      "Iteration 1904, loss = 0.27222610\n",
      "Iteration 1905, loss = 0.27208354\n",
      "Iteration 1906, loss = 0.27194109\n",
      "Iteration 1907, loss = 0.27179877\n",
      "Iteration 1908, loss = 0.27165655\n",
      "Iteration 1909, loss = 0.27151446\n",
      "Iteration 1910, loss = 0.27137248\n",
      "Iteration 1911, loss = 0.27123062\n",
      "Iteration 1912, loss = 0.27108887\n",
      "Iteration 1913, loss = 0.27094724\n",
      "Iteration 1914, loss = 0.27080572\n",
      "Iteration 1915, loss = 0.27066432\n",
      "Iteration 1916, loss = 0.27052303\n",
      "Iteration 1917, loss = 0.27038186\n",
      "Iteration 1918, loss = 0.27024080\n",
      "Iteration 1919, loss = 0.27009985\n",
      "Iteration 1920, loss = 0.26995902\n",
      "Iteration 1921, loss = 0.26981830\n",
      "Iteration 1922, loss = 0.26967770\n",
      "Iteration 1923, loss = 0.26953721\n",
      "Iteration 1924, loss = 0.26939683\n",
      "Iteration 1925, loss = 0.26925657\n",
      "Iteration 1926, loss = 0.26911642\n",
      "Iteration 1927, loss = 0.26897638\n",
      "Iteration 1928, loss = 0.26883645\n",
      "Iteration 1929, loss = 0.26869663\n",
      "Iteration 1930, loss = 0.26855693\n",
      "Iteration 1931, loss = 0.26841734\n",
      "Iteration 1932, loss = 0.26827786\n",
      "Iteration 1933, loss = 0.26813849\n",
      "Iteration 1934, loss = 0.26799923\n",
      "Iteration 1935, loss = 0.26786008\n",
      "Iteration 1936, loss = 0.26772105\n",
      "Iteration 1937, loss = 0.26758212\n",
      "Iteration 1938, loss = 0.26744330\n",
      "Iteration 1939, loss = 0.26730459\n",
      "Iteration 1940, loss = 0.26716600\n",
      "Iteration 1941, loss = 0.26702751\n",
      "Iteration 1942, loss = 0.26688913\n",
      "Iteration 1943, loss = 0.26675086\n",
      "Iteration 1944, loss = 0.26661270\n",
      "Iteration 1945, loss = 0.26647465\n",
      "Iteration 1946, loss = 0.26633671\n",
      "Iteration 1947, loss = 0.26619887\n",
      "Iteration 1948, loss = 0.26606115\n",
      "Iteration 1949, loss = 0.26592353\n",
      "Iteration 1950, loss = 0.26578602\n",
      "Iteration 1951, loss = 0.26564861\n",
      "Iteration 1952, loss = 0.26551132\n",
      "Iteration 1953, loss = 0.26537413\n",
      "Iteration 1954, loss = 0.26523705\n",
      "Iteration 1955, loss = 0.26510007\n",
      "Iteration 1956, loss = 0.26496321\n",
      "Iteration 1957, loss = 0.26482644\n",
      "Iteration 1958, loss = 0.26468979\n",
      "Iteration 1959, loss = 0.26455324\n",
      "Iteration 1960, loss = 0.26441679\n",
      "Iteration 1961, loss = 0.26428046\n",
      "Iteration 1962, loss = 0.26414422\n",
      "Iteration 1963, loss = 0.26400810\n",
      "Iteration 1964, loss = 0.26387207\n",
      "Iteration 1965, loss = 0.26373616\n",
      "Iteration 1966, loss = 0.26360034\n",
      "Iteration 1967, loss = 0.26346463\n",
      "Iteration 1968, loss = 0.26332903\n",
      "Iteration 1969, loss = 0.26319353\n",
      "Iteration 1970, loss = 0.26305813\n",
      "Iteration 1971, loss = 0.26292284\n",
      "Iteration 1972, loss = 0.26278765\n",
      "Iteration 1973, loss = 0.26265257\n",
      "Iteration 1974, loss = 0.26251759\n",
      "Iteration 1975, loss = 0.26238272\n",
      "Iteration 1976, loss = 0.26224795\n",
      "Iteration 1977, loss = 0.26211330\n",
      "Iteration 1978, loss = 0.26197877\n",
      "Iteration 1979, loss = 0.26184438\n",
      "Iteration 1980, loss = 0.26171013\n",
      "Iteration 1981, loss = 0.26157602\n",
      "Iteration 1982, loss = 0.26144185\n",
      "Iteration 1983, loss = 0.26130762\n",
      "Iteration 1984, loss = 0.26117345\n",
      "Iteration 1985, loss = 0.26103962\n",
      "Iteration 1986, loss = 0.26090603\n",
      "Iteration 1987, loss = 0.26077239\n",
      "Iteration 1988, loss = 0.26063871\n",
      "Iteration 1989, loss = 0.26050520\n",
      "Iteration 1990, loss = 0.26037194\n",
      "Iteration 1991, loss = 0.26023875\n",
      "Iteration 1992, loss = 0.26010552\n",
      "Iteration 1993, loss = 0.25997242\n",
      "Iteration 1994, loss = 0.25983953\n",
      "Iteration 1995, loss = 0.25970672\n",
      "Iteration 1996, loss = 0.25957393\n",
      "Iteration 1997, loss = 0.25944125\n",
      "Iteration 1998, loss = 0.25930874\n",
      "Iteration 1999, loss = 0.25917631\n",
      "Iteration 2000, loss = 0.25904393\n",
      "Iteration 2001, loss = 0.25891166\n",
      "Iteration 2002, loss = 0.25877954\n",
      "Iteration 2003, loss = 0.25864749\n",
      "Iteration 2004, loss = 0.25851551\n",
      "Iteration 2005, loss = 0.25838365\n",
      "Iteration 2006, loss = 0.25825191\n",
      "Iteration 2007, loss = 0.25812025\n",
      "Iteration 2008, loss = 0.25798867\n",
      "Iteration 2009, loss = 0.25785721\n",
      "Iteration 2010, loss = 0.25772585\n",
      "Iteration 2011, loss = 0.25759458\n",
      "Iteration 2012, loss = 0.25746339\n",
      "Iteration 2013, loss = 0.25733232\n",
      "Iteration 2014, loss = 0.25720134\n",
      "Iteration 2015, loss = 0.25707045\n",
      "Iteration 2016, loss = 0.25693966\n",
      "Iteration 2017, loss = 0.25680897\n",
      "Iteration 2018, loss = 0.25667838\n",
      "Iteration 2019, loss = 0.25654787\n",
      "Iteration 2020, loss = 0.25641746\n",
      "Iteration 2021, loss = 0.25628715\n",
      "Iteration 2022, loss = 0.25615694\n",
      "Iteration 2023, loss = 0.25602682\n",
      "Iteration 2024, loss = 0.25589679\n",
      "Iteration 2025, loss = 0.25576686\n",
      "Iteration 2026, loss = 0.25563703\n",
      "Iteration 2027, loss = 0.25550728\n",
      "Iteration 2028, loss = 0.25537763\n",
      "Iteration 2029, loss = 0.25524808\n",
      "Iteration 2030, loss = 0.25511862\n",
      "Iteration 2031, loss = 0.25498926\n",
      "Iteration 2032, loss = 0.25485998\n",
      "Iteration 2033, loss = 0.25473081\n",
      "Iteration 2034, loss = 0.25460172\n",
      "Iteration 2035, loss = 0.25447273\n",
      "Iteration 2036, loss = 0.25434383\n",
      "Iteration 2037, loss = 0.25421503\n",
      "Iteration 2038, loss = 0.25408632\n",
      "Iteration 2039, loss = 0.25395770\n",
      "Iteration 2040, loss = 0.25382917\n",
      "Iteration 2041, loss = 0.25370073\n",
      "Iteration 2042, loss = 0.25357239\n",
      "Iteration 2043, loss = 0.25344414\n",
      "Iteration 2044, loss = 0.25331598\n",
      "Iteration 2045, loss = 0.25318791\n",
      "Iteration 2046, loss = 0.25305994\n",
      "Iteration 2047, loss = 0.25293205\n",
      "Iteration 2048, loss = 0.25280426\n",
      "Iteration 2049, loss = 0.25267656\n",
      "Iteration 2050, loss = 0.25254895\n",
      "Iteration 2051, loss = 0.25242143\n",
      "Iteration 2052, loss = 0.25229400\n",
      "Iteration 2053, loss = 0.25216666\n",
      "Iteration 2054, loss = 0.25203942\n",
      "Iteration 2055, loss = 0.25191226\n",
      "Iteration 2056, loss = 0.25178519\n",
      "Iteration 2057, loss = 0.25165822\n",
      "Iteration 2058, loss = 0.25153133\n",
      "Iteration 2059, loss = 0.25140454\n",
      "Iteration 2060, loss = 0.25127783\n",
      "Iteration 2061, loss = 0.25115121\n",
      "Iteration 2062, loss = 0.25102468\n",
      "Iteration 2063, loss = 0.25089824\n",
      "Iteration 2064, loss = 0.25077190\n",
      "Iteration 2065, loss = 0.25064564\n",
      "Iteration 2066, loss = 0.25051946\n",
      "Iteration 2067, loss = 0.25039338\n",
      "Iteration 2068, loss = 0.25026739\n",
      "Iteration 2069, loss = 0.25014148\n",
      "Iteration 2070, loss = 0.25001567\n",
      "Iteration 2071, loss = 0.24988994\n",
      "Iteration 2072, loss = 0.24976430\n",
      "Iteration 2073, loss = 0.24963874\n",
      "Iteration 2074, loss = 0.24951328\n",
      "Iteration 2075, loss = 0.24938790\n",
      "Iteration 2076, loss = 0.24926261\n",
      "Iteration 2077, loss = 0.24913741\n",
      "Iteration 2078, loss = 0.24901230\n",
      "Iteration 2079, loss = 0.24888727\n",
      "Iteration 2080, loss = 0.24876233\n",
      "Iteration 2081, loss = 0.24863748\n",
      "Iteration 2082, loss = 0.24851271\n",
      "Iteration 2083, loss = 0.24838803\n",
      "Iteration 2084, loss = 0.24826344\n",
      "Iteration 2085, loss = 0.24813893\n",
      "Iteration 2086, loss = 0.24801452\n",
      "Iteration 2087, loss = 0.24789018\n",
      "Iteration 2088, loss = 0.24776593\n",
      "Iteration 2089, loss = 0.24764177\n",
      "Iteration 2090, loss = 0.24751770\n",
      "Iteration 2091, loss = 0.24739371\n",
      "Iteration 2092, loss = 0.24726981\n",
      "Iteration 2093, loss = 0.24714599\n",
      "Iteration 2094, loss = 0.24702226\n",
      "Iteration 2095, loss = 0.24689861\n",
      "Iteration 2096, loss = 0.24677505\n",
      "Iteration 2097, loss = 0.24665157\n",
      "Iteration 2098, loss = 0.24652818\n",
      "Iteration 2099, loss = 0.24640487\n",
      "Iteration 2100, loss = 0.24628165\n",
      "Iteration 2101, loss = 0.24615851\n",
      "Iteration 2102, loss = 0.24603546\n",
      "Iteration 2103, loss = 0.24591249\n",
      "Iteration 2104, loss = 0.24578960\n",
      "Iteration 2105, loss = 0.24566680\n",
      "Iteration 2106, loss = 0.24554409\n",
      "Iteration 2107, loss = 0.24542146\n",
      "Iteration 2108, loss = 0.24529891\n",
      "Iteration 2109, loss = 0.24517644\n",
      "Iteration 2110, loss = 0.24505406\n",
      "Iteration 2111, loss = 0.24493177\n",
      "Iteration 2112, loss = 0.24480955\n",
      "Iteration 2113, loss = 0.24468742\n",
      "Iteration 2114, loss = 0.24456537\n",
      "Iteration 2115, loss = 0.24444341\n",
      "Iteration 2116, loss = 0.24432153\n",
      "Iteration 2117, loss = 0.24419973\n",
      "Iteration 2118, loss = 0.24407802\n",
      "Iteration 2119, loss = 0.24395638\n",
      "Iteration 2120, loss = 0.24383483\n",
      "Iteration 2121, loss = 0.24371337\n",
      "Iteration 2122, loss = 0.24359198\n",
      "Iteration 2123, loss = 0.24347068\n",
      "Iteration 2124, loss = 0.24334946\n",
      "Iteration 2125, loss = 0.24322832\n",
      "Iteration 2126, loss = 0.24310726\n",
      "Iteration 2127, loss = 0.24298629\n",
      "Iteration 2128, loss = 0.24286539\n",
      "Iteration 2129, loss = 0.24274458\n",
      "Iteration 2130, loss = 0.24262385\n",
      "Iteration 2131, loss = 0.24250320\n",
      "Iteration 2132, loss = 0.24238264\n",
      "Iteration 2133, loss = 0.24226215\n",
      "Iteration 2134, loss = 0.24214175\n",
      "Iteration 2135, loss = 0.24202142\n",
      "Iteration 2136, loss = 0.24190118\n",
      "Iteration 2137, loss = 0.24178102\n",
      "Iteration 2138, loss = 0.24166094\n",
      "Iteration 2139, loss = 0.24154094\n",
      "Iteration 2140, loss = 0.24142102\n",
      "Iteration 2141, loss = 0.24130118\n",
      "Iteration 2142, loss = 0.24118142\n",
      "Iteration 2143, loss = 0.24106174\n",
      "Iteration 2144, loss = 0.24094214\n",
      "Iteration 2145, loss = 0.24082262\n",
      "Iteration 2146, loss = 0.24070319\n",
      "Iteration 2147, loss = 0.24058383\n",
      "Iteration 2148, loss = 0.24046455\n",
      "Iteration 2149, loss = 0.24034535\n",
      "Iteration 2150, loss = 0.24022623\n",
      "Iteration 2151, loss = 0.24010719\n",
      "Iteration 2152, loss = 0.23998823\n",
      "Iteration 2153, loss = 0.23986935\n",
      "Iteration 2154, loss = 0.23975055\n",
      "Iteration 2155, loss = 0.23963183\n",
      "Iteration 2156, loss = 0.23951319\n",
      "Iteration 2157, loss = 0.23939462\n",
      "Iteration 2158, loss = 0.23927614\n",
      "Iteration 2159, loss = 0.23915773\n",
      "Iteration 2160, loss = 0.23903940\n",
      "Iteration 2161, loss = 0.23892115\n",
      "Iteration 2162, loss = 0.23880298\n",
      "Iteration 2163, loss = 0.23868489\n",
      "Iteration 2164, loss = 0.23856688\n",
      "Iteration 2165, loss = 0.23844894\n",
      "Iteration 2166, loss = 0.23833109\n",
      "Iteration 2167, loss = 0.23821331\n",
      "Iteration 2168, loss = 0.23809561\n",
      "Iteration 2169, loss = 0.23797799\n",
      "Iteration 2170, loss = 0.23786044\n",
      "Iteration 2171, loss = 0.23774298\n",
      "Iteration 2172, loss = 0.23762559\n",
      "Iteration 2173, loss = 0.23750828\n",
      "Iteration 2174, loss = 0.23739104\n",
      "Iteration 2175, loss = 0.23727389\n",
      "Iteration 2176, loss = 0.23715681\n",
      "Iteration 2177, loss = 0.23703981\n",
      "Iteration 2178, loss = 0.23692288\n",
      "Iteration 2179, loss = 0.23680604\n",
      "Iteration 2180, loss = 0.23668927\n",
      "Iteration 2181, loss = 0.23657257\n",
      "Iteration 2182, loss = 0.23645596\n",
      "Iteration 2183, loss = 0.23633942\n",
      "Iteration 2184, loss = 0.23622296\n",
      "Iteration 2185, loss = 0.23610657\n",
      "Iteration 2186, loss = 0.23599026\n",
      "Iteration 2187, loss = 0.23587403\n",
      "Iteration 2188, loss = 0.23575787\n",
      "Iteration 2189, loss = 0.23564179\n",
      "Iteration 2190, loss = 0.23552579\n",
      "Iteration 2191, loss = 0.23540986\n",
      "Iteration 2192, loss = 0.23529401\n",
      "Iteration 2193, loss = 0.23517824\n",
      "Iteration 2194, loss = 0.23506254\n",
      "Iteration 2195, loss = 0.23494692\n",
      "Iteration 2196, loss = 0.23483137\n",
      "Iteration 2197, loss = 0.23471590\n",
      "Iteration 2198, loss = 0.23460051\n",
      "Iteration 2199, loss = 0.23448519\n",
      "Iteration 2200, loss = 0.23436994\n",
      "Iteration 2201, loss = 0.23425478\n",
      "Iteration 2202, loss = 0.23413968\n",
      "Iteration 2203, loss = 0.23402467\n",
      "Iteration 2204, loss = 0.23390972\n",
      "Iteration 2205, loss = 0.23379486\n",
      "Iteration 2206, loss = 0.23368007\n",
      "Iteration 2207, loss = 0.23356535\n",
      "Iteration 2208, loss = 0.23345071\n",
      "Iteration 2209, loss = 0.23333614\n",
      "Iteration 2210, loss = 0.23322165\n",
      "Iteration 2211, loss = 0.23310723\n",
      "Iteration 2212, loss = 0.23299289\n",
      "Iteration 2213, loss = 0.23287862\n",
      "Iteration 2214, loss = 0.23276443\n",
      "Iteration 2215, loss = 0.23265031\n",
      "Iteration 2216, loss = 0.23253627\n",
      "Iteration 2217, loss = 0.23242230\n",
      "Iteration 2218, loss = 0.23230841\n",
      "Iteration 2219, loss = 0.23219459\n",
      "Iteration 2220, loss = 0.23208084\n",
      "Iteration 2221, loss = 0.23196717\n",
      "Iteration 2222, loss = 0.23185357\n",
      "Iteration 2223, loss = 0.23174005\n",
      "Iteration 2224, loss = 0.23162660\n",
      "Iteration 2225, loss = 0.23151322\n",
      "Iteration 2226, loss = 0.23139992\n",
      "Iteration 2227, loss = 0.23128669\n",
      "Iteration 2228, loss = 0.23117354\n",
      "Iteration 2229, loss = 0.23106046\n",
      "Iteration 2230, loss = 0.23094745\n",
      "Iteration 2231, loss = 0.23083452\n",
      "Iteration 2232, loss = 0.23072166\n",
      "Iteration 2233, loss = 0.23060887\n",
      "Iteration 2234, loss = 0.23049616\n",
      "Iteration 2235, loss = 0.23038352\n",
      "Iteration 2236, loss = 0.23027095\n",
      "Iteration 2237, loss = 0.23015846\n",
      "Iteration 2238, loss = 0.23004604\n",
      "Iteration 2239, loss = 0.22993369\n",
      "Iteration 2240, loss = 0.22982142\n",
      "Iteration 2241, loss = 0.22970922\n",
      "Iteration 2242, loss = 0.22959709\n",
      "Iteration 2243, loss = 0.22948504\n",
      "Iteration 2244, loss = 0.22937306\n",
      "Iteration 2245, loss = 0.22926115\n",
      "Iteration 2246, loss = 0.22914933\n",
      "Iteration 2247, loss = 0.22903759\n",
      "Iteration 2248, loss = 0.22892595\n",
      "Iteration 2249, loss = 0.22881441\n",
      "Iteration 2250, loss = 0.22870301\n",
      "Iteration 2251, loss = 0.22859167\n",
      "Iteration 2252, loss = 0.22848036\n",
      "Iteration 2253, loss = 0.22836886\n",
      "Iteration 2254, loss = 0.22825736\n",
      "Iteration 2255, loss = 0.22814609\n",
      "Iteration 2256, loss = 0.22803512\n",
      "Iteration 2257, loss = 0.22792423\n",
      "Iteration 2258, loss = 0.22781319\n",
      "Iteration 2259, loss = 0.22770216\n",
      "Iteration 2260, loss = 0.22759134\n",
      "Iteration 2261, loss = 0.22748071\n",
      "Iteration 2262, loss = 0.22737007\n",
      "Iteration 2263, loss = 0.22725938\n",
      "Iteration 2264, loss = 0.22714881\n",
      "Iteration 2265, loss = 0.22703842\n",
      "Iteration 2266, loss = 0.22692808\n",
      "Iteration 2267, loss = 0.22681773\n",
      "Iteration 2268, loss = 0.22670745\n",
      "Iteration 2269, loss = 0.22659732\n",
      "Iteration 2270, loss = 0.22648727\n",
      "Iteration 2271, loss = 0.22637722\n",
      "Iteration 2272, loss = 0.22626724\n",
      "Iteration 2273, loss = 0.22615738\n",
      "Iteration 2274, loss = 0.22604760\n",
      "Iteration 2275, loss = 0.22593786\n",
      "Iteration 2276, loss = 0.22582817\n",
      "Iteration 2277, loss = 0.22571859\n",
      "Iteration 2278, loss = 0.22560909\n",
      "Iteration 2279, loss = 0.22549964\n",
      "Iteration 2280, loss = 0.22539024\n",
      "Iteration 2281, loss = 0.22528094\n",
      "Iteration 2282, loss = 0.22517172\n",
      "Iteration 2283, loss = 0.22506255\n",
      "Iteration 2284, loss = 0.22495345\n",
      "Iteration 2285, loss = 0.22484443\n",
      "Iteration 2286, loss = 0.22473548\n",
      "Iteration 2287, loss = 0.22462660\n",
      "Iteration 2288, loss = 0.22451778\n",
      "Iteration 2289, loss = 0.22440904\n",
      "Iteration 2290, loss = 0.22430038\n",
      "Iteration 2291, loss = 0.22419178\n",
      "Iteration 2292, loss = 0.22408325\n",
      "Iteration 2293, loss = 0.22397479\n",
      "Iteration 2294, loss = 0.22386641\n",
      "Iteration 2295, loss = 0.22375809\n",
      "Iteration 2296, loss = 0.22364984\n",
      "Iteration 2297, loss = 0.22354166\n",
      "Iteration 2298, loss = 0.22343356\n",
      "Iteration 2299, loss = 0.22332552\n",
      "Iteration 2300, loss = 0.22321756\n",
      "Iteration 2301, loss = 0.22310966\n",
      "Iteration 2302, loss = 0.22300183\n",
      "Iteration 2303, loss = 0.22289408\n",
      "Iteration 2304, loss = 0.22278639\n",
      "Iteration 2305, loss = 0.22267877\n",
      "Iteration 2306, loss = 0.22257122\n",
      "Iteration 2307, loss = 0.22246375\n",
      "Iteration 2308, loss = 0.22235634\n",
      "Iteration 2309, loss = 0.22224900\n",
      "Iteration 2310, loss = 0.22214173\n",
      "Iteration 2311, loss = 0.22203454\n",
      "Iteration 2312, loss = 0.22192741\n",
      "Iteration 2313, loss = 0.22182035\n",
      "Iteration 2314, loss = 0.22171336\n",
      "Iteration 2315, loss = 0.22160644\n",
      "Iteration 2316, loss = 0.22149959\n",
      "Iteration 2317, loss = 0.22139280\n",
      "Iteration 2318, loss = 0.22128609\n",
      "Iteration 2319, loss = 0.22117945\n",
      "Iteration 2320, loss = 0.22107287\n",
      "Iteration 2321, loss = 0.22096637\n",
      "Iteration 2322, loss = 0.22085993\n",
      "Iteration 2323, loss = 0.22075357\n",
      "Iteration 2324, loss = 0.22064727\n",
      "Iteration 2325, loss = 0.22054104\n",
      "Iteration 2326, loss = 0.22043488\n",
      "Iteration 2327, loss = 0.22032879\n",
      "Iteration 2328, loss = 0.22022277\n",
      "Iteration 2329, loss = 0.22011682\n",
      "Iteration 2330, loss = 0.22001093\n",
      "Iteration 2331, loss = 0.21990512\n",
      "Iteration 2332, loss = 0.21979937\n",
      "Iteration 2333, loss = 0.21969370\n",
      "Iteration 2334, loss = 0.21958809\n",
      "Iteration 2335, loss = 0.21948255\n",
      "Iteration 2336, loss = 0.21937708\n",
      "Iteration 2337, loss = 0.21927168\n",
      "Iteration 2338, loss = 0.21916634\n",
      "Iteration 2339, loss = 0.21906108\n",
      "Iteration 2340, loss = 0.21895588\n",
      "Iteration 2341, loss = 0.21885075\n",
      "Iteration 2342, loss = 0.21874569\n",
      "Iteration 2343, loss = 0.21864070\n",
      "Iteration 2344, loss = 0.21853578\n",
      "Iteration 2345, loss = 0.21843092\n",
      "Iteration 2346, loss = 0.21832614\n",
      "Iteration 2347, loss = 0.21822142\n",
      "Iteration 2348, loss = 0.21811677\n",
      "Iteration 2349, loss = 0.21801219\n",
      "Iteration 2350, loss = 0.21790768\n",
      "Iteration 2351, loss = 0.21780323\n",
      "Iteration 2352, loss = 0.21769886\n",
      "Iteration 2353, loss = 0.21759455\n",
      "Iteration 2354, loss = 0.21749031\n",
      "Iteration 2355, loss = 0.21738614\n",
      "Iteration 2356, loss = 0.21728203\n",
      "Iteration 2357, loss = 0.21717799\n",
      "Iteration 2358, loss = 0.21707403\n",
      "Iteration 2359, loss = 0.21697013\n",
      "Iteration 2360, loss = 0.21686629\n",
      "Iteration 2361, loss = 0.21676253\n",
      "Iteration 2362, loss = 0.21665883\n",
      "Iteration 2363, loss = 0.21655520\n",
      "Iteration 2364, loss = 0.21645164\n",
      "Iteration 2365, loss = 0.21634815\n",
      "Iteration 2366, loss = 0.21624472\n",
      "Iteration 2367, loss = 0.21614137\n",
      "Iteration 2368, loss = 0.21603807\n",
      "Iteration 2369, loss = 0.21593485\n",
      "Iteration 2370, loss = 0.21583170\n",
      "Iteration 2371, loss = 0.21572861\n",
      "Iteration 2372, loss = 0.21562559\n",
      "Iteration 2373, loss = 0.21552264\n",
      "Iteration 2374, loss = 0.21541975\n",
      "Iteration 2375, loss = 0.21531694\n",
      "Iteration 2376, loss = 0.21521419\n",
      "Iteration 2377, loss = 0.21511150\n",
      "Iteration 2378, loss = 0.21500889\n",
      "Iteration 2379, loss = 0.21490634\n",
      "Iteration 2380, loss = 0.21480386\n",
      "Iteration 2381, loss = 0.21470145\n",
      "Iteration 2382, loss = 0.21459910\n",
      "Iteration 2383, loss = 0.21449682\n",
      "Iteration 2384, loss = 0.21439461\n",
      "Iteration 2385, loss = 0.21429247\n",
      "Iteration 2386, loss = 0.21419039\n",
      "Iteration 2387, loss = 0.21408838\n",
      "Iteration 2388, loss = 0.21398644\n",
      "Iteration 2389, loss = 0.21388456\n",
      "Iteration 2390, loss = 0.21378275\n",
      "Iteration 2391, loss = 0.21368101\n",
      "Iteration 2392, loss = 0.21357934\n",
      "Iteration 2393, loss = 0.21347773\n",
      "Iteration 2394, loss = 0.21337619\n",
      "Iteration 2395, loss = 0.21327472\n",
      "Iteration 2396, loss = 0.21317331\n",
      "Iteration 2397, loss = 0.21307197\n",
      "Iteration 2398, loss = 0.21297070\n",
      "Iteration 2399, loss = 0.21286949\n",
      "Iteration 2400, loss = 0.21276835\n",
      "Iteration 2401, loss = 0.21266728\n",
      "Iteration 2402, loss = 0.21256628\n",
      "Iteration 2403, loss = 0.21246534\n",
      "Iteration 2404, loss = 0.21236446\n",
      "Iteration 2405, loss = 0.21226366\n",
      "Iteration 2406, loss = 0.21216292\n",
      "Iteration 2407, loss = 0.21206225\n",
      "Iteration 2408, loss = 0.21196164\n",
      "Iteration 2409, loss = 0.21186110\n",
      "Iteration 2410, loss = 0.21176063\n",
      "Iteration 2411, loss = 0.21166023\n",
      "Iteration 2412, loss = 0.21155989\n",
      "Iteration 2413, loss = 0.21145962\n",
      "Iteration 2414, loss = 0.21135941\n",
      "Iteration 2415, loss = 0.21125927\n",
      "Iteration 2416, loss = 0.21115920\n",
      "Iteration 2417, loss = 0.21105919\n",
      "Iteration 2418, loss = 0.21095925\n",
      "Iteration 2419, loss = 0.21085938\n",
      "Iteration 2420, loss = 0.21075957\n",
      "Iteration 2421, loss = 0.21065983\n",
      "Iteration 2422, loss = 0.21056015\n",
      "Iteration 2423, loss = 0.21046054\n",
      "Iteration 2424, loss = 0.21036100\n",
      "Iteration 2425, loss = 0.21026152\n",
      "Iteration 2426, loss = 0.21016211\n",
      "Iteration 2427, loss = 0.21006277\n",
      "Iteration 2428, loss = 0.20996349\n",
      "Iteration 2429, loss = 0.20986428\n",
      "Iteration 2430, loss = 0.20976514\n",
      "Iteration 2431, loss = 0.20966606\n",
      "Iteration 2432, loss = 0.20956704\n",
      "Iteration 2433, loss = 0.20946810\n",
      "Iteration 2434, loss = 0.20936922\n",
      "Iteration 2435, loss = 0.20927040\n",
      "Iteration 2436, loss = 0.20917165\n",
      "Iteration 2437, loss = 0.20907297\n",
      "Iteration 2438, loss = 0.20897435\n",
      "Iteration 2439, loss = 0.20887580\n",
      "Iteration 2440, loss = 0.20877732\n",
      "Iteration 2441, loss = 0.20867890\n",
      "Iteration 2442, loss = 0.20858054\n",
      "Iteration 2443, loss = 0.20848226\n",
      "Iteration 2444, loss = 0.20838404\n",
      "Iteration 2445, loss = 0.20828588\n",
      "Iteration 2446, loss = 0.20818779\n",
      "Iteration 2447, loss = 0.20808977\n",
      "Iteration 2448, loss = 0.20799181\n",
      "Iteration 2449, loss = 0.20789391\n",
      "Iteration 2450, loss = 0.20779609\n",
      "Iteration 2451, loss = 0.20769833\n",
      "Iteration 2452, loss = 0.20760063\n",
      "Iteration 2453, loss = 0.20750300\n",
      "Iteration 2454, loss = 0.20740544\n",
      "Iteration 2455, loss = 0.20730794\n",
      "Iteration 2456, loss = 0.20721050\n",
      "Iteration 2457, loss = 0.20711314\n",
      "Iteration 2458, loss = 0.20701583\n",
      "Iteration 2459, loss = 0.20691860\n",
      "Iteration 2460, loss = 0.20682143\n",
      "Iteration 2461, loss = 0.20672432\n",
      "Iteration 2462, loss = 0.20662728\n",
      "Iteration 2463, loss = 0.20653031\n",
      "Iteration 2464, loss = 0.20643340\n",
      "Iteration 2465, loss = 0.20633655\n",
      "Iteration 2466, loss = 0.20623978\n",
      "Iteration 2467, loss = 0.20614306\n",
      "Iteration 2468, loss = 0.20604642\n",
      "Iteration 2469, loss = 0.20594983\n",
      "Iteration 2470, loss = 0.20585332\n",
      "Iteration 2471, loss = 0.20575687\n",
      "Iteration 2472, loss = 0.20566048\n",
      "Iteration 2473, loss = 0.20556416\n",
      "Iteration 2474, loss = 0.20546790\n",
      "Iteration 2475, loss = 0.20537171\n",
      "Iteration 2476, loss = 0.20527559\n",
      "Iteration 2477, loss = 0.20517953\n",
      "Iteration 2478, loss = 0.20508353\n",
      "Iteration 2479, loss = 0.20498760\n",
      "Iteration 2480, loss = 0.20489174\n",
      "Iteration 2481, loss = 0.20479594\n",
      "Iteration 2482, loss = 0.20470020\n",
      "Iteration 2483, loss = 0.20460453\n",
      "Iteration 2484, loss = 0.20450893\n",
      "Iteration 2485, loss = 0.20441339\n",
      "Iteration 2486, loss = 0.20431792\n",
      "Iteration 2487, loss = 0.20422251\n",
      "Iteration 2488, loss = 0.20412717\n",
      "Iteration 2489, loss = 0.20403190\n",
      "Iteration 2490, loss = 0.20393670\n",
      "Iteration 2491, loss = 0.20384158\n",
      "Iteration 2492, loss = 0.20374655\n",
      "Iteration 2493, loss = 0.20365162\n",
      "Iteration 2494, loss = 0.20355684\n",
      "Iteration 2495, loss = 0.20346212\n",
      "Iteration 2496, loss = 0.20336745\n",
      "Iteration 2497, loss = 0.20327252\n",
      "Iteration 2498, loss = 0.20317753\n",
      "Iteration 2499, loss = 0.20308272\n",
      "Iteration 2500, loss = 0.20298829\n",
      "Iteration 2501, loss = 0.20289401\n",
      "Iteration 2502, loss = 0.20279954\n",
      "Iteration 2503, loss = 0.20270499\n",
      "Iteration 2504, loss = 0.20261063\n",
      "Iteration 2505, loss = 0.20251653\n",
      "Iteration 2506, loss = 0.20242245\n",
      "Iteration 2507, loss = 0.20232825\n",
      "Iteration 2508, loss = 0.20223413\n",
      "Iteration 2509, loss = 0.20214022\n",
      "Iteration 2510, loss = 0.20204639\n",
      "Iteration 2511, loss = 0.20195252\n",
      "Iteration 2512, loss = 0.20185868\n",
      "Iteration 2513, loss = 0.20176499\n",
      "Iteration 2514, loss = 0.20167141\n",
      "Iteration 2515, loss = 0.20157782\n",
      "Iteration 2516, loss = 0.20148426\n",
      "Iteration 2517, loss = 0.20139082\n",
      "Iteration 2518, loss = 0.20129747\n",
      "Iteration 2519, loss = 0.20120415\n",
      "Iteration 2520, loss = 0.20111087\n",
      "Iteration 2521, loss = 0.20101768\n",
      "Iteration 2522, loss = 0.20092458\n",
      "Iteration 2523, loss = 0.20083152\n",
      "Iteration 2524, loss = 0.20073851\n",
      "Iteration 2525, loss = 0.20064557\n",
      "Iteration 2526, loss = 0.20055272\n",
      "Iteration 2527, loss = 0.20045993\n",
      "Iteration 2528, loss = 0.20036717\n",
      "Iteration 2529, loss = 0.20027450\n",
      "Iteration 2530, loss = 0.20018190\n",
      "Iteration 2531, loss = 0.20008936\n",
      "Iteration 2532, loss = 0.19999687\n",
      "Iteration 2533, loss = 0.19990445\n",
      "Iteration 2534, loss = 0.19981210\n",
      "Iteration 2535, loss = 0.19971982\n",
      "Iteration 2536, loss = 0.19962759\n",
      "Iteration 2537, loss = 0.19953543\n",
      "Iteration 2538, loss = 0.19944334\n",
      "Iteration 2539, loss = 0.19935131\n",
      "Iteration 2540, loss = 0.19925934\n",
      "Iteration 2541, loss = 0.19916743\n",
      "Iteration 2542, loss = 0.19907559\n",
      "Iteration 2543, loss = 0.19898382\n",
      "Iteration 2544, loss = 0.19889211\n",
      "Iteration 2545, loss = 0.19880046\n",
      "Iteration 2546, loss = 0.19870888\n",
      "Iteration 2547, loss = 0.19861736\n",
      "Iteration 2548, loss = 0.19852590\n",
      "Iteration 2549, loss = 0.19843451\n",
      "Iteration 2550, loss = 0.19834318\n",
      "Iteration 2551, loss = 0.19825192\n",
      "Iteration 2552, loss = 0.19816072\n",
      "Iteration 2553, loss = 0.19806958\n",
      "Iteration 2554, loss = 0.19797850\n",
      "Iteration 2555, loss = 0.19788749\n",
      "Iteration 2556, loss = 0.19779655\n",
      "Iteration 2557, loss = 0.19770567\n",
      "Iteration 2558, loss = 0.19761485\n",
      "Iteration 2559, loss = 0.19752409\n",
      "Iteration 2560, loss = 0.19743340\n",
      "Iteration 2561, loss = 0.19734277\n",
      "Iteration 2562, loss = 0.19725221\n",
      "Iteration 2563, loss = 0.19716171\n",
      "Iteration 2564, loss = 0.19707127\n",
      "Iteration 2565, loss = 0.19698090\n",
      "Iteration 2566, loss = 0.19689059\n",
      "Iteration 2567, loss = 0.19680034\n",
      "Iteration 2568, loss = 0.19671015\n",
      "Iteration 2569, loss = 0.19662003\n",
      "Iteration 2570, loss = 0.19652998\n",
      "Iteration 2571, loss = 0.19643999\n",
      "Iteration 2572, loss = 0.19635006\n",
      "Iteration 2573, loss = 0.19626019\n",
      "Iteration 2574, loss = 0.19617039\n",
      "Iteration 2575, loss = 0.19608065\n",
      "Iteration 2576, loss = 0.19599097\n",
      "Iteration 2577, loss = 0.19590136\n",
      "Iteration 2578, loss = 0.19581181\n",
      "Iteration 2579, loss = 0.19572232\n",
      "Iteration 2580, loss = 0.19563290\n",
      "Iteration 2581, loss = 0.19554354\n",
      "Iteration 2582, loss = 0.19545424\n",
      "Iteration 2583, loss = 0.19536501\n",
      "Iteration 2584, loss = 0.19527584\n",
      "Iteration 2585, loss = 0.19518673\n",
      "Iteration 2586, loss = 0.19509768\n",
      "Iteration 2587, loss = 0.19500870\n",
      "Iteration 2588, loss = 0.19491978\n",
      "Iteration 2589, loss = 0.19483093\n",
      "Iteration 2590, loss = 0.19474214\n",
      "Iteration 2591, loss = 0.19465341\n",
      "Iteration 2592, loss = 0.19456474\n",
      "Iteration 2593, loss = 0.19447614\n",
      "Iteration 2594, loss = 0.19438760\n",
      "Iteration 2595, loss = 0.19429912\n",
      "Iteration 2596, loss = 0.19421071\n",
      "Iteration 2597, loss = 0.19412236\n",
      "Iteration 2598, loss = 0.19403407\n",
      "Iteration 2599, loss = 0.19394584\n",
      "Iteration 2600, loss = 0.19385768\n",
      "Iteration 2601, loss = 0.19376958\n",
      "Iteration 2602, loss = 0.19368154\n",
      "Iteration 2603, loss = 0.19359357\n",
      "Iteration 2604, loss = 0.19350566\n",
      "Iteration 2605, loss = 0.19341781\n",
      "Iteration 2606, loss = 0.19333002\n",
      "Iteration 2607, loss = 0.19324230\n",
      "Iteration 2608, loss = 0.19315464\n",
      "Iteration 2609, loss = 0.19306704\n",
      "Iteration 2610, loss = 0.19297950\n",
      "Iteration 2611, loss = 0.19289203\n",
      "Iteration 2612, loss = 0.19280462\n",
      "Iteration 2613, loss = 0.19271727\n",
      "Iteration 2614, loss = 0.19262999\n",
      "Iteration 2615, loss = 0.19254277\n",
      "Iteration 2616, loss = 0.19245561\n",
      "Iteration 2617, loss = 0.19236851\n",
      "Iteration 2618, loss = 0.19228148\n",
      "Iteration 2619, loss = 0.19219450\n",
      "Iteration 2620, loss = 0.19210759\n",
      "Iteration 2621, loss = 0.19202075\n",
      "Iteration 2622, loss = 0.19193396\n",
      "Iteration 2623, loss = 0.19184724\n",
      "Iteration 2624, loss = 0.19176058\n",
      "Iteration 2625, loss = 0.19167398\n",
      "Iteration 2626, loss = 0.19158744\n",
      "Iteration 2627, loss = 0.19150097\n",
      "Iteration 2628, loss = 0.19141456\n",
      "Iteration 2629, loss = 0.19132821\n",
      "Iteration 2630, loss = 0.19124193\n",
      "Iteration 2631, loss = 0.19115570\n",
      "Iteration 2632, loss = 0.19106954\n",
      "Iteration 2633, loss = 0.19098344\n",
      "Iteration 2634, loss = 0.19089740\n",
      "Iteration 2635, loss = 0.19081143\n",
      "Iteration 2636, loss = 0.19072551\n",
      "Iteration 2637, loss = 0.19063966\n",
      "Iteration 2638, loss = 0.19055387\n",
      "Iteration 2639, loss = 0.19046815\n",
      "Iteration 2640, loss = 0.19038248\n",
      "Iteration 2641, loss = 0.19029688\n",
      "Iteration 2642, loss = 0.19021134\n",
      "Iteration 2643, loss = 0.19012586\n",
      "Iteration 2644, loss = 0.19004044\n",
      "Iteration 2645, loss = 0.18995509\n",
      "Iteration 2646, loss = 0.18986979\n",
      "Iteration 2647, loss = 0.18978456\n",
      "Iteration 2648, loss = 0.18969939\n",
      "Iteration 2649, loss = 0.18961428\n",
      "Iteration 2650, loss = 0.18952924\n",
      "Iteration 2651, loss = 0.18944425\n",
      "Iteration 2652, loss = 0.18935933\n",
      "Iteration 2653, loss = 0.18927447\n",
      "Iteration 2654, loss = 0.18918967\n",
      "Iteration 2655, loss = 0.18910494\n",
      "Iteration 2656, loss = 0.18902026\n",
      "Iteration 2657, loss = 0.18893565\n",
      "Iteration 2658, loss = 0.18885110\n",
      "Iteration 2659, loss = 0.18876660\n",
      "Iteration 2660, loss = 0.18868218\n",
      "Iteration 2661, loss = 0.18859781\n",
      "Iteration 2662, loss = 0.18851350\n",
      "Iteration 2663, loss = 0.18842926\n",
      "Iteration 2664, loss = 0.18834508\n",
      "Iteration 2665, loss = 0.18826096\n",
      "Iteration 2666, loss = 0.18817690\n",
      "Iteration 2667, loss = 0.18809290\n",
      "Iteration 2668, loss = 0.18800896\n",
      "Iteration 2669, loss = 0.18792509\n",
      "Iteration 2670, loss = 0.18784127\n",
      "Iteration 2671, loss = 0.18775752\n",
      "Iteration 2672, loss = 0.18767383\n",
      "Iteration 2673, loss = 0.18759020\n",
      "Iteration 2674, loss = 0.18750663\n",
      "Iteration 2675, loss = 0.18742312\n",
      "Iteration 2676, loss = 0.18733968\n",
      "Iteration 2677, loss = 0.18725629\n",
      "Iteration 2678, loss = 0.18717297\n",
      "Iteration 2679, loss = 0.18708971\n",
      "Iteration 2680, loss = 0.18700651\n",
      "Iteration 2681, loss = 0.18692337\n",
      "Iteration 2682, loss = 0.18684029\n",
      "Iteration 2683, loss = 0.18675727\n",
      "Iteration 2684, loss = 0.18667431\n",
      "Iteration 2685, loss = 0.18659142\n",
      "Iteration 2686, loss = 0.18650858\n",
      "Iteration 2687, loss = 0.18642581\n",
      "Iteration 2688, loss = 0.18634309\n",
      "Iteration 2689, loss = 0.18626044\n",
      "Iteration 2690, loss = 0.18617785\n",
      "Iteration 2691, loss = 0.18609532\n",
      "Iteration 2692, loss = 0.18601285\n",
      "Iteration 2693, loss = 0.18593044\n",
      "Iteration 2694, loss = 0.18584810\n",
      "Iteration 2695, loss = 0.18576581\n",
      "Iteration 2696, loss = 0.18568358\n",
      "Iteration 2697, loss = 0.18560142\n",
      "Iteration 2698, loss = 0.18551931\n",
      "Iteration 2699, loss = 0.18543727\n",
      "Iteration 2700, loss = 0.18535529\n",
      "Iteration 2701, loss = 0.18527336\n",
      "Iteration 2702, loss = 0.18519150\n",
      "Iteration 2703, loss = 0.18510970\n",
      "Iteration 2704, loss = 0.18502796\n",
      "Iteration 2705, loss = 0.18494628\n",
      "Iteration 2706, loss = 0.18486466\n",
      "Iteration 2707, loss = 0.18478310\n",
      "Iteration 2708, loss = 0.18470160\n",
      "Iteration 2709, loss = 0.18462016\n",
      "Iteration 2710, loss = 0.18453879\n",
      "Iteration 2711, loss = 0.18445747\n",
      "Iteration 2712, loss = 0.18437621\n",
      "Iteration 2713, loss = 0.18429502\n",
      "Iteration 2714, loss = 0.18421388\n",
      "Iteration 2715, loss = 0.18413280\n",
      "Iteration 2716, loss = 0.18405179\n",
      "Iteration 2717, loss = 0.18397083\n",
      "Iteration 2718, loss = 0.18388994\n",
      "Iteration 2719, loss = 0.18380910\n",
      "Iteration 2720, loss = 0.18372833\n",
      "Iteration 2721, loss = 0.18364761\n",
      "Iteration 2722, loss = 0.18356696\n",
      "Iteration 2723, loss = 0.18348636\n",
      "Iteration 2724, loss = 0.18340583\n",
      "Iteration 2725, loss = 0.18332535\n",
      "Iteration 2726, loss = 0.18324494\n",
      "Iteration 2727, loss = 0.18316459\n",
      "Iteration 2728, loss = 0.18308429\n",
      "Iteration 2729, loss = 0.18300406\n",
      "Iteration 2730, loss = 0.18292388\n",
      "Iteration 2731, loss = 0.18284377\n",
      "Iteration 2732, loss = 0.18276372\n",
      "Iteration 2733, loss = 0.18268372\n",
      "Iteration 2734, loss = 0.18260379\n",
      "Iteration 2735, loss = 0.18252392\n",
      "Iteration 2736, loss = 0.18244412\n",
      "Iteration 2737, loss = 0.18236439\n",
      "Iteration 2738, loss = 0.18228475\n",
      "Iteration 2739, loss = 0.18220521\n",
      "Iteration 2740, loss = 0.18212582\n",
      "Iteration 2741, loss = 0.18204651\n",
      "Iteration 2742, loss = 0.18196729\n",
      "Iteration 2743, loss = 0.18188779\n",
      "Iteration 2744, loss = 0.18180811\n",
      "Iteration 2745, loss = 0.18172852\n",
      "Iteration 2746, loss = 0.18164936\n",
      "Iteration 2747, loss = 0.18157048\n",
      "Iteration 2748, loss = 0.18149143\n",
      "Iteration 2749, loss = 0.18141219\n",
      "Iteration 2750, loss = 0.18133307\n",
      "Iteration 2751, loss = 0.18125427\n",
      "Iteration 2752, loss = 0.18117557\n",
      "Iteration 2753, loss = 0.18109669\n",
      "Iteration 2754, loss = 0.18101782\n",
      "Iteration 2755, loss = 0.18093918\n",
      "Iteration 2756, loss = 0.18086068\n",
      "Iteration 2757, loss = 0.18078211\n",
      "Iteration 2758, loss = 0.18070352\n",
      "Iteration 2759, loss = 0.18062509\n",
      "Iteration 2760, loss = 0.18054680\n",
      "Iteration 2761, loss = 0.18046847\n",
      "Iteration 2762, loss = 0.18039016\n",
      "Iteration 2763, loss = 0.18031196\n",
      "Iteration 2764, loss = 0.18023388\n",
      "Iteration 2765, loss = 0.18015580\n",
      "Iteration 2766, loss = 0.18007774\n",
      "Iteration 2767, loss = 0.17999978\n",
      "Iteration 2768, loss = 0.17992192\n",
      "Iteration 2769, loss = 0.17984408\n",
      "Iteration 2770, loss = 0.17976627\n",
      "Iteration 2771, loss = 0.17968854\n",
      "Iteration 2772, loss = 0.17961090\n",
      "Iteration 2773, loss = 0.17953330\n",
      "Iteration 2774, loss = 0.17945574\n",
      "Iteration 2775, loss = 0.17937825\n",
      "Iteration 2776, loss = 0.17930083\n",
      "Iteration 2777, loss = 0.17922347\n",
      "Iteration 2778, loss = 0.17914615\n",
      "Iteration 2779, loss = 0.17906889\n",
      "Iteration 2780, loss = 0.17899170\n",
      "Iteration 2781, loss = 0.17891457\n",
      "Iteration 2782, loss = 0.17883749\n",
      "Iteration 2783, loss = 0.17876047\n",
      "Iteration 2784, loss = 0.17868351\n",
      "Iteration 2785, loss = 0.17860661\n",
      "Iteration 2786, loss = 0.17852977\n",
      "Iteration 2787, loss = 0.17845298\n",
      "Iteration 2788, loss = 0.17837625\n",
      "Iteration 2789, loss = 0.17829959\n",
      "Iteration 2790, loss = 0.17822298\n",
      "Iteration 2791, loss = 0.17814642\n",
      "Iteration 2792, loss = 0.17806993\n",
      "Iteration 2793, loss = 0.17799349\n",
      "Iteration 2794, loss = 0.17791712\n",
      "Iteration 2795, loss = 0.17784079\n",
      "Iteration 2796, loss = 0.17776453\n",
      "Iteration 2797, loss = 0.17768833\n",
      "Iteration 2798, loss = 0.17761218\n",
      "Iteration 2799, loss = 0.17753609\n",
      "Iteration 2800, loss = 0.17746006\n",
      "Iteration 2801, loss = 0.17738409\n",
      "Iteration 2802, loss = 0.17730818\n",
      "Iteration 2803, loss = 0.17723232\n",
      "Iteration 2804, loss = 0.17715652\n",
      "Iteration 2805, loss = 0.17708078\n",
      "Iteration 2806, loss = 0.17700510\n",
      "Iteration 2807, loss = 0.17692947\n",
      "Iteration 2808, loss = 0.17685390\n",
      "Iteration 2809, loss = 0.17677839\n",
      "Iteration 2810, loss = 0.17670294\n",
      "Iteration 2811, loss = 0.17662754\n",
      "Iteration 2812, loss = 0.17655220\n",
      "Iteration 2813, loss = 0.17647692\n",
      "Iteration 2814, loss = 0.17640170\n",
      "Iteration 2815, loss = 0.17632653\n",
      "Iteration 2816, loss = 0.17625142\n",
      "Iteration 2817, loss = 0.17617637\n",
      "Iteration 2818, loss = 0.17610138\n",
      "Iteration 2819, loss = 0.17602644\n",
      "Iteration 2820, loss = 0.17595156\n",
      "Iteration 2821, loss = 0.17587674\n",
      "Iteration 2822, loss = 0.17580197\n",
      "Iteration 2823, loss = 0.17572726\n",
      "Iteration 2824, loss = 0.17565261\n",
      "Iteration 2825, loss = 0.17557802\n",
      "Iteration 2826, loss = 0.17550348\n",
      "Iteration 2827, loss = 0.17542900\n",
      "Iteration 2828, loss = 0.17535458\n",
      "Iteration 2829, loss = 0.17528021\n",
      "Iteration 2830, loss = 0.17520590\n",
      "Iteration 2831, loss = 0.17513165\n",
      "Iteration 2832, loss = 0.17505746\n",
      "Iteration 2833, loss = 0.17498332\n",
      "Iteration 2834, loss = 0.17490924\n",
      "Iteration 2835, loss = 0.17483521\n",
      "Iteration 2836, loss = 0.17476124\n",
      "Iteration 2837, loss = 0.17468733\n",
      "Iteration 2838, loss = 0.17461348\n",
      "Iteration 2839, loss = 0.17453968\n",
      "Iteration 2840, loss = 0.17446594\n",
      "Iteration 2841, loss = 0.17439225\n",
      "Iteration 2842, loss = 0.17431862\n",
      "Iteration 2843, loss = 0.17424505\n",
      "Iteration 2844, loss = 0.17417154\n",
      "Iteration 2845, loss = 0.17409808\n",
      "Iteration 2846, loss = 0.17402467\n",
      "Iteration 2847, loss = 0.17395133\n",
      "Iteration 2848, loss = 0.17387804\n",
      "Iteration 2849, loss = 0.17380481\n",
      "Iteration 2850, loss = 0.17373163\n",
      "Iteration 2851, loss = 0.17365851\n",
      "Iteration 2852, loss = 0.17358544\n",
      "Iteration 2853, loss = 0.17351243\n",
      "Iteration 2854, loss = 0.17343948\n",
      "Iteration 2855, loss = 0.17336659\n",
      "Iteration 2856, loss = 0.17329375\n",
      "Iteration 2857, loss = 0.17322096\n",
      "Iteration 2858, loss = 0.17314823\n",
      "Iteration 2859, loss = 0.17307556\n",
      "Iteration 2860, loss = 0.17300295\n",
      "Iteration 2861, loss = 0.17293039\n",
      "Iteration 2862, loss = 0.17285788\n",
      "Iteration 2863, loss = 0.17278543\n",
      "Iteration 2864, loss = 0.17271304\n",
      "Iteration 2865, loss = 0.17264070\n",
      "Iteration 2866, loss = 0.17256842\n",
      "Iteration 2867, loss = 0.17249620\n",
      "Iteration 2868, loss = 0.17242403\n",
      "Iteration 2869, loss = 0.17235191\n",
      "Iteration 2870, loss = 0.17227986\n",
      "Iteration 2871, loss = 0.17220785\n",
      "Iteration 2872, loss = 0.17213591\n",
      "Iteration 2873, loss = 0.17206401\n",
      "Iteration 2874, loss = 0.17199218\n",
      "Iteration 2875, loss = 0.17192040\n",
      "Iteration 2876, loss = 0.17184867\n",
      "Iteration 2877, loss = 0.17177700\n",
      "Iteration 2878, loss = 0.17170539\n",
      "Iteration 2879, loss = 0.17163383\n",
      "Iteration 2880, loss = 0.17156233\n",
      "Iteration 2881, loss = 0.17149088\n",
      "Iteration 2882, loss = 0.17141948\n",
      "Iteration 2883, loss = 0.17134815\n",
      "Iteration 2884, loss = 0.17127686\n",
      "Iteration 2885, loss = 0.17120563\n",
      "Iteration 2886, loss = 0.17113446\n",
      "Iteration 2887, loss = 0.17106334\n",
      "Iteration 2888, loss = 0.17099228\n",
      "Iteration 2889, loss = 0.17092127\n",
      "Iteration 2890, loss = 0.17085032\n",
      "Iteration 2891, loss = 0.17077942\n",
      "Iteration 2892, loss = 0.17070858\n",
      "Iteration 2893, loss = 0.17063779\n",
      "Iteration 2894, loss = 0.17056706\n",
      "Iteration 2895, loss = 0.17049638\n",
      "Iteration 2896, loss = 0.17042576\n",
      "Iteration 2897, loss = 0.17035519\n",
      "Iteration 2898, loss = 0.17028468\n",
      "Iteration 2899, loss = 0.17021422\n",
      "Iteration 2900, loss = 0.17014381\n",
      "Iteration 2901, loss = 0.17007346\n",
      "Iteration 2902, loss = 0.17000316\n",
      "Iteration 2903, loss = 0.16993292\n",
      "Iteration 2904, loss = 0.16986274\n",
      "Iteration 2905, loss = 0.16979260\n",
      "Iteration 2906, loss = 0.16972253\n",
      "Iteration 2907, loss = 0.16965250\n",
      "Iteration 2908, loss = 0.16958253\n",
      "Iteration 2909, loss = 0.16951262\n",
      "Iteration 2910, loss = 0.16944276\n",
      "Iteration 2911, loss = 0.16937295\n",
      "Iteration 2912, loss = 0.16930320\n",
      "Iteration 2913, loss = 0.16923350\n",
      "Iteration 2914, loss = 0.16916386\n",
      "Iteration 2915, loss = 0.16909427\n",
      "Iteration 2916, loss = 0.16902473\n",
      "Iteration 2917, loss = 0.16895525\n",
      "Iteration 2918, loss = 0.16888582\n",
      "Iteration 2919, loss = 0.16881645\n",
      "Iteration 2920, loss = 0.16874713\n",
      "Iteration 2921, loss = 0.16867787\n",
      "Iteration 2922, loss = 0.16860865\n",
      "Iteration 2923, loss = 0.16853950\n",
      "Iteration 2924, loss = 0.16847039\n",
      "Iteration 2925, loss = 0.16840134\n",
      "Iteration 2926, loss = 0.16833234\n",
      "Iteration 2927, loss = 0.16826340\n",
      "Iteration 2928, loss = 0.16819451\n",
      "Iteration 2929, loss = 0.16812568\n",
      "Iteration 2930, loss = 0.16805689\n",
      "Iteration 2931, loss = 0.16798816\n",
      "Iteration 2932, loss = 0.16791949\n",
      "Iteration 2933, loss = 0.16785087\n",
      "Iteration 2934, loss = 0.16778230\n",
      "Iteration 2935, loss = 0.16771378\n",
      "Iteration 2936, loss = 0.16764532\n",
      "Iteration 2937, loss = 0.16757691\n",
      "Iteration 2938, loss = 0.16750856\n",
      "Iteration 2939, loss = 0.16744026\n",
      "Iteration 2940, loss = 0.16737201\n",
      "Iteration 2941, loss = 0.16730381\n",
      "Iteration 2942, loss = 0.16723567\n",
      "Iteration 2943, loss = 0.16716758\n",
      "Iteration 2944, loss = 0.16709955\n",
      "Iteration 2945, loss = 0.16703156\n",
      "Iteration 2946, loss = 0.16696363\n",
      "Iteration 2947, loss = 0.16689576\n",
      "Iteration 2948, loss = 0.16682793\n",
      "Iteration 2949, loss = 0.16676016\n",
      "Iteration 2950, loss = 0.16669245\n",
      "Iteration 2951, loss = 0.16662478\n",
      "Iteration 2952, loss = 0.16655717\n",
      "Iteration 2953, loss = 0.16648961\n",
      "Iteration 2954, loss = 0.16642210\n",
      "Iteration 2955, loss = 0.16635465\n",
      "Iteration 2956, loss = 0.16628725\n",
      "Iteration 2957, loss = 0.16621990\n",
      "Iteration 2958, loss = 0.16615260\n",
      "Iteration 2959, loss = 0.16608536\n",
      "Iteration 2960, loss = 0.16601816\n",
      "Iteration 2961, loss = 0.16595103\n",
      "Iteration 2962, loss = 0.16588394\n",
      "Iteration 2963, loss = 0.16581691\n",
      "Iteration 2964, loss = 0.16574992\n",
      "Iteration 2965, loss = 0.16568299\n",
      "Iteration 2966, loss = 0.16561612\n",
      "Iteration 2967, loss = 0.16554929\n",
      "Iteration 2968, loss = 0.16548252\n",
      "Iteration 2969, loss = 0.16541580\n",
      "Iteration 2970, loss = 0.16534913\n",
      "Iteration 2971, loss = 0.16528252\n",
      "Iteration 2972, loss = 0.16521595\n",
      "Iteration 2973, loss = 0.16514944\n",
      "Iteration 2974, loss = 0.16508298\n",
      "Iteration 2975, loss = 0.16501657\n",
      "Iteration 2976, loss = 0.16495022\n",
      "Iteration 2977, loss = 0.16488391\n",
      "Iteration 2978, loss = 0.16481766\n",
      "Iteration 2979, loss = 0.16475146\n",
      "Iteration 2980, loss = 0.16468531\n",
      "Iteration 2981, loss = 0.16461922\n",
      "Iteration 2982, loss = 0.16455317\n",
      "Iteration 2983, loss = 0.16448718\n",
      "Iteration 2984, loss = 0.16442124\n",
      "Iteration 2985, loss = 0.16435535\n",
      "Iteration 2986, loss = 0.16428951\n",
      "Iteration 2987, loss = 0.16422372\n",
      "Iteration 2988, loss = 0.16415799\n",
      "Iteration 2989, loss = 0.16409230\n",
      "Iteration 2990, loss = 0.16402667\n",
      "Iteration 2991, loss = 0.16396109\n",
      "Iteration 2992, loss = 0.16389556\n",
      "Iteration 2993, loss = 0.16383009\n",
      "Iteration 2994, loss = 0.16376466\n",
      "Iteration 2995, loss = 0.16369930\n",
      "Iteration 2996, loss = 0.16363400\n",
      "Iteration 2997, loss = 0.16356876\n",
      "Iteration 2998, loss = 0.16350362\n",
      "Iteration 2999, loss = 0.16343858\n",
      "Iteration 3000, loss = 0.16337365\n",
      "Iteration 3001, loss = 0.16330884\n",
      "Iteration 3002, loss = 0.16324392\n",
      "Iteration 3003, loss = 0.16317880\n",
      "Iteration 3004, loss = 0.16311347\n",
      "Iteration 3005, loss = 0.16304837\n",
      "Iteration 3006, loss = 0.16298370\n",
      "Iteration 3007, loss = 0.16291918\n",
      "Iteration 3008, loss = 0.16285449\n",
      "Iteration 3009, loss = 0.16278959\n",
      "Iteration 3010, loss = 0.16272486\n",
      "Iteration 3011, loss = 0.16266044\n",
      "Iteration 3012, loss = 0.16259606\n",
      "Iteration 3013, loss = 0.16253153\n",
      "Iteration 3014, loss = 0.16246699\n",
      "Iteration 3015, loss = 0.16240266\n",
      "Iteration 3016, loss = 0.16233848\n",
      "Iteration 3017, loss = 0.16227422\n",
      "Iteration 3018, loss = 0.16220993\n",
      "Iteration 3019, loss = 0.16214576\n",
      "Iteration 3020, loss = 0.16208174\n",
      "Iteration 3021, loss = 0.16201772\n",
      "Iteration 3022, loss = 0.16195366\n",
      "Iteration 3023, loss = 0.16188969\n",
      "Iteration 3024, loss = 0.16182584\n",
      "Iteration 3025, loss = 0.16176202\n",
      "Iteration 3026, loss = 0.16169820\n",
      "Iteration 3027, loss = 0.16163443\n",
      "Iteration 3028, loss = 0.16157076\n",
      "Iteration 3029, loss = 0.16150714\n",
      "Iteration 3030, loss = 0.16144353\n",
      "Iteration 3031, loss = 0.16137997\n",
      "Iteration 3032, loss = 0.16131648\n",
      "Iteration 3033, loss = 0.16125306\n",
      "Iteration 3034, loss = 0.16118966\n",
      "Iteration 3035, loss = 0.16112630\n",
      "Iteration 3036, loss = 0.16106301\n",
      "Iteration 3037, loss = 0.16099978\n",
      "Iteration 3038, loss = 0.16093659\n",
      "Iteration 3039, loss = 0.16087344\n",
      "Iteration 3040, loss = 0.16081034\n",
      "Iteration 3041, loss = 0.16074730\n",
      "Iteration 3042, loss = 0.16068431\n",
      "Iteration 3043, loss = 0.16062136\n",
      "Iteration 3044, loss = 0.16055846\n",
      "Iteration 3045, loss = 0.16049562\n",
      "Iteration 3046, loss = 0.16043283\n",
      "Iteration 3047, loss = 0.16037008\n",
      "Iteration 3048, loss = 0.16030737\n",
      "Iteration 3049, loss = 0.16024473\n",
      "Iteration 3050, loss = 0.16018213\n",
      "Iteration 3051, loss = 0.16011958\n",
      "Iteration 3052, loss = 0.16005707\n",
      "Iteration 3053, loss = 0.15999462\n",
      "Iteration 3054, loss = 0.15993222\n",
      "Iteration 3055, loss = 0.15986987\n",
      "Iteration 3056, loss = 0.15980756\n",
      "Iteration 3057, loss = 0.15974530\n",
      "Iteration 3058, loss = 0.15968310\n",
      "Iteration 3059, loss = 0.15962094\n",
      "Iteration 3060, loss = 0.15955883\n",
      "Iteration 3061, loss = 0.15949677\n",
      "Iteration 3062, loss = 0.15943475\n",
      "Iteration 3063, loss = 0.15937279\n",
      "Iteration 3064, loss = 0.15931088\n",
      "Iteration 3065, loss = 0.15924901\n",
      "Iteration 3066, loss = 0.15918719\n",
      "Iteration 3067, loss = 0.15912542\n",
      "Iteration 3068, loss = 0.15906370\n",
      "Iteration 3069, loss = 0.15900203\n",
      "Iteration 3070, loss = 0.15894041\n",
      "Iteration 3071, loss = 0.15887883\n",
      "Iteration 3072, loss = 0.15881731\n",
      "Iteration 3073, loss = 0.15875583\n",
      "Iteration 3074, loss = 0.15869440\n",
      "Iteration 3075, loss = 0.15863302\n",
      "Iteration 3076, loss = 0.15857168\n",
      "Iteration 3077, loss = 0.15851040\n",
      "Iteration 3078, loss = 0.15844916\n",
      "Iteration 3079, loss = 0.15838797\n",
      "Iteration 3080, loss = 0.15832683\n",
      "Iteration 3081, loss = 0.15826574\n",
      "Iteration 3082, loss = 0.15820469\n",
      "Iteration 3083, loss = 0.15814370\n",
      "Iteration 3084, loss = 0.15808275\n",
      "Iteration 3085, loss = 0.15802185\n",
      "Iteration 3086, loss = 0.15796099\n",
      "Iteration 3087, loss = 0.15790019\n",
      "Iteration 3088, loss = 0.15783943\n",
      "Iteration 3089, loss = 0.15777872\n",
      "Iteration 3090, loss = 0.15771806\n",
      "Iteration 3091, loss = 0.15765744\n",
      "Iteration 3092, loss = 0.15759688\n",
      "Iteration 3093, loss = 0.15753636\n",
      "Iteration 3094, loss = 0.15747589\n",
      "Iteration 3095, loss = 0.15741546\n",
      "Iteration 3096, loss = 0.15735509\n",
      "Iteration 3097, loss = 0.15729476\n",
      "Iteration 3098, loss = 0.15723448\n",
      "Iteration 3099, loss = 0.15717424\n",
      "Iteration 3100, loss = 0.15711406\n",
      "Iteration 3101, loss = 0.15705392\n",
      "Iteration 3102, loss = 0.15699383\n",
      "Iteration 3103, loss = 0.15693378\n",
      "Iteration 3104, loss = 0.15687378\n",
      "Iteration 3105, loss = 0.15681383\n",
      "Iteration 3106, loss = 0.15675393\n",
      "Iteration 3107, loss = 0.15669407\n",
      "Iteration 3108, loss = 0.15663426\n",
      "Iteration 3109, loss = 0.15657450\n",
      "Iteration 3110, loss = 0.15651479\n",
      "Iteration 3111, loss = 0.15645512\n",
      "Iteration 3112, loss = 0.15639550\n",
      "Iteration 3113, loss = 0.15633593\n",
      "Iteration 3114, loss = 0.15627640\n",
      "Iteration 3115, loss = 0.15621692\n",
      "Iteration 3116, loss = 0.15615748\n",
      "Iteration 3117, loss = 0.15609810\n",
      "Iteration 3118, loss = 0.15603876\n",
      "Iteration 3119, loss = 0.15597946\n",
      "Iteration 3120, loss = 0.15592022\n",
      "Iteration 3121, loss = 0.15586102\n",
      "Iteration 3122, loss = 0.15580186\n",
      "Iteration 3123, loss = 0.15574276\n",
      "Iteration 3124, loss = 0.15568370\n",
      "Iteration 3125, loss = 0.15562468\n",
      "Iteration 3126, loss = 0.15556572\n",
      "Iteration 3127, loss = 0.15550679\n",
      "Iteration 3128, loss = 0.15544792\n",
      "Iteration 3129, loss = 0.15538909\n",
      "Iteration 3130, loss = 0.15533031\n",
      "Iteration 3131, loss = 0.15527157\n",
      "Iteration 3132, loss = 0.15521288\n",
      "Iteration 3133, loss = 0.15515424\n",
      "Iteration 3134, loss = 0.15509564\n",
      "Iteration 3135, loss = 0.15503709\n",
      "Iteration 3136, loss = 0.15497859\n",
      "Iteration 3137, loss = 0.15492013\n",
      "Iteration 3138, loss = 0.15486171\n",
      "Iteration 3139, loss = 0.15480335\n",
      "Iteration 3140, loss = 0.15474503\n",
      "Iteration 3141, loss = 0.15468675\n",
      "Iteration 3142, loss = 0.15462852\n",
      "Iteration 3143, loss = 0.15457034\n",
      "Iteration 3144, loss = 0.15451220\n",
      "Iteration 3145, loss = 0.15445411\n",
      "Iteration 3146, loss = 0.15439606\n",
      "Iteration 3147, loss = 0.15433806\n",
      "Iteration 3148, loss = 0.15428011\n",
      "Iteration 3149, loss = 0.15422220\n",
      "Iteration 3150, loss = 0.15416433\n",
      "Iteration 3151, loss = 0.15410651\n",
      "Iteration 3152, loss = 0.15404874\n",
      "Iteration 3153, loss = 0.15399101\n",
      "Iteration 3154, loss = 0.15393333\n",
      "Iteration 3155, loss = 0.15387570\n",
      "Iteration 3156, loss = 0.15381810\n",
      "Iteration 3157, loss = 0.15376056\n",
      "Iteration 3158, loss = 0.15370306\n",
      "Iteration 3159, loss = 0.15364560\n",
      "Iteration 3160, loss = 0.15358819\n",
      "Iteration 3161, loss = 0.15353083\n",
      "Iteration 3162, loss = 0.15347351\n",
      "Iteration 3163, loss = 0.15341623\n",
      "Iteration 3164, loss = 0.15335900\n",
      "Iteration 3165, loss = 0.15330182\n",
      "Iteration 3166, loss = 0.15324468\n",
      "Iteration 3167, loss = 0.15318759\n",
      "Iteration 3168, loss = 0.15313054\n",
      "Iteration 3169, loss = 0.15307353\n",
      "Iteration 3170, loss = 0.15301657\n",
      "Iteration 3171, loss = 0.15295966\n",
      "Iteration 3172, loss = 0.15290279\n",
      "Iteration 3173, loss = 0.15284596\n",
      "Iteration 3174, loss = 0.15278918\n",
      "Iteration 3175, loss = 0.15273245\n",
      "Iteration 3176, loss = 0.15267575\n",
      "Iteration 3177, loss = 0.15261911\n",
      "Iteration 3178, loss = 0.15256251\n",
      "Iteration 3179, loss = 0.15250595\n",
      "Iteration 3180, loss = 0.15244944\n",
      "Iteration 3181, loss = 0.15239297\n",
      "Iteration 3182, loss = 0.15233654\n",
      "Iteration 3183, loss = 0.15228016\n",
      "Iteration 3184, loss = 0.15222383\n",
      "Iteration 3185, loss = 0.15216754\n",
      "Iteration 3186, loss = 0.15211129\n",
      "Iteration 3187, loss = 0.15205509\n",
      "Iteration 3188, loss = 0.15199893\n",
      "Iteration 3189, loss = 0.15194282\n",
      "Iteration 3190, loss = 0.15188675\n",
      "Iteration 3191, loss = 0.15183072\n",
      "Iteration 3192, loss = 0.15177474\n",
      "Iteration 3193, loss = 0.15171880\n",
      "Iteration 3194, loss = 0.15166291\n",
      "Iteration 3195, loss = 0.15160706\n",
      "Iteration 3196, loss = 0.15155125\n",
      "Iteration 3197, loss = 0.15149549\n",
      "Iteration 3198, loss = 0.15143977\n",
      "Iteration 3199, loss = 0.15138410\n",
      "Iteration 3200, loss = 0.15132847\n",
      "Iteration 3201, loss = 0.15127288\n",
      "Iteration 3202, loss = 0.15121734\n",
      "Iteration 3203, loss = 0.15116184\n",
      "Iteration 3204, loss = 0.15110639\n",
      "Iteration 3205, loss = 0.15105097\n",
      "Iteration 3206, loss = 0.15099561\n",
      "Iteration 3207, loss = 0.15094028\n",
      "Iteration 3208, loss = 0.15088500\n",
      "Iteration 3209, loss = 0.15082976\n",
      "Iteration 3210, loss = 0.15077457\n",
      "Iteration 3211, loss = 0.15071942\n",
      "Iteration 3212, loss = 0.15066431\n",
      "Iteration 3213, loss = 0.15060925\n",
      "Iteration 3214, loss = 0.15055423\n",
      "Iteration 3215, loss = 0.15049925\n",
      "Iteration 3216, loss = 0.15044432\n",
      "Iteration 3217, loss = 0.15038943\n",
      "Iteration 3218, loss = 0.15033458\n",
      "Iteration 3219, loss = 0.15027978\n",
      "Iteration 3220, loss = 0.15022501\n",
      "Iteration 3221, loss = 0.15017030\n",
      "Iteration 3222, loss = 0.15011562\n",
      "Iteration 3223, loss = 0.15006099\n",
      "Iteration 3224, loss = 0.15000640\n",
      "Iteration 3225, loss = 0.14995185\n",
      "Iteration 3226, loss = 0.14989735\n",
      "Iteration 3227, loss = 0.14984289\n",
      "Iteration 3228, loss = 0.14978847\n",
      "Iteration 3229, loss = 0.14973410\n",
      "Iteration 3230, loss = 0.14967977\n",
      "Iteration 3231, loss = 0.14962548\n",
      "Iteration 3232, loss = 0.14957123\n",
      "Iteration 3233, loss = 0.14951703\n",
      "Iteration 3234, loss = 0.14946287\n",
      "Iteration 3235, loss = 0.14940875\n",
      "Iteration 3236, loss = 0.14935467\n",
      "Iteration 3237, loss = 0.14930064\n",
      "Iteration 3238, loss = 0.14924665\n",
      "Iteration 3239, loss = 0.14919270\n",
      "Iteration 3240, loss = 0.14913879\n",
      "Iteration 3241, loss = 0.14908493\n",
      "Iteration 3242, loss = 0.14903111\n",
      "Iteration 3243, loss = 0.14897733\n",
      "Iteration 3244, loss = 0.14892359\n",
      "Iteration 3245, loss = 0.14886990\n",
      "Iteration 3246, loss = 0.14881625\n",
      "Iteration 3247, loss = 0.14876264\n",
      "Iteration 3248, loss = 0.14870907\n",
      "Iteration 3249, loss = 0.14865554\n",
      "Iteration 3250, loss = 0.14860206\n",
      "Iteration 3251, loss = 0.14854862\n",
      "Iteration 3252, loss = 0.14849522\n",
      "Iteration 3253, loss = 0.14844186\n",
      "Iteration 3254, loss = 0.14838855\n",
      "Iteration 3255, loss = 0.14833528\n",
      "Iteration 3256, loss = 0.14828205\n",
      "Iteration 3257, loss = 0.14822886\n",
      "Iteration 3258, loss = 0.14817572\n",
      "Iteration 3259, loss = 0.14812264\n",
      "Iteration 3260, loss = 0.14806961\n",
      "Iteration 3261, loss = 0.14801666\n",
      "Iteration 3262, loss = 0.14796379\n",
      "Iteration 3263, loss = 0.14791106\n",
      "Iteration 3264, loss = 0.14785840\n",
      "Iteration 3265, loss = 0.14780577\n",
      "Iteration 3266, loss = 0.14775285\n",
      "Iteration 3267, loss = 0.14769969\n",
      "Iteration 3268, loss = 0.14764659\n",
      "Iteration 3269, loss = 0.14759392\n",
      "Iteration 3270, loss = 0.14754155\n",
      "Iteration 3271, loss = 0.14748906\n",
      "Iteration 3272, loss = 0.14743631\n",
      "Iteration 3273, loss = 0.14738356\n",
      "Iteration 3274, loss = 0.14733113\n",
      "Iteration 3275, loss = 0.14727888\n",
      "Iteration 3276, loss = 0.14722648\n",
      "Iteration 3277, loss = 0.14717396\n",
      "Iteration 3278, loss = 0.14712159\n",
      "Iteration 3279, loss = 0.14706942\n",
      "Iteration 3280, loss = 0.14701726\n",
      "Iteration 3281, loss = 0.14696498\n",
      "Iteration 3282, loss = 0.14691276\n",
      "Iteration 3283, loss = 0.14686070\n",
      "Iteration 3284, loss = 0.14680870\n",
      "Iteration 3285, loss = 0.14675664\n",
      "Iteration 3286, loss = 0.14670460\n",
      "Iteration 3287, loss = 0.14665267\n",
      "Iteration 3288, loss = 0.14660082\n",
      "Iteration 3289, loss = 0.14654895\n",
      "Iteration 3290, loss = 0.14649709\n",
      "Iteration 3291, loss = 0.14644531\n",
      "Iteration 3292, loss = 0.14639360\n",
      "Iteration 3293, loss = 0.14634191\n",
      "Iteration 3294, loss = 0.14629023\n",
      "Iteration 3295, loss = 0.14623860\n",
      "Iteration 3296, loss = 0.14618705\n",
      "Iteration 3297, loss = 0.14613552\n",
      "Iteration 3298, loss = 0.14608401\n",
      "Iteration 3299, loss = 0.14603255\n",
      "Iteration 3300, loss = 0.14598115\n",
      "Iteration 3301, loss = 0.14592978\n",
      "Iteration 3302, loss = 0.14587845\n",
      "Iteration 3303, loss = 0.14582714\n",
      "Iteration 3304, loss = 0.14577590\n",
      "Iteration 3305, loss = 0.14572469\n",
      "Iteration 3306, loss = 0.14567352\n",
      "Iteration 3307, loss = 0.14562238\n",
      "Iteration 3308, loss = 0.14557129\n",
      "Iteration 3309, loss = 0.14552025\n",
      "Iteration 3310, loss = 0.14546924\n",
      "Iteration 3311, loss = 0.14541826\n",
      "Iteration 3312, loss = 0.14536733\n",
      "Iteration 3313, loss = 0.14531645\n",
      "Iteration 3314, loss = 0.14526560\n",
      "Iteration 3315, loss = 0.14521479\n",
      "Iteration 3316, loss = 0.14516401\n",
      "Iteration 3317, loss = 0.14511328\n",
      "Iteration 3318, loss = 0.14506259\n",
      "Iteration 3319, loss = 0.14501194\n",
      "Iteration 3320, loss = 0.14496133\n",
      "Iteration 3321, loss = 0.14491076\n",
      "Iteration 3322, loss = 0.14486023\n",
      "Iteration 3323, loss = 0.14480974\n",
      "Iteration 3324, loss = 0.14475928\n",
      "Iteration 3325, loss = 0.14470887\n",
      "Iteration 3326, loss = 0.14465850\n",
      "Iteration 3327, loss = 0.14460816\n",
      "Iteration 3328, loss = 0.14455787\n",
      "Iteration 3329, loss = 0.14450761\n",
      "Iteration 3330, loss = 0.14445740\n",
      "Iteration 3331, loss = 0.14440722\n",
      "Iteration 3332, loss = 0.14435708\n",
      "Iteration 3333, loss = 0.14430699\n",
      "Iteration 3334, loss = 0.14425693\n",
      "Iteration 3335, loss = 0.14420691\n",
      "Iteration 3336, loss = 0.14415693\n",
      "Iteration 3337, loss = 0.14410699\n",
      "Iteration 3338, loss = 0.14405709\n",
      "Iteration 3339, loss = 0.14400722\n",
      "Iteration 3340, loss = 0.14395740\n",
      "Iteration 3341, loss = 0.14390762\n",
      "Iteration 3342, loss = 0.14385787\n",
      "Iteration 3343, loss = 0.14380816\n",
      "Iteration 3344, loss = 0.14375850\n",
      "Iteration 3345, loss = 0.14370887\n",
      "Iteration 3346, loss = 0.14365928\n",
      "Iteration 3347, loss = 0.14360973\n",
      "Iteration 3348, loss = 0.14356021\n",
      "Iteration 3349, loss = 0.14351074\n",
      "Iteration 3350, loss = 0.14346131\n",
      "Iteration 3351, loss = 0.14341191\n",
      "Iteration 3352, loss = 0.14336255\n",
      "Iteration 3353, loss = 0.14331324\n",
      "Iteration 3354, loss = 0.14326396\n",
      "Iteration 3355, loss = 0.14321472\n",
      "Iteration 3356, loss = 0.14316551\n",
      "Iteration 3357, loss = 0.14311635\n",
      "Iteration 3358, loss = 0.14306722\n",
      "Iteration 3359, loss = 0.14301814\n",
      "Iteration 3360, loss = 0.14296909\n",
      "Iteration 3361, loss = 0.14292008\n",
      "Iteration 3362, loss = 0.14287111\n",
      "Iteration 3363, loss = 0.14282217\n",
      "Iteration 3364, loss = 0.14277328\n",
      "Iteration 3365, loss = 0.14272442\n",
      "Iteration 3366, loss = 0.14267560\n",
      "Iteration 3367, loss = 0.14262682\n",
      "Iteration 3368, loss = 0.14257808\n",
      "Iteration 3369, loss = 0.14252938\n",
      "Iteration 3370, loss = 0.14248071\n",
      "Iteration 3371, loss = 0.14243208\n",
      "Iteration 3372, loss = 0.14238349\n",
      "Iteration 3373, loss = 0.14233494\n",
      "Iteration 3374, loss = 0.14228643\n",
      "Iteration 3375, loss = 0.14223795\n",
      "Iteration 3376, loss = 0.14218952\n",
      "Iteration 3377, loss = 0.14214112\n",
      "Iteration 3378, loss = 0.14209275\n",
      "Iteration 3379, loss = 0.14204443\n",
      "Iteration 3380, loss = 0.14199614\n",
      "Iteration 3381, loss = 0.14194790\n",
      "Iteration 3382, loss = 0.14189969\n",
      "Iteration 3383, loss = 0.14185151\n",
      "Iteration 3384, loss = 0.14180338\n",
      "Iteration 3385, loss = 0.14175528\n",
      "Iteration 3386, loss = 0.14170722\n",
      "Iteration 3387, loss = 0.14165920\n",
      "Iteration 3388, loss = 0.14161121\n",
      "Iteration 3389, loss = 0.14156327\n",
      "Iteration 3390, loss = 0.14151536\n",
      "Iteration 3391, loss = 0.14146749\n",
      "Iteration 3392, loss = 0.14141965\n",
      "Iteration 3393, loss = 0.14137185\n",
      "Iteration 3394, loss = 0.14132409\n",
      "Iteration 3395, loss = 0.14127637\n",
      "Iteration 3396, loss = 0.14122869\n",
      "Iteration 3397, loss = 0.14118104\n",
      "Iteration 3398, loss = 0.14113343\n",
      "Iteration 3399, loss = 0.14108585\n",
      "Iteration 3400, loss = 0.14103832\n",
      "Iteration 3401, loss = 0.14099082\n",
      "Iteration 3402, loss = 0.14094336\n",
      "Iteration 3403, loss = 0.14089593\n",
      "Iteration 3404, loss = 0.14084854\n",
      "Iteration 3405, loss = 0.14080119\n",
      "Iteration 3406, loss = 0.14075388\n",
      "Iteration 3407, loss = 0.14070660\n",
      "Iteration 3408, loss = 0.14065936\n",
      "Iteration 3409, loss = 0.14061216\n",
      "Iteration 3410, loss = 0.14056500\n",
      "Iteration 3411, loss = 0.14051787\n",
      "Iteration 3412, loss = 0.14047078\n",
      "Iteration 3413, loss = 0.14042372\n",
      "Iteration 3414, loss = 0.14037670\n",
      "Iteration 3415, loss = 0.14032972\n",
      "Iteration 3416, loss = 0.14028278\n",
      "Iteration 3417, loss = 0.14023587\n",
      "Iteration 3418, loss = 0.14018900\n",
      "Iteration 3419, loss = 0.14014216\n",
      "Iteration 3420, loss = 0.14009536\n",
      "Iteration 3421, loss = 0.14004860\n",
      "Iteration 3422, loss = 0.14000187\n",
      "Iteration 3423, loss = 0.13995519\n",
      "Iteration 3424, loss = 0.13990853\n",
      "Iteration 3425, loss = 0.13986192\n",
      "Iteration 3426, loss = 0.13981534\n",
      "Iteration 3427, loss = 0.13976880\n",
      "Iteration 3428, loss = 0.13972229\n",
      "Iteration 3429, loss = 0.13967582\n",
      "Iteration 3430, loss = 0.13962938\n",
      "Iteration 3431, loss = 0.13958299\n",
      "Iteration 3432, loss = 0.13953663\n",
      "Iteration 3433, loss = 0.13949030\n",
      "Iteration 3434, loss = 0.13944401\n",
      "Iteration 3435, loss = 0.13939776\n",
      "Iteration 3436, loss = 0.13935154\n",
      "Iteration 3437, loss = 0.13930536\n",
      "Iteration 3438, loss = 0.13925922\n",
      "Iteration 3439, loss = 0.13921311\n",
      "Iteration 3440, loss = 0.13916703\n",
      "Iteration 3441, loss = 0.13912100\n",
      "Iteration 3442, loss = 0.13907500\n",
      "Iteration 3443, loss = 0.13902903\n",
      "Iteration 3444, loss = 0.13898310\n",
      "Iteration 3445, loss = 0.13893721\n",
      "Iteration 3446, loss = 0.13889135\n",
      "Iteration 3447, loss = 0.13884553\n",
      "Iteration 3448, loss = 0.13879975\n",
      "Iteration 3449, loss = 0.13875400\n",
      "Iteration 3450, loss = 0.13870828\n",
      "Iteration 3451, loss = 0.13866260\n",
      "Iteration 3452, loss = 0.13861696\n",
      "Iteration 3453, loss = 0.13857135\n",
      "Iteration 3454, loss = 0.13852578\n",
      "Iteration 3455, loss = 0.13848024\n",
      "Iteration 3456, loss = 0.13843474\n",
      "Iteration 3457, loss = 0.13838928\n",
      "Iteration 3458, loss = 0.13834385\n",
      "Iteration 3459, loss = 0.13829845\n",
      "Iteration 3460, loss = 0.13825310\n",
      "Iteration 3461, loss = 0.13820777\n",
      "Iteration 3462, loss = 0.13816248\n",
      "Iteration 3463, loss = 0.13811723\n",
      "Iteration 3464, loss = 0.13807201\n",
      "Iteration 3465, loss = 0.13802683\n",
      "Iteration 3466, loss = 0.13798169\n",
      "Iteration 3467, loss = 0.13793657\n",
      "Iteration 3468, loss = 0.13789150\n",
      "Iteration 3469, loss = 0.13784646\n",
      "Iteration 3470, loss = 0.13780145\n",
      "Iteration 3471, loss = 0.13775648\n",
      "Iteration 3472, loss = 0.13771154\n",
      "Iteration 3473, loss = 0.13766664\n",
      "Iteration 3474, loss = 0.13762177\n",
      "Iteration 3475, loss = 0.13757694\n",
      "Iteration 3476, loss = 0.13753215\n",
      "Iteration 3477, loss = 0.13748739\n",
      "Iteration 3478, loss = 0.13744266\n",
      "Iteration 3479, loss = 0.13739797\n",
      "Iteration 3480, loss = 0.13735331\n",
      "Iteration 3481, loss = 0.13730869\n",
      "Iteration 3482, loss = 0.13726410\n",
      "Iteration 3483, loss = 0.13721955\n",
      "Iteration 3484, loss = 0.13717503\n",
      "Iteration 3485, loss = 0.13713055\n",
      "Iteration 3486, loss = 0.13708610\n",
      "Iteration 3487, loss = 0.13704169\n",
      "Iteration 3488, loss = 0.13699731\n",
      "Iteration 3489, loss = 0.13695297\n",
      "Iteration 3490, loss = 0.13690866\n",
      "Iteration 3491, loss = 0.13686438\n",
      "Iteration 3492, loss = 0.13682014\n",
      "Iteration 3493, loss = 0.13677593\n",
      "Iteration 3494, loss = 0.13673176\n",
      "Iteration 3495, loss = 0.13668762\n",
      "Iteration 3496, loss = 0.13664352\n",
      "Iteration 3497, loss = 0.13659945\n",
      "Iteration 3498, loss = 0.13655542\n",
      "Iteration 3499, loss = 0.13651142\n",
      "Iteration 3500, loss = 0.13646745\n",
      "Iteration 3501, loss = 0.13642352\n",
      "Iteration 3502, loss = 0.13637962\n",
      "Iteration 3503, loss = 0.13633576\n",
      "Iteration 3504, loss = 0.13629193\n",
      "Iteration 3505, loss = 0.13624813\n",
      "Iteration 3506, loss = 0.13620437\n",
      "Iteration 3507, loss = 0.13616065\n",
      "Iteration 3508, loss = 0.13611695\n",
      "Iteration 3509, loss = 0.13607329\n",
      "Iteration 3510, loss = 0.13602967\n",
      "Iteration 3511, loss = 0.13598608\n",
      "Iteration 3512, loss = 0.13594252\n",
      "Iteration 3513, loss = 0.13589900\n",
      "Iteration 3514, loss = 0.13585551\n",
      "Iteration 3515, loss = 0.13581205\n",
      "Iteration 3516, loss = 0.13576863\n",
      "Iteration 3517, loss = 0.13572524\n",
      "Iteration 3518, loss = 0.13568189\n",
      "Iteration 3519, loss = 0.13563857\n",
      "Iteration 3520, loss = 0.13559528\n",
      "Iteration 3521, loss = 0.13555203\n",
      "Iteration 3522, loss = 0.13550881\n",
      "Iteration 3523, loss = 0.13546562\n",
      "Iteration 3524, loss = 0.13542247\n",
      "Iteration 3525, loss = 0.13537935\n",
      "Iteration 3526, loss = 0.13533627\n",
      "Iteration 3527, loss = 0.13529322\n",
      "Iteration 3528, loss = 0.13525020\n",
      "Iteration 3529, loss = 0.13520722\n",
      "Iteration 3530, loss = 0.13516426\n",
      "Iteration 3531, loss = 0.13512135\n",
      "Iteration 3532, loss = 0.13507846\n",
      "Iteration 3533, loss = 0.13503561\n",
      "Iteration 3534, loss = 0.13499280\n",
      "Iteration 3535, loss = 0.13495001\n",
      "Iteration 3536, loss = 0.13490726\n",
      "Iteration 3537, loss = 0.13486455\n",
      "Iteration 3538, loss = 0.13482188\n",
      "Iteration 3539, loss = 0.13477925\n",
      "Iteration 3540, loss = 0.13473668\n",
      "Iteration 3541, loss = 0.13469418\n",
      "Iteration 3542, loss = 0.13465177\n",
      "Iteration 3543, loss = 0.13460948\n",
      "Iteration 3544, loss = 0.13456725\n",
      "Iteration 3545, loss = 0.13452499\n",
      "Iteration 3546, loss = 0.13448240\n",
      "Iteration 3547, loss = 0.13443957\n",
      "Iteration 3548, loss = 0.13439687\n",
      "Iteration 3549, loss = 0.13435462\n",
      "Iteration 3550, loss = 0.13431263\n",
      "Iteration 3551, loss = 0.13427046\n",
      "Iteration 3552, loss = 0.13422802\n",
      "Iteration 3553, loss = 0.13418561\n",
      "Iteration 3554, loss = 0.13414353\n",
      "Iteration 3555, loss = 0.13410159\n",
      "Iteration 3556, loss = 0.13405948\n",
      "Iteration 3557, loss = 0.13401726\n",
      "Iteration 3558, loss = 0.13397518\n",
      "Iteration 3559, loss = 0.13393330\n",
      "Iteration 3560, loss = 0.13389141\n",
      "Iteration 3561, loss = 0.13384940\n",
      "Iteration 3562, loss = 0.13380742\n",
      "Iteration 3563, loss = 0.13376561\n",
      "Iteration 3564, loss = 0.13372385\n",
      "Iteration 3565, loss = 0.13368203\n",
      "Iteration 3566, loss = 0.13364020\n",
      "Iteration 3567, loss = 0.13359848\n",
      "Iteration 3568, loss = 0.13355684\n",
      "Iteration 3569, loss = 0.13351517\n",
      "Iteration 3570, loss = 0.13347350\n",
      "Iteration 3571, loss = 0.13343189\n",
      "Iteration 3572, loss = 0.13339036\n",
      "Iteration 3573, loss = 0.13334883\n",
      "Iteration 3574, loss = 0.13330731\n",
      "Iteration 3575, loss = 0.13326582\n",
      "Iteration 3576, loss = 0.13322440\n",
      "Iteration 3577, loss = 0.13318301\n",
      "Iteration 3578, loss = 0.13314163\n",
      "Iteration 3579, loss = 0.13310027\n",
      "Iteration 3580, loss = 0.13305897\n",
      "Iteration 3581, loss = 0.13301771\n",
      "Iteration 3582, loss = 0.13297646\n",
      "Iteration 3583, loss = 0.13293524\n",
      "Iteration 3584, loss = 0.13289406\n",
      "Iteration 3585, loss = 0.13285292\n",
      "Iteration 3586, loss = 0.13281180\n",
      "Iteration 3587, loss = 0.13277071\n",
      "Iteration 3588, loss = 0.13272966\n",
      "Iteration 3589, loss = 0.13268864\n",
      "Iteration 3590, loss = 0.13264766\n",
      "Iteration 3591, loss = 0.13260670\n",
      "Iteration 3592, loss = 0.13256577\n",
      "Iteration 3593, loss = 0.13252488\n",
      "Iteration 3594, loss = 0.13248402\n",
      "Iteration 3595, loss = 0.13244319\n",
      "Iteration 3596, loss = 0.13240239\n",
      "Iteration 3597, loss = 0.13236162\n",
      "Iteration 3598, loss = 0.13232089\n",
      "Iteration 3599, loss = 0.13228018\n",
      "Iteration 3600, loss = 0.13223951\n",
      "Iteration 3601, loss = 0.13219887\n",
      "Iteration 3602, loss = 0.13215826\n",
      "Iteration 3603, loss = 0.13211769\n",
      "Iteration 3604, loss = 0.13207714\n",
      "Iteration 3605, loss = 0.13203662\n",
      "Iteration 3606, loss = 0.13199614\n",
      "Iteration 3607, loss = 0.13195569\n",
      "Iteration 3608, loss = 0.13191527\n",
      "Iteration 3609, loss = 0.13187488\n",
      "Iteration 3610, loss = 0.13183452\n",
      "Iteration 3611, loss = 0.13179419\n",
      "Iteration 3612, loss = 0.13175390\n",
      "Iteration 3613, loss = 0.13171363\n",
      "Iteration 3614, loss = 0.13167340\n",
      "Iteration 3615, loss = 0.13163320\n",
      "Iteration 3616, loss = 0.13159303\n",
      "Iteration 3617, loss = 0.13155289\n",
      "Iteration 3618, loss = 0.13151278\n",
      "Iteration 3619, loss = 0.13147270\n",
      "Iteration 3620, loss = 0.13143265\n",
      "Iteration 3621, loss = 0.13139264\n",
      "Iteration 3622, loss = 0.13135265\n",
      "Iteration 3623, loss = 0.13131270\n",
      "Iteration 3624, loss = 0.13127277\n",
      "Iteration 3625, loss = 0.13123288\n",
      "Iteration 3626, loss = 0.13119302\n",
      "Iteration 3627, loss = 0.13115319\n",
      "Iteration 3628, loss = 0.13111339\n",
      "Iteration 3629, loss = 0.13107362\n",
      "Iteration 3630, loss = 0.13103388\n",
      "Iteration 3631, loss = 0.13099417\n",
      "Iteration 3632, loss = 0.13095450\n",
      "Iteration 3633, loss = 0.13091485\n",
      "Iteration 3634, loss = 0.13087523\n",
      "Iteration 3635, loss = 0.13083565\n",
      "Iteration 3636, loss = 0.13079610\n",
      "Iteration 3637, loss = 0.13075657\n",
      "Iteration 3638, loss = 0.13071708\n",
      "Iteration 3639, loss = 0.13067762\n",
      "Iteration 3640, loss = 0.13063818\n",
      "Iteration 3641, loss = 0.13059878\n",
      "Iteration 3642, loss = 0.13055941\n",
      "Iteration 3643, loss = 0.13052007\n",
      "Iteration 3644, loss = 0.13048076\n",
      "Iteration 3645, loss = 0.13044148\n",
      "Iteration 3646, loss = 0.13040223\n",
      "Iteration 3647, loss = 0.13036301\n",
      "Iteration 3648, loss = 0.13032382\n",
      "Iteration 3649, loss = 0.13028466\n",
      "Iteration 3650, loss = 0.13024553\n",
      "Iteration 3651, loss = 0.13020643\n",
      "Iteration 3652, loss = 0.13016736\n",
      "Iteration 3653, loss = 0.13012833\n",
      "Iteration 3654, loss = 0.13008932\n",
      "Iteration 3655, loss = 0.13005034\n",
      "Iteration 3656, loss = 0.13001139\n",
      "Iteration 3657, loss = 0.12997248\n",
      "Iteration 3658, loss = 0.12993359\n",
      "Iteration 3659, loss = 0.12989473\n",
      "Iteration 3660, loss = 0.12985590\n",
      "Iteration 3661, loss = 0.12981710\n",
      "Iteration 3662, loss = 0.12977834\n",
      "Iteration 3663, loss = 0.12973960\n",
      "Iteration 3664, loss = 0.12970089\n",
      "Iteration 3665, loss = 0.12966221\n",
      "Iteration 3666, loss = 0.12962356\n",
      "Iteration 3667, loss = 0.12958495\n",
      "Iteration 3668, loss = 0.12954636\n",
      "Iteration 3669, loss = 0.12950780\n",
      "Iteration 3670, loss = 0.12946927\n",
      "Iteration 3671, loss = 0.12943077\n",
      "Iteration 3672, loss = 0.12939230\n",
      "Iteration 3673, loss = 0.12935386\n",
      "Iteration 3674, loss = 0.12931545\n",
      "Iteration 3675, loss = 0.12927707\n",
      "Iteration 3676, loss = 0.12923872\n",
      "Iteration 3677, loss = 0.12920040\n",
      "Iteration 3678, loss = 0.12916210\n",
      "Iteration 3679, loss = 0.12912384\n",
      "Iteration 3680, loss = 0.12908561\n",
      "Iteration 3681, loss = 0.12904740\n",
      "Iteration 3682, loss = 0.12900923\n",
      "Iteration 3683, loss = 0.12897109\n",
      "Iteration 3684, loss = 0.12893297\n",
      "Iteration 3685, loss = 0.12889488\n",
      "Iteration 3686, loss = 0.12885683\n",
      "Iteration 3687, loss = 0.12881880\n",
      "Iteration 3688, loss = 0.12878080\n",
      "Iteration 3689, loss = 0.12874283\n",
      "Iteration 3690, loss = 0.12870489\n",
      "Iteration 3691, loss = 0.12866698\n",
      "Iteration 3692, loss = 0.12862910\n",
      "Iteration 3693, loss = 0.12859125\n",
      "Iteration 3694, loss = 0.12855343\n",
      "Iteration 3695, loss = 0.12851564\n",
      "Iteration 3696, loss = 0.12847787\n",
      "Iteration 3697, loss = 0.12844014\n",
      "Iteration 3698, loss = 0.12840243\n",
      "Iteration 3699, loss = 0.12836475\n",
      "Iteration 3700, loss = 0.12832711\n",
      "Iteration 3701, loss = 0.12828949\n",
      "Iteration 3702, loss = 0.12825190\n",
      "Iteration 3703, loss = 0.12821433\n",
      "Iteration 3704, loss = 0.12817680\n",
      "Iteration 3705, loss = 0.12813930\n",
      "Iteration 3706, loss = 0.12810182\n",
      "Iteration 3707, loss = 0.12806438\n",
      "Iteration 3708, loss = 0.12802696\n",
      "Iteration 3709, loss = 0.12798957\n",
      "Iteration 3710, loss = 0.12795221\n",
      "Iteration 3711, loss = 0.12791488\n",
      "Iteration 3712, loss = 0.12787758\n",
      "Iteration 3713, loss = 0.12784031\n",
      "Iteration 3714, loss = 0.12780307\n",
      "Iteration 3715, loss = 0.12776585\n",
      "Iteration 3716, loss = 0.12772866\n",
      "Iteration 3717, loss = 0.12769150\n",
      "Iteration 3718, loss = 0.12765437\n",
      "Iteration 3719, loss = 0.12761727\n",
      "Iteration 3720, loss = 0.12758020\n",
      "Iteration 3721, loss = 0.12754316\n",
      "Iteration 3722, loss = 0.12750614\n",
      "Iteration 3723, loss = 0.12746915\n",
      "Iteration 3724, loss = 0.12743219\n",
      "Iteration 3725, loss = 0.12739526\n",
      "Iteration 3726, loss = 0.12735836\n",
      "Iteration 3727, loss = 0.12732149\n",
      "Iteration 3728, loss = 0.12728464\n",
      "Iteration 3729, loss = 0.12724783\n",
      "Iteration 3730, loss = 0.12721104\n",
      "Iteration 3731, loss = 0.12717428\n",
      "Iteration 3732, loss = 0.12713754\n",
      "Iteration 3733, loss = 0.12710084\n",
      "Iteration 3734, loss = 0.12706417\n",
      "Iteration 3735, loss = 0.12702752\n",
      "Iteration 3736, loss = 0.12699090\n",
      "Iteration 3737, loss = 0.12695431\n",
      "Iteration 3738, loss = 0.12691774\n",
      "Iteration 3739, loss = 0.12688121\n",
      "Iteration 3740, loss = 0.12684470\n",
      "Iteration 3741, loss = 0.12680822\n",
      "Iteration 3742, loss = 0.12677177\n",
      "Iteration 3743, loss = 0.12673535\n",
      "Iteration 3744, loss = 0.12669896\n",
      "Iteration 3745, loss = 0.12666259\n",
      "Iteration 3746, loss = 0.12662625\n",
      "Iteration 3747, loss = 0.12658994\n",
      "Iteration 3748, loss = 0.12655366\n",
      "Iteration 3749, loss = 0.12651740\n",
      "Iteration 3750, loss = 0.12648117\n",
      "Iteration 3751, loss = 0.12644497\n",
      "Iteration 3752, loss = 0.12640880\n",
      "Iteration 3753, loss = 0.12637266\n",
      "Iteration 3754, loss = 0.12633654\n",
      "Iteration 3755, loss = 0.12630045\n",
      "Iteration 3756, loss = 0.12626439\n",
      "Iteration 3757, loss = 0.12622836\n",
      "Iteration 3758, loss = 0.12619235\n",
      "Iteration 3759, loss = 0.12615638\n",
      "Iteration 3760, loss = 0.12612043\n",
      "Iteration 3761, loss = 0.12608450\n",
      "Iteration 3762, loss = 0.12604861\n",
      "Iteration 3763, loss = 0.12601274\n",
      "Iteration 3764, loss = 0.12597690\n",
      "Iteration 3765, loss = 0.12594109\n",
      "Iteration 3766, loss = 0.12590530\n",
      "Iteration 3767, loss = 0.12586955\n",
      "Iteration 3768, loss = 0.12583382\n",
      "Iteration 3769, loss = 0.12579811\n",
      "Iteration 3770, loss = 0.12576244\n",
      "Iteration 3771, loss = 0.12572679\n",
      "Iteration 3772, loss = 0.12569117\n",
      "Iteration 3773, loss = 0.12565558\n",
      "Iteration 3774, loss = 0.12562001\n",
      "Iteration 3775, loss = 0.12558447\n",
      "Iteration 3776, loss = 0.12554896\n",
      "Iteration 3777, loss = 0.12551348\n",
      "Iteration 3778, loss = 0.12547802\n",
      "Iteration 3779, loss = 0.12544259\n",
      "Iteration 3780, loss = 0.12540719\n",
      "Iteration 3781, loss = 0.12537181\n",
      "Iteration 3782, loss = 0.12533646\n",
      "Iteration 3783, loss = 0.12530114\n",
      "Iteration 3784, loss = 0.12526585\n",
      "Iteration 3785, loss = 0.12523058\n",
      "Iteration 3786, loss = 0.12519534\n",
      "Iteration 3787, loss = 0.12516013\n",
      "Iteration 3788, loss = 0.12512494\n",
      "Iteration 3789, loss = 0.12508978\n",
      "Iteration 3790, loss = 0.12505465\n",
      "Iteration 3791, loss = 0.12501954\n",
      "Iteration 3792, loss = 0.12498447\n",
      "Iteration 3793, loss = 0.12494941\n",
      "Iteration 3794, loss = 0.12491439\n",
      "Iteration 3795, loss = 0.12487939\n",
      "Iteration 3796, loss = 0.12484442\n",
      "Iteration 3797, loss = 0.12480948\n",
      "Iteration 3798, loss = 0.12477456\n",
      "Iteration 3799, loss = 0.12473967\n",
      "Iteration 3800, loss = 0.12470480\n",
      "Iteration 3801, loss = 0.12466997\n",
      "Iteration 3802, loss = 0.12463516\n",
      "Iteration 3803, loss = 0.12460037\n",
      "Iteration 3804, loss = 0.12456561\n",
      "Iteration 3805, loss = 0.12453088\n",
      "Iteration 3806, loss = 0.12449618\n",
      "Iteration 3807, loss = 0.12446150\n",
      "Iteration 3808, loss = 0.12442685\n",
      "Iteration 3809, loss = 0.12439223\n",
      "Iteration 3810, loss = 0.12435763\n",
      "Iteration 3811, loss = 0.12432306\n",
      "Iteration 3812, loss = 0.12428851\n",
      "Iteration 3813, loss = 0.12425399\n",
      "Iteration 3814, loss = 0.12421950\n",
      "Iteration 3815, loss = 0.12418503\n",
      "Iteration 3816, loss = 0.12415059\n",
      "Iteration 3817, loss = 0.12411618\n",
      "Iteration 3818, loss = 0.12408179\n",
      "Iteration 3819, loss = 0.12404743\n",
      "Iteration 3820, loss = 0.12401310\n",
      "Iteration 3821, loss = 0.12397879\n",
      "Iteration 3822, loss = 0.12394451\n",
      "Iteration 3823, loss = 0.12391026\n",
      "Iteration 3824, loss = 0.12387603\n",
      "Iteration 3825, loss = 0.12384182\n",
      "Iteration 3826, loss = 0.12380765\n",
      "Iteration 3827, loss = 0.12377350\n",
      "Iteration 3828, loss = 0.12373938\n",
      "Iteration 3829, loss = 0.12370529\n",
      "Iteration 3830, loss = 0.12367123\n",
      "Iteration 3831, loss = 0.12363722\n",
      "Iteration 3832, loss = 0.12360326\n",
      "Iteration 3833, loss = 0.12356937\n",
      "Iteration 3834, loss = 0.12353559\n",
      "Iteration 3835, loss = 0.12350190\n",
      "Iteration 3836, loss = 0.12346830\n",
      "Iteration 3837, loss = 0.12343449\n",
      "Iteration 3838, loss = 0.12340035\n",
      "Iteration 3839, loss = 0.12336598\n",
      "Iteration 3840, loss = 0.12333192\n",
      "Iteration 3841, loss = 0.12329834\n",
      "Iteration 3842, loss = 0.12326487\n",
      "Iteration 3843, loss = 0.12323111\n",
      "Iteration 3844, loss = 0.12319709\n",
      "Iteration 3845, loss = 0.12316327\n",
      "Iteration 3846, loss = 0.12312978\n",
      "Iteration 3847, loss = 0.12309630\n",
      "Iteration 3848, loss = 0.12306258\n",
      "Iteration 3849, loss = 0.12302883\n",
      "Iteration 3850, loss = 0.12299530\n",
      "Iteration 3851, loss = 0.12296191\n",
      "Iteration 3852, loss = 0.12292840\n",
      "Iteration 3853, loss = 0.12289480\n",
      "Iteration 3854, loss = 0.12286132\n",
      "Iteration 3855, loss = 0.12282798\n",
      "Iteration 3856, loss = 0.12279462\n",
      "Iteration 3857, loss = 0.12276117\n",
      "Iteration 3858, loss = 0.12272778\n",
      "Iteration 3859, loss = 0.12269450\n",
      "Iteration 3860, loss = 0.12266125\n",
      "Iteration 3861, loss = 0.12262794\n",
      "Iteration 3862, loss = 0.12259466\n",
      "Iteration 3863, loss = 0.12256145\n",
      "Iteration 3864, loss = 0.12252829\n",
      "Iteration 3865, loss = 0.12249511\n",
      "Iteration 3866, loss = 0.12246194\n",
      "Iteration 3867, loss = 0.12242882\n",
      "Iteration 3868, loss = 0.12239575\n",
      "Iteration 3869, loss = 0.12236269\n",
      "Iteration 3870, loss = 0.12232962\n",
      "Iteration 3871, loss = 0.12229660\n",
      "Iteration 3872, loss = 0.12226363\n",
      "Iteration 3873, loss = 0.12223067\n",
      "Iteration 3874, loss = 0.12219771\n",
      "Iteration 3875, loss = 0.12216479\n",
      "Iteration 3876, loss = 0.12213191\n",
      "Iteration 3877, loss = 0.12209905\n",
      "Iteration 3878, loss = 0.12206620\n",
      "Iteration 3879, loss = 0.12203338\n",
      "Iteration 3880, loss = 0.12200059\n",
      "Iteration 3881, loss = 0.12196783\n",
      "Iteration 3882, loss = 0.12193509\n",
      "Iteration 3883, loss = 0.12190237\n",
      "Iteration 3884, loss = 0.12186968\n",
      "Iteration 3885, loss = 0.12183702\n",
      "Iteration 3886, loss = 0.12180438\n",
      "Iteration 3887, loss = 0.12177176\n",
      "Iteration 3888, loss = 0.12173916\n",
      "Iteration 3889, loss = 0.12170660\n",
      "Iteration 3890, loss = 0.12167406\n",
      "Iteration 3891, loss = 0.12164154\n",
      "Iteration 3892, loss = 0.12160905\n",
      "Iteration 3893, loss = 0.12157658\n",
      "Iteration 3894, loss = 0.12154414\n",
      "Iteration 3895, loss = 0.12151172\n",
      "Iteration 3896, loss = 0.12147933\n",
      "Iteration 3897, loss = 0.12144696\n",
      "Iteration 3898, loss = 0.12141462\n",
      "Iteration 3899, loss = 0.12138230\n",
      "Iteration 3900, loss = 0.12135000\n",
      "Iteration 3901, loss = 0.12131773\n",
      "Iteration 3902, loss = 0.12128549\n",
      "Iteration 3903, loss = 0.12125327\n",
      "Iteration 3904, loss = 0.12122107\n",
      "Iteration 3905, loss = 0.12118890\n",
      "Iteration 3906, loss = 0.12115675\n",
      "Iteration 3907, loss = 0.12112463\n",
      "Iteration 3908, loss = 0.12109253\n",
      "Iteration 3909, loss = 0.12106046\n",
      "Iteration 3910, loss = 0.12102841\n",
      "Iteration 3911, loss = 0.12099638\n",
      "Iteration 3912, loss = 0.12096438\n",
      "Iteration 3913, loss = 0.12093240\n",
      "Iteration 3914, loss = 0.12090045\n",
      "Iteration 3915, loss = 0.12086852\n",
      "Iteration 3916, loss = 0.12083662\n",
      "Iteration 3917, loss = 0.12080474\n",
      "Iteration 3918, loss = 0.12077289\n",
      "Iteration 3919, loss = 0.12074105\n",
      "Iteration 3920, loss = 0.12070925\n",
      "Iteration 3921, loss = 0.12067746\n",
      "Iteration 3922, loss = 0.12064571\n",
      "Iteration 3923, loss = 0.12061397\n",
      "Iteration 3924, loss = 0.12058226\n",
      "Iteration 3925, loss = 0.12055057\n",
      "Iteration 3926, loss = 0.12051891\n",
      "Iteration 3927, loss = 0.12048727\n",
      "Iteration 3928, loss = 0.12045566\n",
      "Iteration 3929, loss = 0.12042407\n",
      "Iteration 3930, loss = 0.12039250\n",
      "Iteration 3931, loss = 0.12036096\n",
      "Iteration 3932, loss = 0.12032944\n",
      "Iteration 3933, loss = 0.12029795\n",
      "Iteration 3934, loss = 0.12026648\n",
      "Iteration 3935, loss = 0.12023503\n",
      "Iteration 3936, loss = 0.12020361\n",
      "Iteration 3937, loss = 0.12017221\n",
      "Iteration 3938, loss = 0.12014084\n",
      "Iteration 3939, loss = 0.12010949\n",
      "Iteration 3940, loss = 0.12007816\n",
      "Iteration 3941, loss = 0.12004686\n",
      "Iteration 3942, loss = 0.12001558\n",
      "Iteration 3943, loss = 0.11998432\n",
      "Iteration 3944, loss = 0.11995309\n",
      "Iteration 3945, loss = 0.11992188\n",
      "Iteration 3946, loss = 0.11989069\n",
      "Iteration 3947, loss = 0.11985953\n",
      "Iteration 3948, loss = 0.11982840\n",
      "Iteration 3949, loss = 0.11979728\n",
      "Iteration 3950, loss = 0.11976619\n",
      "Iteration 3951, loss = 0.11973512\n",
      "Iteration 3952, loss = 0.11970408\n",
      "Iteration 3953, loss = 0.11967306\n",
      "Iteration 3954, loss = 0.11964207\n",
      "Iteration 3955, loss = 0.11961109\n",
      "Iteration 3956, loss = 0.11958014\n",
      "Iteration 3957, loss = 0.11954922\n",
      "Iteration 3958, loss = 0.11951832\n",
      "Iteration 3959, loss = 0.11948744\n",
      "Iteration 3960, loss = 0.11945658\n",
      "Iteration 3961, loss = 0.11942575\n",
      "Iteration 3962, loss = 0.11939494\n",
      "Iteration 3963, loss = 0.11936416\n",
      "Iteration 3964, loss = 0.11933339\n",
      "Iteration 3965, loss = 0.11930266\n",
      "Iteration 3966, loss = 0.11927194\n",
      "Iteration 3967, loss = 0.11924125\n",
      "Iteration 3968, loss = 0.11921058\n",
      "Iteration 3969, loss = 0.11917993\n",
      "Iteration 3970, loss = 0.11914931\n",
      "Iteration 3971, loss = 0.11911871\n",
      "Iteration 3972, loss = 0.11908814\n",
      "Iteration 3973, loss = 0.11905758\n",
      "Iteration 3974, loss = 0.11902705\n",
      "Iteration 3975, loss = 0.11899655\n",
      "Iteration 3976, loss = 0.11896606\n",
      "Iteration 3977, loss = 0.11893560\n",
      "Iteration 3978, loss = 0.11890517\n",
      "Iteration 3979, loss = 0.11887475\n",
      "Iteration 3980, loss = 0.11884436\n",
      "Iteration 3981, loss = 0.11881399\n",
      "Iteration 3982, loss = 0.11878365\n",
      "Iteration 3983, loss = 0.11875333\n",
      "Iteration 3984, loss = 0.11872303\n",
      "Iteration 3985, loss = 0.11869275\n",
      "Iteration 3986, loss = 0.11866250\n",
      "Iteration 3987, loss = 0.11863227\n",
      "Iteration 3988, loss = 0.11860206\n",
      "Iteration 3989, loss = 0.11857187\n",
      "Iteration 3990, loss = 0.11854171\n",
      "Iteration 3991, loss = 0.11851157\n",
      "Iteration 3992, loss = 0.11848146\n",
      "Iteration 3993, loss = 0.11845136\n",
      "Iteration 3994, loss = 0.11842129\n",
      "Iteration 3995, loss = 0.11839125\n",
      "Iteration 3996, loss = 0.11836122\n",
      "Iteration 3997, loss = 0.11833122\n",
      "Iteration 3998, loss = 0.11830124\n",
      "Iteration 3999, loss = 0.11827128\n",
      "Iteration 4000, loss = 0.11824135\n",
      "Iteration 4001, loss = 0.11821143\n",
      "Iteration 4002, loss = 0.11818155\n",
      "Iteration 4003, loss = 0.11815168\n",
      "Iteration 4004, loss = 0.11812183\n",
      "Iteration 4005, loss = 0.11809201\n",
      "Iteration 4006, loss = 0.11806221\n",
      "Iteration 4007, loss = 0.11803244\n",
      "Iteration 4008, loss = 0.11800268\n",
      "Iteration 4009, loss = 0.11797295\n",
      "Iteration 4010, loss = 0.11794324\n",
      "Iteration 4011, loss = 0.11791356\n",
      "Iteration 4012, loss = 0.11788389\n",
      "Iteration 4013, loss = 0.11785425\n",
      "Iteration 4014, loss = 0.11782463\n",
      "Iteration 4015, loss = 0.11779504\n",
      "Iteration 4016, loss = 0.11776546\n",
      "Iteration 4017, loss = 0.11773591\n",
      "Iteration 4018, loss = 0.11770638\n",
      "Iteration 4019, loss = 0.11767687\n",
      "Iteration 4020, loss = 0.11764739\n",
      "Iteration 4021, loss = 0.11761792\n",
      "Iteration 4022, loss = 0.11758848\n",
      "Iteration 4023, loss = 0.11755907\n",
      "Iteration 4024, loss = 0.11752967\n",
      "Iteration 4025, loss = 0.11750030\n",
      "Iteration 4026, loss = 0.11747094\n",
      "Iteration 4027, loss = 0.11744161\n",
      "Iteration 4028, loss = 0.11741231\n",
      "Iteration 4029, loss = 0.11738302\n",
      "Iteration 4030, loss = 0.11735376\n",
      "Iteration 4031, loss = 0.11732452\n",
      "Iteration 4032, loss = 0.11729530\n",
      "Iteration 4033, loss = 0.11726610\n",
      "Iteration 4034, loss = 0.11723693\n",
      "Iteration 4035, loss = 0.11720777\n",
      "Iteration 4036, loss = 0.11717864\n",
      "Iteration 4037, loss = 0.11714953\n",
      "Iteration 4038, loss = 0.11712045\n",
      "Iteration 4039, loss = 0.11709138\n",
      "Iteration 4040, loss = 0.11706234\n",
      "Iteration 4041, loss = 0.11703332\n",
      "Iteration 4042, loss = 0.11700432\n",
      "Iteration 4043, loss = 0.11697534\n",
      "Iteration 4044, loss = 0.11694638\n",
      "Iteration 4045, loss = 0.11691745\n",
      "Iteration 4046, loss = 0.11688854\n",
      "Iteration 4047, loss = 0.11685965\n",
      "Iteration 4048, loss = 0.11683078\n",
      "Iteration 4049, loss = 0.11680193\n",
      "Iteration 4050, loss = 0.11677311\n",
      "Iteration 4051, loss = 0.11674431\n",
      "Iteration 4052, loss = 0.11671552\n",
      "Iteration 4053, loss = 0.11668676\n",
      "Iteration 4054, loss = 0.11665803\n",
      "Iteration 4055, loss = 0.11662931\n",
      "Iteration 4056, loss = 0.11660062\n",
      "Iteration 4057, loss = 0.11657194\n",
      "Iteration 4058, loss = 0.11654329\n",
      "Iteration 4059, loss = 0.11651466\n",
      "Iteration 4060, loss = 0.11648605\n",
      "Iteration 4061, loss = 0.11645747\n",
      "Iteration 4062, loss = 0.11642890\n",
      "Iteration 4063, loss = 0.11640036\n",
      "Iteration 4064, loss = 0.11637184\n",
      "Iteration 4065, loss = 0.11634334\n",
      "Iteration 4066, loss = 0.11631486\n",
      "Iteration 4067, loss = 0.11628640\n",
      "Iteration 4068, loss = 0.11625796\n",
      "Iteration 4069, loss = 0.11622955\n",
      "Iteration 4070, loss = 0.11620115\n",
      "Iteration 4071, loss = 0.11617278\n",
      "Iteration 4072, loss = 0.11614443\n",
      "Iteration 4073, loss = 0.11611610\n",
      "Iteration 4074, loss = 0.11608779\n",
      "Iteration 4075, loss = 0.11605951\n",
      "Iteration 4076, loss = 0.11603124\n",
      "Iteration 4077, loss = 0.11600300\n",
      "Iteration 4078, loss = 0.11597477\n",
      "Iteration 4079, loss = 0.11594657\n",
      "Iteration 4080, loss = 0.11591839\n",
      "Iteration 4081, loss = 0.11589023\n",
      "Iteration 4082, loss = 0.11586210\n",
      "Iteration 4083, loss = 0.11583398\n",
      "Iteration 4084, loss = 0.11580588\n",
      "Iteration 4085, loss = 0.11577781\n",
      "Iteration 4086, loss = 0.11574976\n",
      "Iteration 4087, loss = 0.11572172\n",
      "Iteration 4088, loss = 0.11569371\n",
      "Iteration 4089, loss = 0.11566572\n",
      "Iteration 4090, loss = 0.11563775\n",
      "Iteration 4091, loss = 0.11560980\n",
      "Iteration 4092, loss = 0.11558188\n",
      "Iteration 4093, loss = 0.11555397\n",
      "Iteration 4094, loss = 0.11552609\n",
      "Iteration 4095, loss = 0.11549822\n",
      "Iteration 4096, loss = 0.11547038\n",
      "Iteration 4097, loss = 0.11544256\n",
      "Iteration 4098, loss = 0.11541476\n",
      "Iteration 4099, loss = 0.11538698\n",
      "Iteration 4100, loss = 0.11535922\n",
      "Iteration 4101, loss = 0.11533148\n",
      "Iteration 4102, loss = 0.11530376\n",
      "Iteration 4103, loss = 0.11527606\n",
      "Iteration 4104, loss = 0.11524839\n",
      "Iteration 4105, loss = 0.11522073\n",
      "Iteration 4106, loss = 0.11519310\n",
      "Iteration 4107, loss = 0.11516548\n",
      "Iteration 4108, loss = 0.11513789\n",
      "Iteration 4109, loss = 0.11511032\n",
      "Iteration 4110, loss = 0.11508277\n",
      "Iteration 4111, loss = 0.11505524\n",
      "Iteration 4112, loss = 0.11502773\n",
      "Iteration 4113, loss = 0.11500024\n",
      "Iteration 4114, loss = 0.11497277\n",
      "Iteration 4115, loss = 0.11494532\n",
      "Iteration 4116, loss = 0.11491789\n",
      "Iteration 4117, loss = 0.11489049\n",
      "Iteration 4118, loss = 0.11486310\n",
      "Iteration 4119, loss = 0.11483573\n",
      "Iteration 4120, loss = 0.11480839\n",
      "Iteration 4121, loss = 0.11478106\n",
      "Iteration 4122, loss = 0.11475376\n",
      "Iteration 4123, loss = 0.11472648\n",
      "Iteration 4124, loss = 0.11469921\n",
      "Iteration 4125, loss = 0.11467197\n",
      "Iteration 4126, loss = 0.11464475\n",
      "Iteration 4127, loss = 0.11461755\n",
      "Iteration 4128, loss = 0.11459036\n",
      "Iteration 4129, loss = 0.11456320\n",
      "Iteration 4130, loss = 0.11453606\n",
      "Iteration 4131, loss = 0.11450894\n",
      "Iteration 4132, loss = 0.11448184\n",
      "Iteration 4133, loss = 0.11445476\n",
      "Iteration 4134, loss = 0.11442770\n",
      "Iteration 4135, loss = 0.11440067\n",
      "Iteration 4136, loss = 0.11437365\n",
      "Iteration 4137, loss = 0.11434665\n",
      "Iteration 4138, loss = 0.11431967\n",
      "Iteration 4139, loss = 0.11429272\n",
      "Iteration 4140, loss = 0.11426579\n",
      "Iteration 4141, loss = 0.11423889\n",
      "Iteration 4142, loss = 0.11421203\n",
      "Iteration 4143, loss = 0.11418521\n",
      "Iteration 4144, loss = 0.11415847\n",
      "Iteration 4145, loss = 0.11413184\n",
      "Iteration 4146, loss = 0.11410533\n",
      "Iteration 4147, loss = 0.11407891\n",
      "Iteration 4148, loss = 0.11405226\n",
      "Iteration 4149, loss = 0.11402519\n",
      "Iteration 4150, loss = 0.11399781\n",
      "Iteration 4151, loss = 0.11397080\n",
      "Iteration 4152, loss = 0.11394438\n",
      "Iteration 4153, loss = 0.11391804\n",
      "Iteration 4154, loss = 0.11389130\n",
      "Iteration 4155, loss = 0.11386428\n",
      "Iteration 4156, loss = 0.11383756\n",
      "Iteration 4157, loss = 0.11381121\n",
      "Iteration 4158, loss = 0.11378475\n",
      "Iteration 4159, loss = 0.11375801\n",
      "Iteration 4160, loss = 0.11373130\n",
      "Iteration 4161, loss = 0.11370489\n",
      "Iteration 4162, loss = 0.11367853\n",
      "Iteration 4163, loss = 0.11365197\n",
      "Iteration 4164, loss = 0.11362538\n",
      "Iteration 4165, loss = 0.11359899\n",
      "Iteration 4166, loss = 0.11357267\n",
      "Iteration 4167, loss = 0.11354625\n",
      "Iteration 4168, loss = 0.11351978\n",
      "Iteration 4169, loss = 0.11349343\n",
      "Iteration 4170, loss = 0.11346717\n",
      "Iteration 4171, loss = 0.11344084\n",
      "Iteration 4172, loss = 0.11341448\n",
      "Iteration 4173, loss = 0.11338821\n",
      "Iteration 4174, loss = 0.11336200\n",
      "Iteration 4175, loss = 0.11333576\n",
      "Iteration 4176, loss = 0.11330951\n",
      "Iteration 4177, loss = 0.11328330\n",
      "Iteration 4178, loss = 0.11325716\n",
      "Iteration 4179, loss = 0.11323100\n",
      "Iteration 4180, loss = 0.11320484\n",
      "Iteration 4181, loss = 0.11317872\n",
      "Iteration 4182, loss = 0.11315264\n",
      "Iteration 4183, loss = 0.11312656\n",
      "Iteration 4184, loss = 0.11310049\n",
      "Iteration 4185, loss = 0.11307444\n",
      "Iteration 4186, loss = 0.11304843\n",
      "Iteration 4187, loss = 0.11302244\n",
      "Iteration 4188, loss = 0.11299645\n",
      "Iteration 4189, loss = 0.11297048\n",
      "Iteration 4190, loss = 0.11294455\n",
      "Iteration 4191, loss = 0.11291863\n",
      "Iteration 4192, loss = 0.11289272\n",
      "Iteration 4193, loss = 0.11286683\n",
      "Iteration 4194, loss = 0.11284097\n",
      "Iteration 4195, loss = 0.11281513\n",
      "Iteration 4196, loss = 0.11278930\n",
      "Iteration 4197, loss = 0.11276349\n",
      "Iteration 4198, loss = 0.11273770\n",
      "Iteration 4199, loss = 0.11271194\n",
      "Iteration 4200, loss = 0.11268619\n",
      "Iteration 4201, loss = 0.11266046\n",
      "Iteration 4202, loss = 0.11263475\n",
      "Iteration 4203, loss = 0.11260906\n",
      "Iteration 4204, loss = 0.11258339\n",
      "Iteration 4205, loss = 0.11255773\n",
      "Iteration 4206, loss = 0.11253210\n",
      "Iteration 4207, loss = 0.11250649\n",
      "Iteration 4208, loss = 0.11248089\n",
      "Iteration 4209, loss = 0.11245532\n",
      "Iteration 4210, loss = 0.11242976\n",
      "Iteration 4211, loss = 0.11240422\n",
      "Iteration 4212, loss = 0.11237870\n",
      "Iteration 4213, loss = 0.11235320\n",
      "Iteration 4214, loss = 0.11232772\n",
      "Iteration 4215, loss = 0.11230226\n",
      "Iteration 4216, loss = 0.11227682\n",
      "Iteration 4217, loss = 0.11225139\n",
      "Iteration 4218, loss = 0.11222599\n",
      "Iteration 4219, loss = 0.11220060\n",
      "Iteration 4220, loss = 0.11217524\n",
      "Iteration 4221, loss = 0.11214989\n",
      "Iteration 4222, loss = 0.11212456\n",
      "Iteration 4223, loss = 0.11209925\n",
      "Iteration 4224, loss = 0.11207396\n",
      "Iteration 4225, loss = 0.11204869\n",
      "Iteration 4226, loss = 0.11202343\n",
      "Iteration 4227, loss = 0.11199820\n",
      "Iteration 4228, loss = 0.11197298\n",
      "Iteration 4229, loss = 0.11194779\n",
      "Iteration 4230, loss = 0.11192261\n",
      "Iteration 4231, loss = 0.11189745\n",
      "Iteration 4232, loss = 0.11187231\n",
      "Iteration 4233, loss = 0.11184719\n",
      "Iteration 4234, loss = 0.11182208\n",
      "Iteration 4235, loss = 0.11179700\n",
      "Iteration 4236, loss = 0.11177193\n",
      "Iteration 4237, loss = 0.11174689\n",
      "Iteration 4238, loss = 0.11172186\n",
      "Iteration 4239, loss = 0.11169685\n",
      "Iteration 4240, loss = 0.11167186\n",
      "Iteration 4241, loss = 0.11164688\n",
      "Iteration 4242, loss = 0.11162193\n",
      "Iteration 4243, loss = 0.11159699\n",
      "Iteration 4244, loss = 0.11157208\n",
      "Iteration 4245, loss = 0.11154718\n",
      "Iteration 4246, loss = 0.11152230\n",
      "Iteration 4247, loss = 0.11149744\n",
      "Iteration 4248, loss = 0.11147260\n",
      "Iteration 4249, loss = 0.11144777\n",
      "Iteration 4250, loss = 0.11142297\n",
      "Iteration 4251, loss = 0.11139818\n",
      "Iteration 4252, loss = 0.11137341\n",
      "Iteration 4253, loss = 0.11134866\n",
      "Iteration 4254, loss = 0.11132393\n",
      "Iteration 4255, loss = 0.11129922\n",
      "Iteration 4256, loss = 0.11127452\n",
      "Iteration 4257, loss = 0.11124984\n",
      "Iteration 4258, loss = 0.11122519\n",
      "Iteration 4259, loss = 0.11120055\n",
      "Iteration 4260, loss = 0.11117592\n",
      "Iteration 4261, loss = 0.11115132\n",
      "Iteration 4262, loss = 0.11112674\n",
      "Iteration 4263, loss = 0.11110217\n",
      "Iteration 4264, loss = 0.11107762\n",
      "Iteration 4265, loss = 0.11105309\n",
      "Iteration 4266, loss = 0.11102858\n",
      "Iteration 4267, loss = 0.11100409\n",
      "Iteration 4268, loss = 0.11097961\n",
      "Iteration 4269, loss = 0.11095515\n",
      "Iteration 4270, loss = 0.11093072\n",
      "Iteration 4271, loss = 0.11090630\n",
      "Iteration 4272, loss = 0.11088189\n",
      "Iteration 4273, loss = 0.11085751\n",
      "Iteration 4274, loss = 0.11083314\n",
      "Iteration 4275, loss = 0.11080879\n",
      "Iteration 4276, loss = 0.11078446\n",
      "Iteration 4277, loss = 0.11076015\n",
      "Iteration 4278, loss = 0.11073586\n",
      "Iteration 4279, loss = 0.11071158\n",
      "Iteration 4280, loss = 0.11068732\n",
      "Iteration 4281, loss = 0.11066308\n",
      "Iteration 4282, loss = 0.11063886\n",
      "Iteration 4283, loss = 0.11061466\n",
      "Iteration 4284, loss = 0.11059047\n",
      "Iteration 4285, loss = 0.11056631\n",
      "Iteration 4286, loss = 0.11054216\n",
      "Iteration 4287, loss = 0.11051802\n",
      "Iteration 4288, loss = 0.11049391\n",
      "Iteration 4289, loss = 0.11046981\n",
      "Iteration 4290, loss = 0.11044574\n",
      "Iteration 4291, loss = 0.11042168\n",
      "Iteration 4292, loss = 0.11039763\n",
      "Iteration 4293, loss = 0.11037361\n",
      "Iteration 4294, loss = 0.11034960\n",
      "Iteration 4295, loss = 0.11032561\n",
      "Iteration 4296, loss = 0.11030164\n",
      "Iteration 4297, loss = 0.11027769\n",
      "Iteration 4298, loss = 0.11025376\n",
      "Iteration 4299, loss = 0.11022984\n",
      "Iteration 4300, loss = 0.11020594\n",
      "Iteration 4301, loss = 0.11018206\n",
      "Iteration 4302, loss = 0.11015819\n",
      "Iteration 4303, loss = 0.11013435\n",
      "Iteration 4304, loss = 0.11011052\n",
      "Iteration 4305, loss = 0.11008671\n",
      "Iteration 4306, loss = 0.11006291\n",
      "Iteration 4307, loss = 0.11003914\n",
      "Iteration 4308, loss = 0.11001538\n",
      "Iteration 4309, loss = 0.10999164\n",
      "Iteration 4310, loss = 0.10996791\n",
      "Iteration 4311, loss = 0.10994421\n",
      "Iteration 4312, loss = 0.10992052\n",
      "Iteration 4313, loss = 0.10989685\n",
      "Iteration 4314, loss = 0.10987320\n",
      "Iteration 4315, loss = 0.10984956\n",
      "Iteration 4316, loss = 0.10982595\n",
      "Iteration 4317, loss = 0.10980235\n",
      "Iteration 4318, loss = 0.10977876\n",
      "Iteration 4319, loss = 0.10975520\n",
      "Iteration 4320, loss = 0.10973165\n",
      "Iteration 4321, loss = 0.10970812\n",
      "Iteration 4322, loss = 0.10968461\n",
      "Iteration 4323, loss = 0.10966111\n",
      "Iteration 4324, loss = 0.10963764\n",
      "Iteration 4325, loss = 0.10961418\n",
      "Iteration 4326, loss = 0.10959073\n",
      "Iteration 4327, loss = 0.10956731\n",
      "Iteration 4328, loss = 0.10954390\n",
      "Iteration 4329, loss = 0.10952051\n",
      "Iteration 4330, loss = 0.10949713\n",
      "Iteration 4331, loss = 0.10947378\n",
      "Iteration 4332, loss = 0.10945044\n",
      "Iteration 4333, loss = 0.10942712\n",
      "Iteration 4334, loss = 0.10940381\n",
      "Iteration 4335, loss = 0.10938053\n",
      "Iteration 4336, loss = 0.10935726\n",
      "Iteration 4337, loss = 0.10933401\n",
      "Iteration 4338, loss = 0.10931077\n",
      "Iteration 4339, loss = 0.10928755\n",
      "Iteration 4340, loss = 0.10926435\n",
      "Iteration 4341, loss = 0.10924117\n",
      "Iteration 4342, loss = 0.10921800\n",
      "Iteration 4343, loss = 0.10919485\n",
      "Iteration 4344, loss = 0.10917172\n",
      "Iteration 4345, loss = 0.10914860\n",
      "Iteration 4346, loss = 0.10912551\n",
      "Iteration 4347, loss = 0.10910243\n",
      "Iteration 4348, loss = 0.10907936\n",
      "Iteration 4349, loss = 0.10905632\n",
      "Iteration 4350, loss = 0.10903329\n",
      "Iteration 4351, loss = 0.10901027\n",
      "Iteration 4352, loss = 0.10898728\n",
      "Iteration 4353, loss = 0.10896430\n",
      "Iteration 4354, loss = 0.10894134\n",
      "Iteration 4355, loss = 0.10891839\n",
      "Iteration 4356, loss = 0.10889547\n",
      "Iteration 4357, loss = 0.10887256\n",
      "Iteration 4358, loss = 0.10884966\n",
      "Iteration 4359, loss = 0.10882679\n",
      "Iteration 4360, loss = 0.10880393\n",
      "Iteration 4361, loss = 0.10878108\n",
      "Iteration 4362, loss = 0.10875826\n",
      "Iteration 4363, loss = 0.10873545\n",
      "Iteration 4364, loss = 0.10871266\n",
      "Iteration 4365, loss = 0.10868988\n",
      "Iteration 4366, loss = 0.10866712\n",
      "Iteration 4367, loss = 0.10864438\n",
      "Iteration 4368, loss = 0.10862166\n",
      "Iteration 4369, loss = 0.10859895\n",
      "Iteration 4370, loss = 0.10857626\n",
      "Iteration 4371, loss = 0.10855359\n",
      "Iteration 4372, loss = 0.10853093\n",
      "Iteration 4373, loss = 0.10850829\n",
      "Iteration 4374, loss = 0.10848566\n",
      "Iteration 4375, loss = 0.10846306\n",
      "Iteration 4376, loss = 0.10844047\n",
      "Iteration 4377, loss = 0.10841789\n",
      "Iteration 4378, loss = 0.10839534\n",
      "Iteration 4379, loss = 0.10837279\n",
      "Iteration 4380, loss = 0.10835027\n",
      "Iteration 4381, loss = 0.10832776\n",
      "Iteration 4382, loss = 0.10830527\n",
      "Iteration 4383, loss = 0.10828280\n",
      "Iteration 4384, loss = 0.10826034\n",
      "Iteration 4385, loss = 0.10823790\n",
      "Iteration 4386, loss = 0.10821548\n",
      "Iteration 4387, loss = 0.10819307\n",
      "Iteration 4388, loss = 0.10817068\n",
      "Iteration 4389, loss = 0.10814830\n",
      "Iteration 4390, loss = 0.10812595\n",
      "Iteration 4391, loss = 0.10810361\n",
      "Iteration 4392, loss = 0.10808128\n",
      "Iteration 4393, loss = 0.10805897\n",
      "Iteration 4394, loss = 0.10803668\n",
      "Iteration 4395, loss = 0.10801441\n",
      "Iteration 4396, loss = 0.10799215\n",
      "Iteration 4397, loss = 0.10796990\n",
      "Iteration 4398, loss = 0.10794768\n",
      "Iteration 4399, loss = 0.10792547\n",
      "Iteration 4400, loss = 0.10790328\n",
      "Iteration 4401, loss = 0.10788110\n",
      "Iteration 4402, loss = 0.10785894\n",
      "Iteration 4403, loss = 0.10783679\n",
      "Iteration 4404, loss = 0.10781467\n",
      "Iteration 4405, loss = 0.10779255\n",
      "Iteration 4406, loss = 0.10777046\n",
      "Iteration 4407, loss = 0.10774838\n",
      "Iteration 4408, loss = 0.10772632\n",
      "Iteration 4409, loss = 0.10770427\n",
      "Iteration 4410, loss = 0.10768224\n",
      "Iteration 4411, loss = 0.10766023\n",
      "Iteration 4412, loss = 0.10763823\n",
      "Iteration 4413, loss = 0.10761625\n",
      "Iteration 4414, loss = 0.10759429\n",
      "Iteration 4415, loss = 0.10757234\n",
      "Iteration 4416, loss = 0.10755040\n",
      "Iteration 4417, loss = 0.10752849\n",
      "Iteration 4418, loss = 0.10750659\n",
      "Iteration 4419, loss = 0.10748470\n",
      "Iteration 4420, loss = 0.10746284\n",
      "Iteration 4421, loss = 0.10744098\n",
      "Iteration 4422, loss = 0.10741915\n",
      "Iteration 4423, loss = 0.10739733\n",
      "Iteration 4424, loss = 0.10737553\n",
      "Iteration 4425, loss = 0.10735374\n",
      "Iteration 4426, loss = 0.10733197\n",
      "Iteration 4427, loss = 0.10731021\n",
      "Iteration 4428, loss = 0.10728847\n",
      "Iteration 4429, loss = 0.10726675\n",
      "Iteration 4430, loss = 0.10724504\n",
      "Iteration 4431, loss = 0.10722335\n",
      "Iteration 4432, loss = 0.10720168\n",
      "Iteration 4433, loss = 0.10718002\n",
      "Iteration 4434, loss = 0.10715837\n",
      "Iteration 4435, loss = 0.10713675\n",
      "Iteration 4436, loss = 0.10711513\n",
      "Iteration 4437, loss = 0.10709354\n",
      "Iteration 4438, loss = 0.10707196\n",
      "Iteration 4439, loss = 0.10705040\n",
      "Iteration 4440, loss = 0.10702885\n",
      "Iteration 4441, loss = 0.10700732\n",
      "Iteration 4442, loss = 0.10698580\n",
      "Iteration 4443, loss = 0.10696430\n",
      "Iteration 4444, loss = 0.10694282\n",
      "Iteration 4445, loss = 0.10692135\n",
      "Iteration 4446, loss = 0.10689990\n",
      "Iteration 4447, loss = 0.10687846\n",
      "Iteration 4448, loss = 0.10685704\n",
      "Iteration 4449, loss = 0.10683563\n",
      "Iteration 4450, loss = 0.10681424\n",
      "Iteration 4451, loss = 0.10679287\n",
      "Iteration 4452, loss = 0.10677151\n",
      "Iteration 4453, loss = 0.10675017\n",
      "Iteration 4454, loss = 0.10672884\n",
      "Iteration 4455, loss = 0.10670753\n",
      "Iteration 4456, loss = 0.10668624\n",
      "Iteration 4457, loss = 0.10666496\n",
      "Iteration 4458, loss = 0.10664369\n",
      "Iteration 4459, loss = 0.10662244\n",
      "Iteration 4460, loss = 0.10660121\n",
      "Iteration 4461, loss = 0.10658000\n",
      "Iteration 4462, loss = 0.10655879\n",
      "Iteration 4463, loss = 0.10653761\n",
      "Iteration 4464, loss = 0.10651644\n",
      "Iteration 4465, loss = 0.10649528\n",
      "Iteration 4466, loss = 0.10647414\n",
      "Iteration 4467, loss = 0.10645302\n",
      "Iteration 4468, loss = 0.10643191\n",
      "Iteration 4469, loss = 0.10641082\n",
      "Iteration 4470, loss = 0.10638974\n",
      "Iteration 4471, loss = 0.10636868\n",
      "Iteration 4472, loss = 0.10634764\n",
      "Iteration 4473, loss = 0.10632661\n",
      "Iteration 4474, loss = 0.10630559\n",
      "Iteration 4475, loss = 0.10628460\n",
      "Iteration 4476, loss = 0.10626361\n",
      "Iteration 4477, loss = 0.10624265\n",
      "Iteration 4478, loss = 0.10622171\n",
      "Iteration 4479, loss = 0.10620080\n",
      "Iteration 4480, loss = 0.10617992\n",
      "Iteration 4481, loss = 0.10615910\n",
      "Iteration 4482, loss = 0.10613838\n",
      "Iteration 4483, loss = 0.10611778\n",
      "Iteration 4484, loss = 0.10609735\n",
      "Iteration 4485, loss = 0.10607686\n",
      "Iteration 4486, loss = 0.10605603\n",
      "Iteration 4487, loss = 0.10603458\n",
      "Iteration 4488, loss = 0.10601312\n",
      "Iteration 4489, loss = 0.10599232\n",
      "Iteration 4490, loss = 0.10597201\n",
      "Iteration 4491, loss = 0.10595146\n",
      "Iteration 4492, loss = 0.10593034\n",
      "Iteration 4493, loss = 0.10590927\n",
      "Iteration 4494, loss = 0.10588873\n",
      "Iteration 4495, loss = 0.10586831\n",
      "Iteration 4496, loss = 0.10584752\n",
      "Iteration 4497, loss = 0.10582658\n",
      "Iteration 4498, loss = 0.10580596\n",
      "Iteration 4499, loss = 0.10578554\n",
      "Iteration 4500, loss = 0.10576489\n",
      "Iteration 4501, loss = 0.10574410\n",
      "Iteration 4502, loss = 0.10572351\n",
      "Iteration 4503, loss = 0.10570308\n",
      "Iteration 4504, loss = 0.10568252\n",
      "Iteration 4505, loss = 0.10566186\n",
      "Iteration 4506, loss = 0.10564133\n",
      "Iteration 4507, loss = 0.10562091\n",
      "Iteration 4508, loss = 0.10560042\n",
      "Iteration 4509, loss = 0.10557986\n",
      "Iteration 4510, loss = 0.10555939\n",
      "Iteration 4511, loss = 0.10553900\n",
      "Iteration 4512, loss = 0.10551857\n",
      "Iteration 4513, loss = 0.10549810\n",
      "Iteration 4514, loss = 0.10547770\n",
      "Iteration 4515, loss = 0.10545735\n",
      "Iteration 4516, loss = 0.10543697\n",
      "Iteration 4517, loss = 0.10541658\n",
      "Iteration 4518, loss = 0.10539624\n",
      "Iteration 4519, loss = 0.10537594\n",
      "Iteration 4520, loss = 0.10535563\n",
      "Iteration 4521, loss = 0.10533531\n",
      "Iteration 4522, loss = 0.10531503\n",
      "Iteration 4523, loss = 0.10529478\n",
      "Iteration 4524, loss = 0.10527452\n",
      "Iteration 4525, loss = 0.10525427\n",
      "Iteration 4526, loss = 0.10523405\n",
      "Iteration 4527, loss = 0.10521385\n",
      "Iteration 4528, loss = 0.10519366\n",
      "Iteration 4529, loss = 0.10517347\n",
      "Iteration 4530, loss = 0.10515331\n",
      "Iteration 4531, loss = 0.10513317\n",
      "Iteration 4532, loss = 0.10511304\n",
      "Iteration 4533, loss = 0.10509291\n",
      "Iteration 4534, loss = 0.10507281\n",
      "Iteration 4535, loss = 0.10505272\n",
      "Iteration 4536, loss = 0.10503265\n",
      "Iteration 4537, loss = 0.10501259\n",
      "Iteration 4538, loss = 0.10499254\n",
      "Iteration 4539, loss = 0.10497251\n",
      "Iteration 4540, loss = 0.10495250\n",
      "Iteration 4541, loss = 0.10493250\n",
      "Iteration 4542, loss = 0.10491251\n",
      "Iteration 4543, loss = 0.10489254\n",
      "Iteration 4544, loss = 0.10487258\n",
      "Iteration 4545, loss = 0.10485264\n",
      "Iteration 4546, loss = 0.10483271\n",
      "Iteration 4547, loss = 0.10481280\n",
      "Iteration 4548, loss = 0.10479290\n",
      "Iteration 4549, loss = 0.10477302\n",
      "Iteration 4550, loss = 0.10475315\n",
      "Iteration 4551, loss = 0.10473330\n",
      "Iteration 4552, loss = 0.10471346\n",
      "Iteration 4553, loss = 0.10469363\n",
      "Iteration 4554, loss = 0.10467382\n",
      "Iteration 4555, loss = 0.10465402\n",
      "Iteration 4556, loss = 0.10463424\n",
      "Iteration 4557, loss = 0.10461447\n",
      "Iteration 4558, loss = 0.10459472\n",
      "Iteration 4559, loss = 0.10457498\n",
      "Iteration 4560, loss = 0.10455526\n",
      "Iteration 4561, loss = 0.10453555\n",
      "Iteration 4562, loss = 0.10451585\n",
      "Iteration 4563, loss = 0.10449617\n",
      "Iteration 4564, loss = 0.10447651\n",
      "Iteration 4565, loss = 0.10445685\n",
      "Iteration 4566, loss = 0.10443722\n",
      "Iteration 4567, loss = 0.10441759\n",
      "Iteration 4568, loss = 0.10439798\n",
      "Iteration 4569, loss = 0.10437839\n",
      "Iteration 4570, loss = 0.10435881\n",
      "Iteration 4571, loss = 0.10433924\n",
      "Iteration 4572, loss = 0.10431969\n",
      "Iteration 4573, loss = 0.10430016\n",
      "Iteration 4574, loss = 0.10428063\n",
      "Iteration 4575, loss = 0.10426112\n",
      "Iteration 4576, loss = 0.10424163\n",
      "Iteration 4577, loss = 0.10422215\n",
      "Iteration 4578, loss = 0.10420268\n",
      "Iteration 4579, loss = 0.10418323\n",
      "Iteration 4580, loss = 0.10416379\n",
      "Iteration 4581, loss = 0.10414437\n",
      "Iteration 4582, loss = 0.10412496\n",
      "Iteration 4583, loss = 0.10410557\n",
      "Iteration 4584, loss = 0.10408619\n",
      "Iteration 4585, loss = 0.10406682\n",
      "Iteration 4586, loss = 0.10404747\n",
      "Iteration 4587, loss = 0.10402813\n",
      "Iteration 4588, loss = 0.10400881\n",
      "Iteration 4589, loss = 0.10398950\n",
      "Iteration 4590, loss = 0.10397020\n",
      "Iteration 4591, loss = 0.10395092\n",
      "Iteration 4592, loss = 0.10393165\n",
      "Iteration 4593, loss = 0.10391240\n",
      "Iteration 4594, loss = 0.10389316\n",
      "Iteration 4595, loss = 0.10387393\n",
      "Iteration 4596, loss = 0.10385472\n",
      "Iteration 4597, loss = 0.10383552\n",
      "Iteration 4598, loss = 0.10381634\n",
      "Iteration 4599, loss = 0.10379717\n",
      "Iteration 4600, loss = 0.10377801\n",
      "Iteration 4601, loss = 0.10375887\n",
      "Iteration 4602, loss = 0.10373975\n",
      "Iteration 4603, loss = 0.10372063\n",
      "Iteration 4604, loss = 0.10370153\n",
      "Iteration 4605, loss = 0.10368245\n",
      "Iteration 4606, loss = 0.10366338\n",
      "Iteration 4607, loss = 0.10364432\n",
      "Iteration 4608, loss = 0.10362527\n",
      "Iteration 4609, loss = 0.10360625\n",
      "Iteration 4610, loss = 0.10358723\n",
      "Iteration 4611, loss = 0.10356823\n",
      "Iteration 4612, loss = 0.10354924\n",
      "Iteration 4613, loss = 0.10353026\n",
      "Iteration 4614, loss = 0.10351130\n",
      "Iteration 4615, loss = 0.10349236\n",
      "Iteration 4616, loss = 0.10347342\n",
      "Iteration 4617, loss = 0.10345451\n",
      "Iteration 4618, loss = 0.10343560\n",
      "Iteration 4619, loss = 0.10341671\n",
      "Iteration 4620, loss = 0.10339783\n",
      "Iteration 4621, loss = 0.10337897\n",
      "Iteration 4622, loss = 0.10336012\n",
      "Iteration 4623, loss = 0.10334128\n",
      "Iteration 4624, loss = 0.10332246\n",
      "Iteration 4625, loss = 0.10330365\n",
      "Iteration 4626, loss = 0.10328486\n",
      "Iteration 4627, loss = 0.10326607\n",
      "Iteration 4628, loss = 0.10324731\n",
      "Iteration 4629, loss = 0.10322855\n",
      "Iteration 4630, loss = 0.10320981\n",
      "Iteration 4631, loss = 0.10319109\n",
      "Iteration 4632, loss = 0.10317237\n",
      "Iteration 4633, loss = 0.10315367\n",
      "Iteration 4634, loss = 0.10313499\n",
      "Iteration 4635, loss = 0.10311632\n",
      "Iteration 4636, loss = 0.10309766\n",
      "Iteration 4637, loss = 0.10307901\n",
      "Iteration 4638, loss = 0.10306038\n",
      "Iteration 4639, loss = 0.10304176\n",
      "Iteration 4640, loss = 0.10302316\n",
      "Iteration 4641, loss = 0.10300457\n",
      "Iteration 4642, loss = 0.10298599\n",
      "Iteration 4643, loss = 0.10296743\n",
      "Iteration 4644, loss = 0.10294888\n",
      "Iteration 4645, loss = 0.10293034\n",
      "Iteration 4646, loss = 0.10291182\n",
      "Iteration 4647, loss = 0.10289331\n",
      "Iteration 4648, loss = 0.10287481\n",
      "Iteration 4649, loss = 0.10285633\n",
      "Iteration 4650, loss = 0.10283786\n",
      "Iteration 4651, loss = 0.10281941\n",
      "Iteration 4652, loss = 0.10280097\n",
      "Iteration 4653, loss = 0.10278254\n",
      "Iteration 4654, loss = 0.10276412\n",
      "Iteration 4655, loss = 0.10274572\n",
      "Iteration 4656, loss = 0.10272733\n",
      "Iteration 4657, loss = 0.10270896\n",
      "Iteration 4658, loss = 0.10269059\n",
      "Iteration 4659, loss = 0.10267225\n",
      "Iteration 4660, loss = 0.10265391\n",
      "Iteration 4661, loss = 0.10263559\n",
      "Iteration 4662, loss = 0.10261728\n",
      "Iteration 4663, loss = 0.10259899\n",
      "Iteration 4664, loss = 0.10258071\n",
      "Iteration 4665, loss = 0.10256244\n",
      "Iteration 4666, loss = 0.10254418\n",
      "Iteration 4667, loss = 0.10252594\n",
      "Iteration 4668, loss = 0.10250771\n",
      "Iteration 4669, loss = 0.10248950\n",
      "Iteration 4670, loss = 0.10247130\n",
      "Iteration 4671, loss = 0.10245311\n",
      "Iteration 4672, loss = 0.10243493\n",
      "Iteration 4673, loss = 0.10241677\n",
      "Iteration 4674, loss = 0.10239862\n",
      "Iteration 4675, loss = 0.10238049\n",
      "Iteration 4676, loss = 0.10236236\n",
      "Iteration 4677, loss = 0.10234425\n",
      "Iteration 4678, loss = 0.10232616\n",
      "Iteration 4679, loss = 0.10230808\n",
      "Iteration 4680, loss = 0.10229001\n",
      "Iteration 4681, loss = 0.10227195\n",
      "Iteration 4682, loss = 0.10225391\n",
      "Iteration 4683, loss = 0.10223588\n",
      "Iteration 4684, loss = 0.10221786\n",
      "Iteration 4685, loss = 0.10219985\n",
      "Iteration 4686, loss = 0.10218186\n",
      "Iteration 4687, loss = 0.10216389\n",
      "Iteration 4688, loss = 0.10214592\n",
      "Iteration 4689, loss = 0.10212797\n",
      "Iteration 4690, loss = 0.10211003\n",
      "Iteration 4691, loss = 0.10209210\n",
      "Iteration 4692, loss = 0.10207419\n",
      "Iteration 4693, loss = 0.10205629\n",
      "Iteration 4694, loss = 0.10203841\n",
      "Iteration 4695, loss = 0.10202053\n",
      "Iteration 4696, loss = 0.10200267\n",
      "Iteration 4697, loss = 0.10198482\n",
      "Iteration 4698, loss = 0.10196699\n",
      "Iteration 4699, loss = 0.10194917\n",
      "Iteration 4700, loss = 0.10193136\n",
      "Iteration 4701, loss = 0.10191356\n",
      "Iteration 4702, loss = 0.10189578\n",
      "Iteration 4703, loss = 0.10187801\n",
      "Iteration 4704, loss = 0.10186025\n",
      "Iteration 4705, loss = 0.10184251\n",
      "Iteration 4706, loss = 0.10182478\n",
      "Iteration 4707, loss = 0.10180706\n",
      "Iteration 4708, loss = 0.10178936\n",
      "Iteration 4709, loss = 0.10177166\n",
      "Iteration 4710, loss = 0.10175398\n",
      "Iteration 4711, loss = 0.10173632\n",
      "Iteration 4712, loss = 0.10171866\n",
      "Iteration 4713, loss = 0.10170102\n",
      "Iteration 4714, loss = 0.10168339\n",
      "Iteration 4715, loss = 0.10166578\n",
      "Iteration 4716, loss = 0.10164818\n",
      "Iteration 4717, loss = 0.10163059\n",
      "Iteration 4718, loss = 0.10161301\n",
      "Iteration 4719, loss = 0.10159544\n",
      "Iteration 4720, loss = 0.10157789\n",
      "Iteration 4721, loss = 0.10156035\n",
      "Iteration 4722, loss = 0.10154283\n",
      "Iteration 4723, loss = 0.10152531\n",
      "Iteration 4724, loss = 0.10150781\n",
      "Iteration 4725, loss = 0.10149033\n",
      "Iteration 4726, loss = 0.10147285\n",
      "Iteration 4727, loss = 0.10145539\n",
      "Iteration 4728, loss = 0.10143794\n",
      "Iteration 4729, loss = 0.10142050\n",
      "Iteration 4730, loss = 0.10140308\n",
      "Iteration 4731, loss = 0.10138567\n",
      "Iteration 4732, loss = 0.10136827\n",
      "Iteration 4733, loss = 0.10135088\n",
      "Iteration 4734, loss = 0.10133351\n",
      "Iteration 4735, loss = 0.10131614\n",
      "Iteration 4736, loss = 0.10129880\n",
      "Iteration 4737, loss = 0.10128146\n",
      "Iteration 4738, loss = 0.10126414\n",
      "Iteration 4739, loss = 0.10124682\n",
      "Iteration 4740, loss = 0.10122953\n",
      "Iteration 4741, loss = 0.10121224\n",
      "Iteration 4742, loss = 0.10119497\n",
      "Iteration 4743, loss = 0.10117771\n",
      "Iteration 4744, loss = 0.10116046\n",
      "Iteration 4745, loss = 0.10114322\n",
      "Iteration 4746, loss = 0.10112600\n",
      "Iteration 4747, loss = 0.10110879\n",
      "Iteration 4748, loss = 0.10109159\n",
      "Iteration 4749, loss = 0.10107440\n",
      "Iteration 4750, loss = 0.10105723\n",
      "Iteration 4751, loss = 0.10104007\n",
      "Iteration 4752, loss = 0.10102292\n",
      "Iteration 4753, loss = 0.10100579\n",
      "Iteration 4754, loss = 0.10098866\n",
      "Iteration 4755, loss = 0.10097155\n",
      "Iteration 4756, loss = 0.10095445\n",
      "Iteration 4757, loss = 0.10093737\n",
      "Iteration 4758, loss = 0.10092029\n",
      "Iteration 4759, loss = 0.10090323\n",
      "Iteration 4760, loss = 0.10088618\n",
      "Iteration 4761, loss = 0.10086914\n",
      "Iteration 4762, loss = 0.10085212\n",
      "Iteration 4763, loss = 0.10083511\n",
      "Iteration 4764, loss = 0.10081811\n",
      "Iteration 4765, loss = 0.10080112\n",
      "Iteration 4766, loss = 0.10078415\n",
      "Iteration 4767, loss = 0.10076718\n",
      "Iteration 4768, loss = 0.10075023\n",
      "Iteration 4769, loss = 0.10073329\n",
      "Iteration 4770, loss = 0.10071637\n",
      "Iteration 4771, loss = 0.10069946\n",
      "Iteration 4772, loss = 0.10068255\n",
      "Iteration 4773, loss = 0.10066566\n",
      "Iteration 4774, loss = 0.10064879\n",
      "Iteration 4775, loss = 0.10063192\n",
      "Iteration 4776, loss = 0.10061507\n",
      "Iteration 4777, loss = 0.10059823\n",
      "Iteration 4778, loss = 0.10058140\n",
      "Iteration 4779, loss = 0.10056459\n",
      "Iteration 4780, loss = 0.10054778\n",
      "Iteration 4781, loss = 0.10053099\n",
      "Iteration 4782, loss = 0.10051421\n",
      "Iteration 4783, loss = 0.10049745\n",
      "Iteration 4784, loss = 0.10048069\n",
      "Iteration 4785, loss = 0.10046395\n",
      "Iteration 4786, loss = 0.10044722\n",
      "Iteration 4787, loss = 0.10043050\n",
      "Iteration 4788, loss = 0.10041379\n",
      "Iteration 4789, loss = 0.10039710\n",
      "Iteration 4790, loss = 0.10038041\n",
      "Iteration 4791, loss = 0.10036374\n",
      "Iteration 4792, loss = 0.10034709\n",
      "Iteration 4793, loss = 0.10033044\n",
      "Iteration 4794, loss = 0.10031381\n",
      "Iteration 4795, loss = 0.10029718\n",
      "Iteration 4796, loss = 0.10028057\n",
      "Iteration 4797, loss = 0.10026398\n",
      "Iteration 4798, loss = 0.10024739\n",
      "Iteration 4799, loss = 0.10023082\n",
      "Iteration 4800, loss = 0.10021425\n",
      "Iteration 4801, loss = 0.10019771\n",
      "Iteration 4802, loss = 0.10018117\n",
      "Iteration 4803, loss = 0.10016464\n",
      "Iteration 4804, loss = 0.10014813\n",
      "Iteration 4805, loss = 0.10013163\n",
      "Iteration 4806, loss = 0.10011514\n",
      "Iteration 4807, loss = 0.10009866\n",
      "Iteration 4808, loss = 0.10008219\n",
      "Iteration 4809, loss = 0.10006574\n",
      "Iteration 4810, loss = 0.10004929\n",
      "Iteration 4811, loss = 0.10003286\n",
      "Iteration 4812, loss = 0.10001645\n",
      "Iteration 4813, loss = 0.10000004\n",
      "Iteration 4814, loss = 0.09998364\n",
      "Iteration 4815, loss = 0.09996726\n",
      "Iteration 4816, loss = 0.09995089\n",
      "Iteration 4817, loss = 0.09993453\n",
      "Iteration 4818, loss = 0.09991818\n",
      "Iteration 4819, loss = 0.09990185\n",
      "Iteration 4820, loss = 0.09988552\n",
      "Iteration 4821, loss = 0.09986921\n",
      "Iteration 4822, loss = 0.09985291\n",
      "Iteration 4823, loss = 0.09983662\n",
      "Iteration 4824, loss = 0.09982035\n",
      "Iteration 4825, loss = 0.09980408\n",
      "Iteration 4826, loss = 0.09978783\n",
      "Iteration 4827, loss = 0.09977159\n",
      "Iteration 4828, loss = 0.09975536\n",
      "Iteration 4829, loss = 0.09973914\n",
      "Iteration 4830, loss = 0.09972294\n",
      "Iteration 4831, loss = 0.09970675\n",
      "Iteration 4832, loss = 0.09969058\n",
      "Iteration 4833, loss = 0.09967442\n",
      "Iteration 4834, loss = 0.09965830\n",
      "Iteration 4835, loss = 0.09964223\n",
      "Iteration 4836, loss = 0.09962622\n",
      "Iteration 4837, loss = 0.09961034\n",
      "Iteration 4838, loss = 0.09959459\n",
      "Iteration 4839, loss = 0.09957895\n",
      "Iteration 4840, loss = 0.09956305\n",
      "Iteration 4841, loss = 0.09954662\n",
      "Iteration 4842, loss = 0.09952978\n",
      "Iteration 4843, loss = 0.09951333\n",
      "Iteration 4844, loss = 0.09949759\n",
      "Iteration 4845, loss = 0.09948199\n",
      "Iteration 4846, loss = 0.09946588\n",
      "Iteration 4847, loss = 0.09944941\n",
      "Iteration 4848, loss = 0.09943327\n",
      "Iteration 4849, loss = 0.09941758\n",
      "Iteration 4850, loss = 0.09940176\n",
      "Iteration 4851, loss = 0.09938556\n",
      "Iteration 4852, loss = 0.09936940\n",
      "Iteration 4853, loss = 0.09935359\n",
      "Iteration 4854, loss = 0.09933783\n",
      "Iteration 4855, loss = 0.09932181\n",
      "Iteration 4856, loss = 0.09930574\n",
      "Iteration 4857, loss = 0.09928991\n",
      "Iteration 4858, loss = 0.09927415\n",
      "Iteration 4859, loss = 0.09925823\n",
      "Iteration 4860, loss = 0.09924226\n",
      "Iteration 4861, loss = 0.09922644\n",
      "Iteration 4862, loss = 0.09921069\n",
      "Iteration 4863, loss = 0.09919485\n",
      "Iteration 4864, loss = 0.09917896\n",
      "Iteration 4865, loss = 0.09916317\n",
      "Iteration 4866, loss = 0.09914744\n",
      "Iteration 4867, loss = 0.09913166\n",
      "Iteration 4868, loss = 0.09911584\n",
      "Iteration 4869, loss = 0.09910009\n",
      "Iteration 4870, loss = 0.09908439\n",
      "Iteration 4871, loss = 0.09906866\n",
      "Iteration 4872, loss = 0.09905291\n",
      "Iteration 4873, loss = 0.09903720\n",
      "Iteration 4874, loss = 0.09902153\n",
      "Iteration 4875, loss = 0.09900585\n",
      "Iteration 4876, loss = 0.09899015\n",
      "Iteration 4877, loss = 0.09897449\n",
      "Iteration 4878, loss = 0.09895886\n",
      "Iteration 4879, loss = 0.09894322\n",
      "Iteration 4880, loss = 0.09892758\n",
      "Iteration 4881, loss = 0.09891196\n",
      "Iteration 4882, loss = 0.09889637\n",
      "Iteration 4883, loss = 0.09888078\n",
      "Iteration 4884, loss = 0.09886519\n",
      "Iteration 4885, loss = 0.09884961\n",
      "Iteration 4886, loss = 0.09883406\n",
      "Iteration 4887, loss = 0.09881852\n",
      "Iteration 4888, loss = 0.09880298\n",
      "Iteration 4889, loss = 0.09878745\n",
      "Iteration 4890, loss = 0.09877194\n",
      "Iteration 4891, loss = 0.09875644\n",
      "Iteration 4892, loss = 0.09874094\n",
      "Iteration 4893, loss = 0.09872546\n",
      "Iteration 4894, loss = 0.09870999\n",
      "Iteration 4895, loss = 0.09869454\n",
      "Iteration 4896, loss = 0.09867909\n",
      "Iteration 4897, loss = 0.09866365\n",
      "Iteration 4898, loss = 0.09864823\n",
      "Iteration 4899, loss = 0.09863282\n",
      "Iteration 4900, loss = 0.09861742\n",
      "Iteration 4901, loss = 0.09860202\n",
      "Iteration 4902, loss = 0.09858664\n",
      "Iteration 4903, loss = 0.09857127\n",
      "Iteration 4904, loss = 0.09855592\n",
      "Iteration 4905, loss = 0.09854057\n",
      "Iteration 4906, loss = 0.09852523\n",
      "Iteration 4907, loss = 0.09850991\n",
      "Iteration 4908, loss = 0.09849460\n",
      "Iteration 4909, loss = 0.09847929\n",
      "Iteration 4910, loss = 0.09846400\n",
      "Iteration 4911, loss = 0.09844872\n",
      "Iteration 4912, loss = 0.09843345\n",
      "Iteration 4913, loss = 0.09841819\n",
      "Iteration 4914, loss = 0.09840295\n",
      "Iteration 4915, loss = 0.09838771\n",
      "Iteration 4916, loss = 0.09837249\n",
      "Iteration 4917, loss = 0.09835727\n",
      "Iteration 4918, loss = 0.09834207\n",
      "Iteration 4919, loss = 0.09832688\n",
      "Iteration 4920, loss = 0.09831169\n",
      "Iteration 4921, loss = 0.09829652\n",
      "Iteration 4922, loss = 0.09828136\n",
      "Iteration 4923, loss = 0.09826621\n",
      "Iteration 4924, loss = 0.09825108\n",
      "Iteration 4925, loss = 0.09823595\n",
      "Iteration 4926, loss = 0.09822083\n",
      "Iteration 4927, loss = 0.09820573\n",
      "Iteration 4928, loss = 0.09819063\n",
      "Iteration 4929, loss = 0.09817555\n",
      "Iteration 4930, loss = 0.09816048\n",
      "Iteration 4931, loss = 0.09814542\n",
      "Iteration 4932, loss = 0.09813037\n",
      "Iteration 4933, loss = 0.09811533\n",
      "Iteration 4934, loss = 0.09810030\n",
      "Iteration 4935, loss = 0.09808528\n",
      "Iteration 4936, loss = 0.09807027\n",
      "Iteration 4937, loss = 0.09805527\n",
      "Iteration 4938, loss = 0.09804029\n",
      "Iteration 4939, loss = 0.09802531\n",
      "Iteration 4940, loss = 0.09801035\n",
      "Iteration 4941, loss = 0.09799540\n",
      "Iteration 4942, loss = 0.09798045\n",
      "Iteration 4943, loss = 0.09796552\n",
      "Iteration 4944, loss = 0.09795060\n",
      "Iteration 4945, loss = 0.09793569\n",
      "Iteration 4946, loss = 0.09792079\n",
      "Iteration 4947, loss = 0.09790590\n",
      "Iteration 4948, loss = 0.09789102\n",
      "Iteration 4949, loss = 0.09787616\n",
      "Iteration 4950, loss = 0.09786130\n",
      "Iteration 4951, loss = 0.09784645\n",
      "Iteration 4952, loss = 0.09783162\n",
      "Iteration 4953, loss = 0.09781679\n",
      "Iteration 4954, loss = 0.09780198\n",
      "Iteration 4955, loss = 0.09778717\n",
      "Iteration 4956, loss = 0.09777238\n",
      "Iteration 4957, loss = 0.09775760\n",
      "Iteration 4958, loss = 0.09774283\n",
      "Iteration 4959, loss = 0.09772807\n",
      "Iteration 4960, loss = 0.09771332\n",
      "Iteration 4961, loss = 0.09769858\n",
      "Iteration 4962, loss = 0.09768385\n",
      "Iteration 4963, loss = 0.09766913\n",
      "Iteration 4964, loss = 0.09765442\n",
      "Iteration 4965, loss = 0.09763972\n",
      "Iteration 4966, loss = 0.09762504\n",
      "Iteration 4967, loss = 0.09761036\n",
      "Iteration 4968, loss = 0.09759570\n",
      "Iteration 4969, loss = 0.09758104\n",
      "Iteration 4970, loss = 0.09756640\n",
      "Iteration 4971, loss = 0.09755176\n",
      "Iteration 4972, loss = 0.09753714\n",
      "Iteration 4973, loss = 0.09752253\n",
      "Iteration 4974, loss = 0.09750792\n",
      "Iteration 4975, loss = 0.09749333\n",
      "Iteration 4976, loss = 0.09747875\n",
      "Iteration 4977, loss = 0.09746418\n",
      "Iteration 4978, loss = 0.09744962\n",
      "Iteration 4979, loss = 0.09743507\n",
      "Iteration 4980, loss = 0.09742053\n",
      "Iteration 4981, loss = 0.09740600\n",
      "Iteration 4982, loss = 0.09739148\n",
      "Iteration 4983, loss = 0.09737697\n",
      "Iteration 4984, loss = 0.09736248\n",
      "Iteration 4985, loss = 0.09734799\n",
      "Iteration 4986, loss = 0.09733351\n",
      "Iteration 4987, loss = 0.09731905\n",
      "Iteration 4988, loss = 0.09730459\n",
      "Iteration 4989, loss = 0.09729014\n",
      "Iteration 4990, loss = 0.09727571\n",
      "Iteration 4991, loss = 0.09726128\n",
      "Iteration 4992, loss = 0.09724687\n",
      "Iteration 4993, loss = 0.09723247\n",
      "Iteration 4994, loss = 0.09721807\n",
      "Iteration 4995, loss = 0.09720369\n",
      "Iteration 4996, loss = 0.09718932\n",
      "Iteration 4997, loss = 0.09717495\n",
      "Iteration 4998, loss = 0.09716060\n",
      "Iteration 4999, loss = 0.09714626\n",
      "Iteration 5000, loss = 0.09713193\n",
      "--\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (5000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#  Train the neural network on Iris data\n",
    "#\n",
    "#  From PDL Pg 186\n",
    "#\n",
    "\n",
    "#  The following variables are in scope ..\n",
    "#\n",
    "#     np_iris[\"train_fs\"]\n",
    "#     np_iris[\"test_fs\" ]\n",
    "#\n",
    "#        ^^^^  cols 0 and 1 are features, col 2 is the label\n",
    "#\n",
    "\n",
    "import pickle\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "i_clf = MLPClassifier(\n",
    "   hidden_layer_sizes = (3, 2),\n",
    "   activation         = \"logistic\",\n",
    "   solver             = \"adam\",\n",
    "   tol                = 1e-9,\n",
    "   max_iter           = 5000,\n",
    "   verbose            = True,\n",
    "   )\n",
    "\n",
    "\n",
    "i_clf.fit(np_iris[\"train_fs\"][:, [0, 1]], np_iris[\"train_fs\"][:, 2])\n",
    "l_probability = i_clf.predict_proba(np_iris[\"test_fs\"][:, [0, 1]])\n",
    "\n",
    "\n",
    "print(\"--\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652f3a82-175b-48c1-9ba1-2e92c24d35d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df30ed00-e15b-41c4-afb9-b7f6c949e0c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b37e69-c673-4d62-bfc0-54fe7422b8f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aad863a-124f-4428-a0aa-312936d86212",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0df4b7d-946c-4437-8965-d67a07a089c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3976e37-8702-4f53-bf97-e28a958675f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a397bde-64f0-4ec9-a728-d37e4c51ecea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
