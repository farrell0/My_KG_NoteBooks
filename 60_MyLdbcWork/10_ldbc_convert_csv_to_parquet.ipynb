{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159d7c38-2aaf-4738-b8a5-76f56e572f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  This Notebook does not work in it's current form; mostly LDBC files that have moved ..\n",
    "#\n",
    "#  Code to harvest below; reading a CSV and outputting Parquet.\n",
    "\n",
    "\n",
    "#  CONNECTION HANDLE\n",
    "\n",
    "import os\n",
    "\n",
    "from katana import remote\n",
    "from katana.remote import import_data\n",
    "\n",
    "\n",
    "my_client = remote.Client()\n",
    "\n",
    "print(my_client)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9313e843-4fa3-451b-87ae-4491c7736884",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_PARTITIONS  = 3\n",
    "   #\n",
    "DB_NAME         = \"my_db\"\n",
    "GRAPH_NAME      = \"my_graph\"\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f05b99-33a4-4792-afc2-4eda26546123",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  CONNECT TO GRAPH\n",
    "\n",
    "for l_graph in my_client.get_database(name=DB_NAME).graphs_in_database():\n",
    "   if (l_graph.name == GRAPH_NAME):\n",
    "      my_graph=my_client.get_database(name=DB_NAME).get_graph_by_id(id=l_graph.graph_id)\n",
    "         #\n",
    "      break\n",
    "\n",
    "print(my_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eeb3eb8-01dc-4e11-82c5-1f222072558b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f906b9-e83f-4499-9344-872a2a81e64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  LOAD FROM CSV\n",
    "\n",
    "#  The data files are not in place, fyi ..\n",
    "\n",
    "l_inp = \"gs://farrell-bucket/ldbc/50_csv_sf-0.003-bi-node_headers.txt\"\n",
    "l_enp = \"gs://farrell-bucket/ldbc/51_csv_sf-0.003-bi-edge_headers.txt\"\n",
    "\n",
    "# l_dir = \"gs://katana-demo-datasets/csv-datasets/ldbc\"\n",
    "l_dir = \"gs://farrell-bucket/ldbc\"\n",
    "\n",
    "\n",
    "import_data.csv(\n",
    "   my_graph,\n",
    "      #\n",
    "   input_node_path     = l_inp,\n",
    "   input_edge_path     = l_enp,\n",
    "   input_dir           = l_dir,\n",
    "      #\n",
    "   data_delimiter      = \"|\",\n",
    "   schema_delimiter    = \"|\",\n",
    "   files_have_headers  = True,\n",
    "   )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be368507-fc1d-42e6-b757-8434e2282a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(my_graph.num_nodes())\n",
    "display(my_graph.num_edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e740c7b1-8805-4da4-91a9-849e0dfe4ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0ddcab-fa3a-4993-84e9-f5f439fd6594",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  VARIABLES ONLY\n",
    "\n",
    "import dask.dataframe as dd\n",
    "import numpy as np\n",
    "import os\n",
    "   #\n",
    "from google.cloud import storage\n",
    "   #\n",
    "import pandas as pd\n",
    "import gcsfs\n",
    "\n",
    "\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/home/jovyan/work/03_MyKeys.json\"\n",
    "\n",
    "l_Bucket     = \"farrell-bucket\"\n",
    "\n",
    "l_InputDir  = \"sf-0.003-csv\"                       #  These values should not start with a slash\n",
    "l_OutputDir = \"sf-0.003-parquet\"\n",
    "\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbe540b-b518-40e5-ad7c-fc7816e706e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374ca94a-d57b-4e87-ac87-947c6151d4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  READ CSV, OUTPUT PARQUET\n",
    "\n",
    "def f_ReadCsvOutputParquet(i_Bucket, i_File, i_OutputDir):\n",
    "\n",
    "   l_File = \"gs://\" + i_Bucket + \"/\" + i_File\n",
    "      #\n",
    "   # print(\"Processing: \" + i_File)\n",
    "   print(\".\", end=\"\")\n",
    "\n",
    "   df = dd.read_csv(l_File,\n",
    "      delimiter='|',\n",
    "      dtype={\n",
    "      \"id\": np.dtype(\"O\"),\n",
    "      \"name\": np.dtype(\"O\"),\n",
    "      \"url\": np.dtype(\"O\"),\n",
    "         #\n",
    "      # \"creationDate\": np.dtype(\"O\"),\n",
    "      \"locationIP\": np.dtype(\"O\"),\n",
    "      \"browserUsed\": np.dtype(\"O\"),\n",
    "      \"content\": np.dtype(\"O\"),\n",
    "      \"length\": np.dtype(\"O\"),\n",
    "         #\n",
    "      \"imageFile\": np.dtype(\"O\"),\n",
    "      \"language\": np.dtype(\"O\"),\n",
    "         #\n",
    "      \"title\": np.dtype(\"O\"),\n",
    "         #\n",
    "      \"firstName\": np.dtype(\"O\"),\n",
    "      \"lastName\": np.dtype(\"O\"),\n",
    "      \"gender\": np.dtype(\"O\"),\n",
    "      # \"birthday\": np.dtype(\"O\"),\n",
    "      \"speaks\": np.dtype(\"O\"),\n",
    "      \"email\": np.dtype(\"O\"),\n",
    "         #\n",
    "      \"START_ID\": np.dtype(\"O\"),\n",
    "      \"END_ID\": np.dtype(\"O\"),\n",
    "      },\n",
    "\n",
    "      converters={\n",
    "         'creationDate': lambda x: pd.to_datetime(x, unit='ns'),\n",
    "         'birthday': lambda x: pd.to_datetime(x).date(),\n",
    "            # \n",
    "         #'language': lambda x: x.split(';'),\n",
    "         #'email': lambda x: x.split(';')\n",
    "      }\n",
    "      )\n",
    "\n",
    "   # print(df.count())\n",
    "   # print(df.head(1))\n",
    "\n",
    "   i_TrueFile1 = os.path.basename(i_File)\n",
    "   i_TrueFile2 = os. path. splitext(i_TrueFile1)[0]\n",
    "      #\n",
    "   l_OutputDir = \"gs://\" + i_Bucket + \"/\" + i_OutputDir + \"/\" + i_TrueFile2\n",
    "\n",
    "   df.to_parquet(\n",
    "      l_OutputDir,\n",
    "      write_index=False,\n",
    "      write_metadata_file=False,\n",
    "      engine=\"pyarrow\",\n",
    "      version=\"2.6\")\n",
    "\n",
    "print(\"--\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b17dcf4-ed1f-4ae8-8aba-afd4c8578115",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  GCS doesn't have directories like a standard filesystem\n",
    "#\n",
    "#  This loop will output top level folder names ending with a \"/\",\n",
    "#  and all child files as a relative pathname\n",
    "\n",
    "def f_ProcessFiles(i_Bucket, i_InputDir, i_OutputDir):\n",
    "    \n",
    "   l_files = storage.Client().get_bucket(i_Bucket)\n",
    "\n",
    "   for l_file in l_files.list_blobs(prefix=i_InputDir):\n",
    "      if (l_file.name.endswith(\".csv\")):\n",
    "         f_ReadCsvOutputParquet(i_Bucket, l_file.name, i_OutputDir)\n",
    "\n",
    "print(\"--\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba8987f-2043-4f70-b01e-105d71019c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Driver for function above\n",
    "\n",
    "print(\"   \", end=\"\")\n",
    "\n",
    "f_ProcessFiles(l_Bucket, l_InputDir, l_OutputDir)\n",
    "\n",
    "print(\"\"     )\n",
    "print(\"   --\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfb09a0-e0de-4969-8ad1-ce77ac06a644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f22d77c-e148-4655-a602-6455efcbdec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Better versions of the 2 above; allows for skipping, and quantity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8522790-6565-426b-b232-324ac5a9af53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  GCS doesn't have directories like a standard filesystem\n",
    "#\n",
    "#  This loop will output top level folder names ending with a \"/\",\n",
    "#  and all child files as a relative pathname\n",
    "\n",
    "def f_ProcessFiles(i_Bucket, i_InputDir, i_OutputDir, i_start, i_end):\n",
    "    \n",
    "   l_files = storage.Client().get_bucket(i_Bucket)\n",
    "      #\n",
    "   l_cntr = 0\n",
    "\n",
    "   for l_file in l_files.list_blobs(prefix=i_InputDir):\n",
    "      if (l_file.name.endswith(\".csv\")):\n",
    "         l_cntr = l_cntr + 1\n",
    "            #\n",
    "         if (l_cntr >= i_start) and (l_cntr <= i_end):\n",
    "            f_ReadCsvOutputParquet(i_Bucket, l_file.name, i_OutputDir)\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7b0d22-e0a6-46d0-9345-280e41277eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Driver for function above\n",
    "#\n",
    "#  These ranges are inclusive\n",
    "\n",
    "l_start = 3\n",
    "l_end   = 5\n",
    "\n",
    "print(\"   \", end=\"\")\n",
    "\n",
    "f_ProcessFiles(l_Bucket, l_InputDir, l_OutputDir, l_start, l_end)\n",
    "\n",
    "print(\"\"     )\n",
    "print(\"   --\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
