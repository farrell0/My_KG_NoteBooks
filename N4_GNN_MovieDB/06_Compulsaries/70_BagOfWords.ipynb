{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d776f754-ef98-4cbd-a423-647c483341ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Here we demonstrate the following Dask DataFrame techniques;\n",
    "#\n",
    "#     .  How to create a Dask DataFrame from an array.\n",
    "#     .  Introduction to  sklearn.feature_extraction.text import CountVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682150e3-6a50-4197-a239-80d1d2ea6edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "NUM_PARTITIONS  = 3\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0416c10d-a1cb-433d-ad5e-85498aaa1744",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "   #\n",
    "import dask.array as da\n",
    "import dask.dataframe as dd\n",
    "   #\n",
    "from dask.dataframe import from_pandas\n",
    "\n",
    "from tabulate import tabulate\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "13c6a99e-7b23-4cf6-9880-d70fffb4b889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131298d0-5cb4-4b62-8413-5e5571ef4b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Create our source Dask DataFrame from an array.\n",
    "#     (We use this later, briefly, for comparisons.)\n",
    "\n",
    "np_airports = np.array([\n",
    "       #\n",
    "   [\"MKE\", \"Milwaukee\"     , \"WI\", \"Airport\"],\n",
    "   [\"ORD\", \"Chicago O-Hare\", \"IL\", \"Airport\"],\n",
    "   [\"SJC\", \"San Jose\"      , \"CA\", \"Airport\"],\n",
    "   [\"LAX\", \"Los Angeles\"   , \"CA\", \"Airport\"],\n",
    "   [\"DEN\", \"Denver\"        , \"CO\", \"Airport\"],\n",
    "       #\n",
    "   ], dtype=\"str\")\n",
    "\n",
    "pd_airports = pd.DataFrame(np_airports, columns = [\"airport_code\", \"airport_name\", \"state_code\", \"LABEL\"])\n",
    "   #\n",
    "dd_airports = from_pandas(pd_airports, npartitions = NUM_PARTITIONS)\n",
    "\n",
    "\n",
    "for l_each in dd_airports.itertuples():\n",
    "   print(\"Airport:  %3s   %-18s   %-2s   %-10s\" % (l_each.airport_code, l_each.airport_name, l_each.state_code, l_each.LABEL))\n",
    "\n",
    "print(\"--\")\n",
    "\n",
    "#  Sample output,\n",
    "#\n",
    "#  Airport:  MKE   Milwaukee            WI   Airport   \n",
    "#  Airport:  ORD   Chicago O-Hare       IL   Airport   \n",
    "#  Airport:  SJC   San Jose             CA   Airport   \n",
    "#  Airport:  LAX   Los Angeles          CA   Airport   \n",
    "#  Airport:  DEN   Denver               CO   Airport  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48a0b2d-d27e-4401-b4d6-d33a8a245398",
   "metadata": {},
   "source": [
    "#  Introduction to CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828d62e9-1cf8-4eee-9147-f351961b95f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Working with  sklearn.feature_extraction.text.CountVectorizer\n",
    "#\n",
    "#     See,\n",
    "#        https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html#tokenizing-text-with-scikit-learn\n",
    "\n",
    "\n",
    "#  .  The results array is sorted by input key value.\n",
    "#        ..  So below, baseball occupies the output's zero'th position,\n",
    "#            where cricket will occupy the 1'st.\n",
    "#  .  Words will be automatically split on whitespace, other.\n",
    "#        ..  So I plan to do my own splitting beforehand, lest I not know what the output array represents.\n",
    "#               O-Hare          was not split\n",
    "#               football-helmet was split\n",
    "#  .  Duplicates do receive multiple entries in the output. Not certain of the effect of that.\n",
    "#\n",
    "#  .  2d integer64 array is output;\n",
    "#        ..  1st col is a counter, index into array\n",
    "#        ..  2nd col is for each word in the array, and the reference to 1st col, this word's position in the input\n",
    "#\n",
    "\n",
    "my_cv = CountVectorizer()\n",
    "\n",
    "my_input_arr = [ \n",
    "   \"baseball\",\n",
    "   \"football\",\n",
    "   \"cricket\",\n",
    "   \"golf\",\n",
    "   \"racing\",\n",
    "   \"fencing\",\n",
    "   \"cricket\",\n",
    "]\n",
    "\n",
    "\n",
    "my_cv_counts = my_cv.fit_transform(my_input_arr)\n",
    "   #\n",
    "#  my_cv_counts = my_cv.fit_transform(dd_airports.airport_code)\n",
    "#  my_cv_counts = my_cv.fit_transform(dd_airports.airport_name)\n",
    "\n",
    "\n",
    "print(type(my_cv_counts))                  #  <class 'scipy.sparse._csr.csr_matrix'>\n",
    "\n",
    "                                           #  For my_input_arr above\n",
    "                                           #  -----------------------------------\n",
    "print(my_cv_counts.shape)                  #  (7, 6)\n",
    "print(my_cv_counts      )                  #  (0, 0)    1\n",
    "                                           #  (1, 3)    1\n",
    "                                           #  (2, 1)    1\n",
    "                                           #  (3, 4)    1\n",
    "                                           #  (4, 5)    1\n",
    "                                           #  (5, 2)    1\n",
    "                    \n",
    "                                           #  For dd_airports.airport_code above\n",
    "                                           #  -----------------------------------\n",
    "                                           #  (5, 5)\n",
    "                                           #  (0, 2)    1\n",
    "                                           #  (1, 3)    1\n",
    "                                           #  (2, 4)    1\n",
    "                                           #  (3, 1)    1\n",
    "                                           #  (4, 0)    1\n",
    " \n",
    "                                           #  For dd_airports.airport_name above\n",
    "                                           #  -----------------------------------\n",
    "                                           #  (5, 8)\n",
    "                                           #  (0, 6)    1\n",
    "                                           #  (1, 1)    1\n",
    "                                           #  (1, 3)    1\n",
    "                                           #  (2, 7)    1\n",
    "                                           #  (2, 4)    1\n",
    "                                           #  (3, 5)    1\n",
    "                                           #  (3, 0)    1\n",
    "                                           #  (4, 2)    1\n",
    "        \n",
    "print(\"\")\n",
    "\n",
    "df_words = pd.DataFrame(my_cv_counts.toarray())\n",
    "   #\n",
    "for l_each in df_words.iterrows():\n",
    "   print(l_each)                 \n",
    "\n",
    "\n",
    "#  For my_input_arr above  (printed as it is output)\n",
    "# \n",
    "#  (0, 0    1 1    0 2    0 3    0 4    0 5    0 Name: 0, dtype: int64)\n",
    "#  (1, 0    0 1    0 2    0 3    1 4    0 5    0 Name: 1, dtype: int64)\n",
    "#  (2, 0    0 1    1 2    0 3    0 4    0 5    0 Name: 2, dtype: int64)\n",
    "#  (3, 0    0 1    0 2    0 3    0 4    1 5    0 Name: 3, dtype: int64)\n",
    "#  (4, 0    0 1    0 2    0 3    0 4    0 5    1 Name: 4, dtype: int64)\n",
    "#  (5, 0    0 1    1 2    1 3    0 4    0 5    0 Name: 5, dtype: int64)\n",
    "#  (6, 0    0 1    1 2    0 3    0 4    0 5    0 Name: 6, dtype: int64)\n",
    "\n",
    "#  Above better formatted as,\n",
    "#\n",
    "#  (0,    0 1    1 0    2 0    3 0   4 0   5 0   Name: 0, dtype: int64)\n",
    "#  (1,    0 0    1 0    2 0    3 1   4 0   5 0   Name: 1, dtype: int64)\n",
    "#  (2,    0 0    1 1    2 0    3 0   4 0   5 0   Name: 2, dtype: int64)\n",
    "#  (3,    0 0    1 0    2 0    3 0   4 1   5 0   Name: 3, dtype: int64)\n",
    "#  (4,    0 0    1 0    2 0    3 0   4 0   5 1   Name: 4, dtype: int64)\n",
    "#  (5,    0 0    1 1    2 1    3 0   4 0   5 0   Name: 5, dtype: int64)\n",
    "#  (6,    0 0    1 1    2 0    3 0   4 0   5 0   Name: 6, dtype: int64)\n",
    "#\n",
    "#   A     B C    B C    B C   ......\n",
    "#\n",
    "#  So,\n",
    "#     A   == row number, offset into the array, 0-6 (7) total rows   from my_input_arr\n",
    "#     B   == col number, offset inside the row, 0-5 (6) unique words from my_input_arr\n",
    "#     C   ==  1|0  is this keyword  0-5 (6)  found in this row  0-6 (7)\n",
    "#\n",
    "#  So, if you had  1000  input records times  20  unique words, the array would be  1000x20\n",
    "#\n",
    "#\n",
    "#  If the value of row-2 was (football, cricket, golf), its entry would appear as,\n",
    "# \n",
    "#  (1,    0 0    1 1    2 0    3 1   4 1   5 0 Name: 1, dtype: int64)\n",
    "#\n",
    "#     Recall that the unique keywords sort as; (0)baseball (1)cricket (2)fencing (3)football (4)golf (5)racing\n",
    "\n",
    "\n",
    "print(\"--\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff5ac0a-32c9-401f-9fcb-b3b5500f533d",
   "metadata": {},
   "source": [
    "#  A more complete example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8a7e4b85-372f-491c-84b1-fc7714788163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data: baseball             Category: sport\n",
      "Data: bread                Category: food\n",
      "Data: cheese               Category: food\n",
      "Data: cricket              Category: sport\n",
      "Data: eggs                 Category: food\n",
      "Data: fencing              Category: sport\n",
      "Data: football             Category: sport\n",
      "Data: golf                 Category: sport\n",
      "Data: racing               Category: sport\n",
      "Data: wine                 Category: food\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#  More complete [ training ] data, as required ..\n",
    "#\n",
    "#     .  data[]       -- Our records, emails, nodes in a graph, .. in this case, a simple word\n",
    "#     .  category[]   -- Our unique list of node labels, for example\n",
    "#     .  target[]     -- Matches data[] above, the code of the node label.\n",
    "\n",
    "my_train = {}\n",
    "\n",
    "my_train[\"data\"] = [ \n",
    "   \"baseball\" ,\n",
    "   \"football\" ,\n",
    "   \"cricket\"  ,\n",
    "   \"golf\"     ,\n",
    "   \"racing\"   ,\n",
    "   \"fencing\"  ,\n",
    "      #\n",
    "   \"eggs\"     ,\n",
    "   \"bread\"    ,\n",
    "   \"cheese\"   ,\n",
    "   \"wine\"     ,\n",
    "   ]\n",
    "   #\n",
    "my_train[\"category\"] = [\n",
    "   \"sport\",\n",
    "   \"food\" ,\n",
    "   ]\n",
    "\n",
    "\n",
    "my_train[\"data\"].sort()                #  We could have entered the data pre-sorted\n",
    "my_train[\"category\"].sort()            #  Here, just reminding us that we should sort.\n",
    "\n",
    "\n",
    "my_train[\"target\"] = [1, 0, 0, 1, 0, 1, 1, 1, 1, 0]\n",
    "\n",
    "   ###\n",
    "    \n",
    "for l_index, l_zip in enumerate(zip(my_train[\"data\"], my_train[\"target\"])):\n",
    "   print(\"Data: %-18s   Category: %s\" % (l_zip[0], my_train[\"category\"][l_zip[1]]) )\n",
    "\n",
    "#  Sample output,\n",
    "#\n",
    "#     Data: baseball             Category: sport\n",
    "#     Data: bread                Category: food\n",
    "#     Data: cheese               Category: food\n",
    "#     Data: cricket              Category: sport\n",
    "#     Data: eggs                 Category: food\n",
    "#     Data: fencing              Category: sport\n",
    "#     Data: football             Category: sport\n",
    "#     Data: golf                 Category: sport\n",
    "#     Data: racing               Category: sport\n",
    "#     Data: wine                 Category: food\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3d5b19c6-dd02-4476-964a-9ab6ea263120",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "my_cv = CountVectorizer()\n",
    "\n",
    "#  Same as we did before/above\n",
    "#\n",
    "my_cv_counts = my_cv.fit_transform(my_train[\"data\"])\n",
    "\n",
    "\n",
    "#  New: Now normalize fact that longer (documents/Nodes/other) have more words and would get unfair weights\n",
    "#       term frequency / inverse document frequency\n",
    "#       (Tf-idf)\n",
    "#\n",
    "#       Effectively, reduce the weight of words that occur in more (documents/Nodes/other),\n",
    "#          in favor of words that occur in fewer (documents/Nodes/other)\n",
    "\n",
    "my_tf_transformer = TfidfTransfotmer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d87255-4a99-4c74-8a9a-8ed80cd19406",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c3cf4b-6b60-4e2c-aba4-a04c47a7cd23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0264d8-fc6c-416d-ac38-335e00fdd2ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f49097-92db-4c90-b76c-199b5a7ca20b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
