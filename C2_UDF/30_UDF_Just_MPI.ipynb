{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43b1755b-2af6-4570-84de-764a0024a919",
   "metadata": {
    "tags": []
   },
   "source": [
    "#  Display options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a11486-11ea-4a6c-8f7b-6f3808dfc85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Setting display options \n",
    "\n",
    "import pandas as pd\n",
    "   #\n",
    "pd.set_option(\"display.width\", 480)\n",
    "\n",
    "#  Sets horizontal scroll for wide outputs\n",
    "#\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"\"))\n",
    "\n",
    "from tabulate import tabulate\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60db99d1-7e45-49d6-8e9c-b6a998c56d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  When using UDFs, these execute on another host.\n",
    "#\n",
    "#  As such, these methods will need to be copied and run locally also.\n",
    "#\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option(\"max_colwidth\", None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ebe820-bc6d-4948-b668-8b81cd3cbf29",
   "metadata": {
    "tags": []
   },
   "source": [
    "#  Setup stuff: Connectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea957d6-30c6-427a-ae4e-15ecfba0c4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from katana import remote\n",
    "\n",
    "my_client = remote.Client()\n",
    "\n",
    "print(my_client)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97571016-7c02-4e90-a75a-136a0dacd35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "NUM_PARTITIONS  = 3\n",
    "   #\n",
    "DB_NAME         = \"my_db\"\n",
    "GRAPH_NAME      = \"my_graph\"\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5db6505-7fc8-4779-b194-9ed8945b9a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  CONNECT TO GRAPH\n",
    "\n",
    "my_graph, *_ = my_client.get_database(name=DB_NAME).find_graphs_by_name(GRAPH_NAME)\n",
    "\n",
    "print(my_graph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195ac246-4852-44ac-8a63-8631b38858ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "display(my_graph.num_nodes())\n",
    "display(my_graph.num_edges())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2077542-b136-4b57-9c87-c81942e08db3",
   "metadata": {
    "tags": []
   },
   "source": [
    "#  UDFs, Part 01: Just MPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4463948c-cc50-4fc0-b001-28dc883f2f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Best practice; Have a second NoteBook open with the Operations Widget ready-\n",
    "#\n",
    "#  Why ?\n",
    "#     When you make mistakes below, you will want to kill the Python job that is running\n",
    "#     on the worker nodes, the UDFs you submit below-\n",
    "#\n",
    "#  Source,\n",
    "#     https://mpi4py.readthedocs.io/en/stable/tutorial.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f746be-53db-4cc4-a765-53605277f265",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Example for testing; generates a random dataset\n",
    "#\n",
    "\n",
    "def my_func():\n",
    "    \n",
    "   #  from katana_enterprise.distributed import single_host\n",
    "   from katana.distributed import single_host\n",
    "      #\n",
    "   import numpy as np                                                     #  Unparitioned objects are just fine; we are working from only\n",
    "                                                                          #  one node\n",
    "\n",
    "   l_return = np.random.randint(1, 101, 4)                                #  Generate an array[4] of random numbers in the range of 1-100\n",
    "      #                                                                   #     <class 'numpy.ndarray'>\n",
    "   print(l_return)\n",
    "    \n",
    "    \n",
    "   #  return l_return\n",
    "   #\n",
    "   #     ValueError: Hosts returned more than one value. Please have only one host return a value while other's return a None.\n",
    "   #     The returned results (index is the host # in list):\n",
    "   #     [array([71, 25, 37, 40]), array([72, 19, 18, 63]), array([34, 39, 44, 59])]\n",
    "\n",
    "\n",
    "   return single_host(host=0, result=l_return)                            #  Multiple worker nodes (hosts), can only return from 1\n",
    "\n",
    "\n",
    "my_return = my_graph.run(lambda g: my_func())\n",
    "   #\n",
    "print(\"\")\n",
    "display(\"My return: %s\" % (str(my_return)))\n",
    "\n",
    "\n",
    "#  Sample output,\n",
    "#\n",
    "#     Host 0 output:\n",
    "#     [80 68 81 23]\n",
    "#     \n",
    "#     Host 1 output:\n",
    "#     [61 17 47 34]\n",
    "#     \n",
    "#     Host 2 output:\n",
    "#     [11 93 11 57]\n",
    "#     \n",
    "#     'My return: [80 68 81 23]'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8370fdf-c573-4e91-b7e9-c2ec4ef8131d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Knowing which worker node your code is running on; this approach is preferred\n",
    "#\n",
    "\n",
    "def my_func():\n",
    "    \n",
    "   from katana.distributed import MPI\n",
    "\n",
    "   l_comm         = MPI.COMM_WORLD                                        #  Think of this as a connection handle to this subsystem\n",
    "      #\n",
    "   l_nodenumber   = (l_comm.Get_rank())\n",
    "   l_nodecount    = (l_comm.Get_size())\n",
    "      #\n",
    "   print(\"This node number: %d   Count of all Nodes: %d\" % (l_nodenumber, l_nodecount))\n",
    "\n",
    "   return\n",
    "\n",
    "\n",
    "l_result = my_graph.run(lambda g: my_func())\n",
    "   #\n",
    "print(\"--\")\n",
    "\n",
    "#  Sample output,\n",
    "#\n",
    "#     Host 0 output:\n",
    "#     This node number: 0   Count of all Nodes: 3\n",
    "#     \n",
    "#     Host 1 output:\n",
    "#     This node number: 1   Count of all Nodes: 3\n",
    "#     \n",
    "#     Host 2 output:\n",
    "#     This node number: 2   Count of all Nodes: 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfaf4819-ee02-4fd2-ac0f-8c0efc8d02a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Example using single inter worker node data passing (what MPI labels, \"Point\n",
    "#  to Point Communication\") ; approach 1\n",
    "#\n",
    "\n",
    "def my_func():\n",
    "    \n",
    "   from katana.distributed import single_host\n",
    "   from katana.distributed import MPI\n",
    "      #\n",
    "   import numpy as np\n",
    "\n",
    "\n",
    "   l_comm         = MPI.COMM_WORLD\n",
    "   l_nodenumber   = (l_comm.Get_rank())\n",
    "   l_nodecount    = (l_comm.Get_size())\n",
    "      #\n",
    "   l_return       = None\n",
    "\n",
    "    \n",
    "   if (l_nodenumber == 0):                                                #  Just node-0, just receive\n",
    "      l_datarecv = l_comm.recv(source = 1)\n",
    "      print(l_datarecv)\n",
    "      l_return = l_datarecv\n",
    "   elif (l_nodenumber == 1):                                              #  Just node-1, just send\n",
    "      l_datasend = np.random.randint(1, 101, 4)\n",
    "      l_comm.send(l_datasend, dest = 0)\n",
    "    \n",
    "   return single_host(host=0, result=l_return)\n",
    "\n",
    "\n",
    "my_return = my_graph.run(lambda g: my_func())\n",
    "   #\n",
    "print(\"\")\n",
    "display(my_return)\n",
    "\n",
    "\n",
    "#  Sample output,\n",
    "#\n",
    "#     Host 0 output:\n",
    "#     [57  2 16 73]\n",
    "#     \n",
    "#     array([57,  2, 16, 73])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e2f082-b629-45a5-8d74-91439e4d96d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Example using multiple inter worker node data passing; approach 2\n",
    "#\n",
    "#  Change,\n",
    "#\n",
    "#     .  Do work on all nodes\n",
    "#\n",
    "#     **  This approach will hang or similar as written-\n",
    "#\n",
    "\n",
    "#  def my_func():\n",
    "#      \n",
    "#     from katana.distributed import single_host\n",
    "#     from katana.distributed import MPI\n",
    "#        #\n",
    "#     import numpy as np\n",
    "#  \n",
    "#  \n",
    "#     l_comm         = MPI.COMM_WORLD\n",
    "#     l_nodenumber   = (l_comm.Get_rank())\n",
    "#     l_nodecount    = (l_comm.Get_size())\n",
    "#  \n",
    "#  \n",
    "#     def f_dowork():                                                        \n",
    "#  \n",
    "#        #\n",
    "#        #  Do whatever here to generate what you want to return to the calling\n",
    "#        #  function/client\n",
    "#        #\n",
    "#      \n",
    "#        l_result = np.random.randint(1, 101, 4)\n",
    "#           #\n",
    "#        return l_result\n",
    "#      \n",
    "#      \n",
    "#     #  Once this works, this function should not need to change\n",
    "#     #\n",
    "#     def f_datasend(i_datasend):\n",
    "#        if (l_nodenumber > 0):                                               #  Generally; it seems to hang if a node tries to\n",
    "#           l_comm.isend(i_datasend, dest = 0)                                #  send/recv to itself.\n",
    "#           print(\"SENT\")\n",
    "#  \n",
    "#      \n",
    "#     #  Once this works, this function should not need to change\n",
    "#     #\n",
    "#     def f_datarecv(i_result):\n",
    "#      \n",
    "#        l_return = []\n",
    "#           #\n",
    "#        if (l_nodenumber == 0):\n",
    "#           l_return.append(i_result)\n",
    "#  \n",
    "#          \n",
    "#        #  if l_nodenumber == 0:                                            #  This block hangs every time.\n",
    "#        #     l_datarecv = l_comm.recv(source = 1)                          #\n",
    "#        #     print(l_datarecv)                                             #  Change the order to be 2, then 1, and it works\n",
    "#        #     l_return.append(l_datarecv)                                   #  frequently, not always\n",
    "#        #        #\n",
    "#        #     l_datarecv = l_comm.recv(source = 2)\n",
    "#        #     print(l_datarecv)\n",
    "#        #     l_return.append(l_datarecv)\n",
    "#          \n",
    "#        if l_nodenumber == 0:                                               #  This block works semi-frequently , because of the \n",
    "#           l_datarecv = l_comm.recv(source = 2)                             #  order; 2 then 1\n",
    "#           print(\"RECV\")\n",
    "#           l_return.append(l_datarecv)                                      #     **  Do not use the approach in this cell\n",
    "#              #\n",
    "#           l_datarecv = l_comm.recv(source = 1)\n",
    "#           print(\"RECV\")\n",
    "#           l_return.append(l_datarecv)\n",
    "#      \n",
    "#      \n",
    "#        return l_return\n",
    "#      \n",
    "#      \n",
    "#              #####################################################\n",
    "#          \n",
    "#  \n",
    "#     l_result    = f_dowork()\n",
    "#        #\n",
    "#     f_datasend(l_result)\n",
    "#        #\n",
    "#     l_returnall = f_datarecv(l_result)\n",
    "#  \n",
    "#  \n",
    "#     return single_host(host=0, result=l_returnall)\n",
    "#  \n",
    "#  \n",
    "#  my_return = my_graph.run(lambda g: my_func())\n",
    "#     #\n",
    "#  print(\"\")\n",
    "#  display(my_return)\n",
    "\n",
    "\n",
    "#  Error,\n",
    "#\n",
    "#      CancelledError: \n",
    "\n",
    "#  Output, when it works,\n",
    "#\n",
    "#     Host 0 output:\n",
    "#     RECV\n",
    "#     RECV\n",
    "#     \n",
    "#     Host 1 output:\n",
    "#     SENT\n",
    "#     \n",
    "#     Host 2 output:\n",
    "#     SENT\n",
    "#     \n",
    "#     [array([13, 77, 17, 87]), array([ 3, 86, 14, 37]), array([70,  4, 17, 93])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3c9e52-afbe-4054-8fd5-60b0969208f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Our use case; Working on all nodes, sending results to a single node,\n",
    "#  so we may return consolidated results to the calling function.\n",
    "#\n",
    "#  Approach 1\n",
    "#\n",
    "#  MPI labels this \"Collective Communication\".\n",
    "#\n",
    "#     .  MPI expects you have (n) nodes, and want to send all data back\n",
    "#        to that one node or similar. As such, [ you run one MPI method ] \n",
    "#        that itself will change behavior whether it run one the \"one\" node, \n",
    "#        or other nodes.\n",
    "#\n",
    "#        We don't need to program that part. MPI does that for us.\n",
    "#\n",
    "#     .  For our use case, outlined above, we will use gather().\n",
    "#        MPI also includes; scatter() broadcast(), and more.\n",
    "#\n",
    "\n",
    "def my_func():\n",
    "    \n",
    "   from katana.distributed import single_host\n",
    "   from katana.distributed import MPI\n",
    "      #\n",
    "   import numpy as np\n",
    "\n",
    "\n",
    "   l_comm         = MPI.COMM_WORLD\n",
    "   l_nodenumber   = (l_comm.Get_rank())\n",
    "   l_nodecount    = (l_comm.Get_size())\n",
    "\n",
    "\n",
    "            #####################################################\n",
    "        \n",
    "        \n",
    "   def f_dowork():                                                        \n",
    "\n",
    "      #\n",
    "      #  Do whatever here to generate what you want to return to the calling\n",
    "      #  function/client\n",
    "      #\n",
    "    \n",
    "      l_result = np.random.randint(1, 101, 4)\n",
    "         #\n",
    "      return l_result\n",
    "    \n",
    "    \n",
    "            #####################################################\n",
    "        \n",
    "        \n",
    "   #  This function should not need to change\n",
    "   #\n",
    "   def f_datagather(i_datasend):\n",
    "      l_return = l_comm.gather(i_datasend, root = 0)\n",
    "      return l_return\n",
    "    \n",
    "\n",
    "   l_result    = f_dowork()\n",
    "   l_returnall = f_datagather(l_result)\n",
    "      #\n",
    "   return single_host(host=0, result=l_returnall)\n",
    "\n",
    "\n",
    "my_return = my_graph.run(lambda g: my_func())\n",
    "   #\n",
    "print(\"\")\n",
    "display(my_return)\n",
    "\n",
    "\n",
    "#  Sample output,\n",
    "#\n",
    "#     [array([63, 38, 22,  1]), array([23, 85, 24,  3]), array([91, 82, 14, 10])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b78be6d-4a80-4416-827d-1e13cc5e8f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Same as above; approach 2\n",
    "#\n",
    "#  Add,\n",
    "#     .  Move to an end result of Pandas DataFrames\n",
    "#        Why ?\n",
    "#        So that our return columns are easily labeled, and the support\n",
    "#        methods we inherit.\n",
    "#\n",
    "#        We do not shuttle DataFrames from the worker nodes using MPI.\n",
    "#        What we would receive on node-0 would be an array of DataFrames.\n",
    "#        \n",
    "#        Instead we shuttle arrays of dictionaries, and return that. This\n",
    "#        return array does need its geometry changed, based on how MPI\n",
    "#        assembles things, but ..\n",
    "#\n",
    "#     .  Here we get our (evetually DataFrame data) from KAPI.\n",
    "#\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def my_func(i_graph, i_label, i_properties):\n",
    "    \n",
    "   from katana.distributed import single_host\n",
    "   from katana.distributed import MPI\n",
    "      #\n",
    "   import numpy as np\n",
    "\n",
    "\n",
    "   l_comm         = MPI.COMM_WORLD\n",
    "   l_nodenumber   = (l_comm.Get_rank())\n",
    "   l_nodecount    = (l_comm.Get_size())\n",
    "\n",
    "   #####################################################\n",
    "\n",
    "   #  This function should not need to change\n",
    "   #\n",
    "   def f_datagather(i_datasend):\n",
    "      l_return = l_comm.gather(i_datasend, root = 0)\n",
    "      return l_return\n",
    "\n",
    "   #####################################################\n",
    "        \n",
    "        \n",
    "   def f_dowork(i_graph, i_label, i_properties):                                                        \n",
    "\n",
    "      #\n",
    "      #  Do whatever here to generate what you want to return to the calling\n",
    "      #  function/client\n",
    "      #\n",
    "    \n",
    "      l_node_props = {each: i_graph.nodes.get_property(each) for each in i_properties}\n",
    "      l_nodes      = []\n",
    "         #\n",
    "      for l_node in i_graph.nodes.masters():\n",
    "         if (i_label in i_graph.nodes.labels(l_node) ):\n",
    "                \n",
    "            #  This works, but .. .. when we have a larger number of \n",
    "            #  columns, this would get wordy. Move it to a comprehension\n",
    "            #  technique.\n",
    "            #\n",
    "            #  l_nodes.append( {\n",
    "            #     \"src_node\"     : l_nodenumber,                                    #  Since we have it, return also the source node.\n",
    "            #        #\n",
    "            #     i_properties[0]: l_node_props[i_properties[0]][l_node],           #  First node property sent into this method\n",
    "            #     i_properties[1]: l_node_props[i_properties[1]][l_node],           #  Second node property\n",
    "            #  } ) \n",
    "            \n",
    "            #  Functionally same block as above, but using comprehension\n",
    "            #\n",
    "            l_dict = {each: l_node_props[each][l_node] for each in i_properties}    #  Column from the graph, this outputs a dictionary\n",
    "            l_dict[\"src_node\"] = l_nodenumber                                       #  Adding to the dictionary\n",
    "               #\n",
    "            l_nodes.append(l_dict)\n",
    "            \n",
    "      return l_nodes\n",
    "    \n",
    "    \n",
    "   #####################################################\n",
    "        \n",
    "        \n",
    "   l_result    = f_dowork(i_graph, i_label, i_properties)                           #  This function does the actual work on the nodes.\n",
    "      #\n",
    "   l_returnall = f_datagather(l_result)                                             #  This function gathers all results to a single node.\n",
    "      #\n",
    "   return single_host(host=0, result=l_returnall)                                   #  Return to the calling function in Jupyter, other.\n",
    "\n",
    "\n",
    "print(\"--\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5947da-243c-46f4-90f8-0a44bb450ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Actually using the cell above,\n",
    "#\n",
    "\n",
    "\n",
    "my_collist    = [\"id\", \"airport_name\"]                                        #  Properties to return from the graph. These should\n",
    "                                                                              #  all exist in the graph \"label\" below.\n",
    "\n",
    "    \n",
    "my_datalist = my_graph.run(lambda g: my_func(g, \"Airport\", my_collist))       #  \"Airport\" is a node label in our graph.    \n",
    "\n",
    "\n",
    "\n",
    "#  MPI likes to return lists. And actuallt MPI gather() returns\n",
    "#  a list of lists; one list for each node, each containing a list\n",
    "#  of records found.\n",
    "#\n",
    "#  And we want DataFrames.\n",
    "#\n",
    "my_datalistflat = np.hstack(my_datalist)                                      #  Flatten the list of lists\n",
    "\n",
    "\n",
    "my_collist2     = my_collist.insert(0, \"src_node\")                            #  We actually return one additional column; add that\n",
    "                                                                              #  to the column list\n",
    "    \n",
    "\n",
    "my_dataframe = pd.DataFrame.from_records(my_datalistflat, index = my_collist2)\n",
    "   #\n",
    "print(tabulate(my_dataframe, headers='keys', tablefmt='psql'))\n",
    "\n",
    "\n",
    "#  Sample output,\n",
    "#\n",
    "#     +----+------------+------+----------------+\n",
    "#     |    |   src_node | id   | airport_name   |\n",
    "#     |----+------------+------+----------------|\n",
    "#     |  0 |          0 | MKE  | Milwaukee      |\n",
    "#     |  1 |          0 | ORD  | Chicago O-Hare |\n",
    "#     |  2 |          1 | SJC  | San Jose       |\n",
    "#     |  3 |          2 | DEN  | Denver         |\n",
    "#     +----+------------+------+----------------+\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf75e78-1621-437e-82bc-4bf9ed637fe8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36c9867-ca4d-4ce0-9c3f-7355ed932347",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
