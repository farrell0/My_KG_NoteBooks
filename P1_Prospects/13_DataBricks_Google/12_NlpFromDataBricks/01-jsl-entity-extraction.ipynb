{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b6f68fa2-a9a3-4b44-b7ba-c2ca27dd50a2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Patient Cohort Building with Unstructured Data: Entity Extraction\n",
    "\n",
    "In this notebook, we extract clinical and medical entities from texts to create a Knowledge Graph (KG) using pre-trained models in [Spark NLP relation extraction models for healthcare](https://www.johnsnowlabs.com/databricks/?utm_term=sparknlp&utm_campaign=Search+%7C+Spark+NLP&utm_source=adwords&utm_medium=ppc&hsa_acc=7272492311&hsa_cam=12543136013&hsa_grp=121056973604&hsa_ad=605485254464&hsa_src=g&hsa_tgt=kwd-1243265465686&hsa_kw=sparknlp&hsa_mt=p&hsa_net=adwords&hsa_ver=3&gclid=Cj0KCQiAmaibBhCAARIsAKUlaKRjPen9d1iGLcnRo3Ep10euMmW8dd5HuwERjTbbgyaOcNYrwaAeu8caAvmmEALw_wcB).\n",
    "In the first step of this workflow, we use pre-trained models to extract the entities and their relationships and in the next step we create a KG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "52300184-c376-4be9-8345-d403495afd4a",
     "showTitle": true,
     "title": "Initial Configurations"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import PipelineModel,Pipeline\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp_jsl.annotator import *\n",
    "from sparknlp.base import *\n",
    "import sparknlp_jsl\n",
    "import sparknlp\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option(\"display.max_colwidth\",100)\n",
    "\n",
    "print('sparknlp.version : ',sparknlp.version())\n",
    "print('sparknlp_jsl.version : ',sparknlp_jsl.version())\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "24595378-66ef-4661-9b26-30eddec1c72f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark._jvm.com.johnsnowlabs.util.start.registerListenerAndStartRefresh()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "426ee12f-03e6-4e83-a9ff-dfedbf42cd3c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Download Medical Dataset\n",
    "\n",
    "In this notebook, we will use synthetic medical records in csv format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c763b17c-84f8-4915-b8b9-5950aebe47d0",
     "showTitle": true,
     "title": "set up paths"
    }
   },
   "outputs": [],
   "source": [
    "notes_path='/FileStore/HLS/jsl_kg/data/'\n",
    "delta_path='/FileStore/HLS/jsl_kg/delta/jsl/'\n",
    "\n",
    "dbutils.fs.mkdirs(notes_path)\n",
    "os.environ['notes_path']=f'/dbfs{notes_path}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "43b25c66-936e-4e41-93bd-51d7dc7ee37b",
     "showTitle": true,
     "title": "download raw data "
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "cd $notes_path\n",
    "wget -q https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp-workshop/master/databricks/python/healthcare_case_studies/data/data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7e8c4761-ca52-42d4-9288-c3dd4956839a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(f'{notes_path}/'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "27755a42-4b65-4ffc-8c03-27cc0d91e212",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Read Data and Write to Bronze Delta Layer\n",
    "\n",
    "There are 965 clinical records stored in delta table. We read the data and write the records into bronze delta tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "69652b32-d816-4967-911a-bdefa1e4ab71",
     "showTitle": true,
     "title": "ingest data"
    }
   },
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(pd.read_csv(f'/dbfs{notes_path}/data.csv', sep=';'))\n",
    "df.limit(20).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "172b3f9b-27a1-47f3-880b-481476c612b6",
     "showTitle": true,
     "title": "write data into deltalake "
    }
   },
   "outputs": [],
   "source": [
    "df.write.format('delta').mode('overwrite').save(f'{delta_path}/bronze/dataset')\n",
    "display(dbutils.fs.ls(f'{delta_path}/bronze/dataset'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "74e7bb26-bd78-47b8-a219-4ddd804ab108",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Posology RE Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5121efde-b4e8-4fbd-a340-5296a54b9da6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Posology Relation Extraction\n",
    "\n",
    "Posology relation extraction pretrained model supports the following relatios:\n",
    "\n",
    "DRUG-DOSAGE  \n",
    "DRUG-FREQUENCY  \n",
    "DRUG-ADE (Adversed Drug Events)  \n",
    "DRUG-FORM  \n",
    "DRUG-ROUTE  \n",
    "DRUG-DURATION  \n",
    "DRUG-REASON  \n",
    "DRUG=STRENGTH  \n",
    "\n",
    "The model has been validated against the posology dataset described in (Magge, Scotch, & Gonzalez-Hernandez, 2018).\n",
    "\n",
    "| Relation | Recall | Precision | F1 | F1 (Magge, Scotch, & Gonzalez-Hernandez, 2018) |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| DRUG-ADE | 0.66 | 1.00 | **0.80** | 0.76 |\n",
    "| DRUG-DOSAGE | 0.89 | 1.00 | **0.94** | 0.91 |\n",
    "| DRUG-DURATION | 0.75 | 1.00 | **0.85** | 0.92 |\n",
    "| DRUG-FORM | 0.88 | 1.00 | **0.94** | 0.95* |\n",
    "| DRUG-FREQUENCY | 0.79 | 1.00 | **0.88** | 0.90 |\n",
    "| DRUG-REASON | 0.60 | 1.00 | **0.75** | 0.70 |\n",
    "| DRUG-ROUTE | 0.79 | 1.00 | **0.88** | 0.95* |\n",
    "| DRUG-STRENGTH | 0.95 | 1.00 | **0.98** | 0.97 |\n",
    "\n",
    "\n",
    "*Magge, Scotch, Gonzalez-Hernandez (2018) collapsed DRUG-FORM and DRUG-ROUTE into a single relation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "91a7c32e-f158-455e-8283-2ff372311da2",
     "showTitle": true,
     "title": "setup the pipeline"
    }
   },
   "outputs": [],
   "source": [
    "documenter = DocumentAssembler()\\\n",
    "    .setInputCol(\"text\")\\\n",
    "    .setOutputCol(\"documents\")\n",
    "\n",
    "sentencer = SentenceDetector()\\\n",
    "    .setInputCols([\"documents\"])\\\n",
    "    .setOutputCol(\"sentences\")\n",
    "\n",
    "tokenizer = sparknlp.annotators.Tokenizer()\\\n",
    "    .setInputCols([\"sentences\"])\\\n",
    "    .setOutputCol(\"tokens\")\n",
    "\n",
    "words_embedder = WordEmbeddingsModel()\\\n",
    "    .pretrained(\"embeddings_clinical\", \"en\", \"clinical/models\")\\\n",
    "    .setInputCols([\"sentences\", \"tokens\"])\\\n",
    "    .setOutputCol(\"embeddings\")\n",
    "\n",
    "pos_tagger = PerceptronModel()\\\n",
    "    .pretrained(\"pos_clinical\", \"en\", \"clinical/models\") \\\n",
    "    .setInputCols([\"sentences\", \"tokens\"])\\\n",
    "    .setOutputCol(\"pos_tags\")\n",
    "\n",
    "posology_ner = MedicalNerModel()\\\n",
    "    .pretrained(\"ner_posology\", \"en\", \"clinical/models\")\\\n",
    "    .setInputCols(\"sentences\", \"tokens\", \"embeddings\")\\\n",
    "    .setOutputCol(\"ners\")   \n",
    "\n",
    "posology_ner_converter = NerConverterInternal() \\\n",
    "    .setInputCols([\"sentences\", \"tokens\", \"ners\"]) \\\n",
    "    .setOutputCol(\"ner_chunks\")\n",
    "\n",
    "dependency_parser = DependencyParserModel()\\\n",
    "    .pretrained(\"dependency_conllu\", \"en\")\\\n",
    "    .setInputCols([\"sentences\", \"pos_tags\", \"tokens\"])\\\n",
    "    .setOutputCol(\"dependencies\")\n",
    "\n",
    "reModel = RelationExtractionModel()\\\n",
    "    .pretrained(\"posology_re\")\\\n",
    "    .setInputCols([\"embeddings\", \"pos_tags\", \"ner_chunks\", \"dependencies\"])\\\n",
    "    .setOutputCol(\"posology_relations\")\\\n",
    "    .setMaxSyntacticDistance(4)\n",
    "\n",
    "pipeline = Pipeline(stages=[\n",
    "    documenter,\n",
    "    sentencer,\n",
    "    tokenizer, \n",
    "    words_embedder, \n",
    "    pos_tagger, \n",
    "    posology_ner,\n",
    "    posology_ner_converter,\n",
    "    dependency_parser,\n",
    "    reModel\n",
    "])\n",
    "\n",
    "empty_data = spark.createDataFrame([[\"\"]]).toDF(\"text\")\n",
    "\n",
    "model = pipeline.fit(empty_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4f32d1c2-88b3-473d-acfb-498a94fd1cd5",
     "showTitle": true,
     "title": "transform raw data"
    }
   },
   "outputs": [],
   "source": [
    "results = model.transform(df)\n",
    "results.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4080b706-793f-4edf-ae13-a92b374a9020",
     "showTitle": true,
     "title": "extract entities from results"
    }
   },
   "outputs": [],
   "source": [
    "result_df = results.select('subject_id','date',F.explode(F.arrays_zip(results.posology_relations.result, results.posology_relations.metadata)).alias(\"cols\")) \\\n",
    "                   .select('subject_id','date',F.expr(\"cols['0']\").alias(\"relation\"),\n",
    "                                               F.expr(\"cols['1']['entity1']\").alias(\"entity1\"),\n",
    "                                               F.expr(\"cols['1']['entity1_begin']\").alias(\"entity1_begin\"),\n",
    "                                               F.expr(\"cols['1']['entity1_end']\").alias(\"entity1_end\"),\n",
    "                                               F.expr(\"cols['1']['chunk1']\").alias(\"chunk1\"),\n",
    "                                               F.expr(\"cols['1']['entity2']\").alias(\"entity2\"),\n",
    "                                               F.expr(\"cols['1']['entity2_begin']\").alias(\"entity2_begin\"),\n",
    "                                               F.expr(\"cols['1']['entity2_end']\").alias(\"entity2_end\"),\n",
    "                                               F.expr(\"cols['1']['chunk2']\").alias(\"chunk2\"),\n",
    "                                               F.expr(\"cols['1']['confidence']\").alias(\"confidence\"))\n",
    "result_df.limit(20).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f9e905c-b752-45f7-9f66-26a5cae5e1ca",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## RxNorm Code Extraction From Re_Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "61682a72-6673-4238-ac3a-b12eb0093ff6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# drug + strength or form\n",
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "result_df = (\n",
    "  result_df.withColumn('rx_text', when((F.col('entity1')=='DRUG') & ((F.col('entity2')=='FORM') | (F.col('entity2')=='STRENGTH') | (F.col('entity2')=='DOSAGE') ), F.concat(F.col('chunk1'),F.lit(' '), F.col('chunk2')))\n",
    " .when( ((F.col('entity1')=='FORM') | (F.col('entity1')=='STRENGTH') | (F.col('entity1')=='DOSAGE') ) & (F.col('entity2')=='DRUG'), F.concat(F.col('chunk2'),F.lit(' '), F.col('chunk1')))\n",
    " .when( (F.col('entity1')=='DRUG') & ((F.col('entity2')!='FORM') & (F.col('entity2')!='STRENGTH') & (F.col('entity2')!='DOSAGE') ), F.col('chunk1'))\n",
    " .when( (F.col('entity2')=='DRUG') & ((F.col('entity1')!='FORM') & (F.col('entity1')!='STRENGTH') & (F.col('entity1')!='DOSAGE') ), F.col('chunk2'))\n",
    "                   .otherwise(F.lit(' '))\n",
    "                   )\n",
    ")\n",
    "\n",
    "result_df.display(20,70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8d14701c-74c7-40d8-9b02-1f66f7457813",
     "showTitle": true,
     "title": "RxNorm extraction pipeline"
    }
   },
   "outputs": [],
   "source": [
    "documentAssembler = DocumentAssembler()\\\n",
    "      .setInputCol(\"rx_text\")\\\n",
    "      .setOutputCol(\"ner_chunk\")\n",
    "\n",
    "sbert_embedder = BertSentenceEmbeddings.pretrained('sbiobert_base_cased_mli', 'en','clinical/models')\\\n",
    "      .setInputCols([\"ner_chunk\"])\\\n",
    "      .setOutputCol(\"sentence_embeddings\")\n",
    "    \n",
    "rxnorm_resolver = SentenceEntityResolverModel.pretrained(\"sbiobertresolve_rxnorm_augmented\",\"en\", \"clinical/models\") \\\n",
    "      .setInputCols([\"ner_chunk\", \"sentence_embeddings\"]) \\\n",
    "      .setOutputCol(\"rxnorm_code\")\\\n",
    "      .setDistanceFunction(\"EUCLIDEAN\")\n",
    "\n",
    "rxnorm_pipelineModel = PipelineModel(\n",
    "    stages = [\n",
    "        documentAssembler,\n",
    "        sbert_embedder,\n",
    "        rxnorm_resolver])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a84b7b8-a923-4a3f-91cd-0645ae39c74d",
     "showTitle": true,
     "title": "Extract RxNorm from results_df"
    }
   },
   "outputs": [],
   "source": [
    "rxnorm_results = rxnorm_pipelineModel.transform(result_df)\n",
    "rxnorm_result = rxnorm_results.select('subject_id','date', 'relation', 'entity1', 'entity1_begin','entity1_end',  'chunk1', 'entity2', 'entity2_begin', 'entity2_end', \n",
    "                                         'chunk2', 'confidence', 'rx_text', \n",
    "                                         F.explode(F.arrays_zip(rxnorm_results.ner_chunk.result, \n",
    "                                                                rxnorm_results.ner_chunk.metadata, \n",
    "                                                                rxnorm_results.rxnorm_code.result, \n",
    "                                                                rxnorm_results.rxnorm_code.metadata)).alias(\"cols\")) \\\n",
    "                                     .select('subject_id','date', 'relation', 'entity1', 'entity1_begin','entity1_end',  'chunk1', 'entity2', 'entity2_begin', 'entity2_end',\n",
    "                                             'chunk2', 'confidence', 'rx_text',\n",
    "                                             F.expr(\"cols['1']['sentence']\").alias(\"sent_id\"),\n",
    "                                             F.expr(\"cols['0']\").alias(\"ner_chunk\"),\n",
    "                                             F.expr(\"cols['1']['entity']\").alias(\"entity\"), \n",
    "                                             F.expr(\"cols['2']\").alias('rxnorm_code'),\n",
    "                                             F.expr(\"cols['3']['all_k_results']\").alias(\"all_codes\"),\n",
    "                                             F.expr(\"cols['3']['all_k_resolutions']\").alias(\"resolutions\"))\n",
    "rxnorm_result.limit(20).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "356657ba-c053-462c-92bb-ad22175bf1e1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rxnorm_result = rxnorm_result.withColumn('all_codes', F.split(F.col('all_codes'), ':::'))\\\n",
    "                             .withColumn('resolutions', F.split(F.col('resolutions'), ':::'))\n",
    "rxnorm_result.limit(20).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2db089d2-2c46-4082-8481-3f7e4d3312ec",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Split Resolutions to Resolution Drug and Write Results to Golden Delta Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "01e9202c-7aa2-4e5b-85c3-41e01f4c4fc2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pd_rxnorm_result = rxnorm_result.toPandas()\n",
    "pd_rxnorm_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dc779dff-9713-46a4-a620-37cc1caac7b1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pd_rxnorm_result['drug_resolution']= pd_rxnorm_result['resolutions'].apply(lambda x: x[0])\n",
    "pd_rxnorm_result['drug_resolution'] = pd_rxnorm_result['drug_resolution'].str.lower()\n",
    "pd_rxnorm_result['chunk1']          = pd_rxnorm_result['chunk1'].str.lower()\n",
    "pd_rxnorm_result['chunk2']          = pd_rxnorm_result['chunk2'].str.lower()\n",
    "pd_rxnorm_result.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3db1a25e-5778-4f8e-8fe3-4925dacde074",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "outname = 'posology_RE_rxnorm_w_drug_resolutions.csv'\n",
    "outdir = f'/FileStore/HLS/jsl_kg/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d491b8e0-0ffe-472e-8a1e-6f2da5032b55",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pd_rxnorm_result.to_csv(f'/dbfs{outdir+outname}', index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e450293e-913b-4a55-99ed-79e30c17efee",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## NER JSL Slim\n",
    "\n",
    "Model card of the ner_jsl_slim is [here](https://nlp.johnsnowlabs.com/2021/08/13/ner_jsl_slim_en.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a7fd9bc9-a080-41c9-b1de-1025a95dd76a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "documentAssembler = DocumentAssembler()\\\n",
    "      .setInputCol(\"text\")\\\n",
    "      .setOutputCol(\"document\")\n",
    "\n",
    "sentenceDetector = SentenceDetector()\\\n",
    "      .setInputCols([\"document\"])\\\n",
    "      .setOutputCol(\"sentence\")\\\n",
    "      .setCustomBounds([\"\\|\"])\n",
    "\n",
    "tokenizer = Tokenizer()\\\n",
    "      .setInputCols([\"sentence\"])\\\n",
    "      .setOutputCol(\"token\")\\\n",
    "\n",
    "word_embeddings = WordEmbeddingsModel.pretrained(\"embeddings_clinical\", \"en\", \"clinical/models\")\\\n",
    "      .setInputCols([\"sentence\", \"token\"])\\\n",
    "      .setOutputCol(\"embeddings\")\n",
    "\n",
    "jsl_ner = MedicalNerModel.pretrained(\"ner_jsl_slim\", \"en\", \"clinical/models\") \\\n",
    "      .setInputCols([\"sentence\", \"token\", \"embeddings\"]) \\\n",
    "      .setOutputCol(\"ner\")\n",
    "\n",
    "jsl_converter = NerConverter() \\\n",
    "      .setInputCols([\"sentence\", \"token\", \"ner\"]) \\\n",
    "      .setOutputCol(\"ner_chunk\")\\\n",
    "      .setWhiteList(['Symptom','Body_Part', 'Procedure', 'Disease_Syndrome_Disorder', 'Test'])\n",
    "\n",
    "ner_pipeline = Pipeline(\n",
    "    stages = [\n",
    "        documentAssembler,\n",
    "        sentenceDetector,\n",
    "        tokenizer,\n",
    "        word_embeddings,\n",
    "        jsl_ner,\n",
    "        jsl_converter\n",
    "        ])\n",
    "\n",
    "data_ner = spark.createDataFrame([[\"\"]]).toDF(\"text\")\n",
    "model = ner_pipeline.fit(data_ner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cea74ba9-910e-448e-bf6a-8952ac0e4f4b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "results = model.transform(df)\n",
    "results.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d9d0f511-dbe5-4f3d-8b7f-ada10158420f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "result_df = results.select('subject_id','date',\n",
    "                           F.explode(F.arrays_zip(results.ner_chunk.result, results.ner_chunk.begin, results.ner_chunk.end, results.ner_chunk.metadata)).alias(\"cols\")) \\\n",
    "                    .select('subject_id','date',\n",
    "                            F.expr(\"cols['3']['sentence']\").alias(\"sentence_id\"),\n",
    "                            F.expr(\"cols['0']\").alias(\"chunk\"),\n",
    "                            F.expr(\"cols['1']\").alias(\"begin\"),\n",
    "                            F.expr(\"cols['2']\").alias(\"end\"),\n",
    "                            F.expr(\"cols['3']['entity']\").alias(\"ner_label\"))\\\n",
    "                    .filter(\"ner_label!='O'\")\n",
    "result_df.limit(20).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e16ece89-b67c-4f9b-a37a-197fffe1c126",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pd_result = result_df.toPandas()\n",
    "pd_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2a46e563-03b0-407a-9909-c1dcb6c67652",
     "showTitle": true,
     "title": "save results"
    }
   },
   "outputs": [],
   "source": [
    "outname = 'ner_jsl_slim_results.csv'\n",
    "outdir = f'/FileStore/HLS/jsl_kg/data/'\n",
    "pd_result.to_csv(f'/dbfs{outdir+outname}', index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a9f8097b-d894-4ba5-bf82-9095ccde10e4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## License\n",
    "Copyright / License info of the notebook. Copyright [2021] the Notebook Authors.  The source in this notebook is provided subject to the [Apache 2.0 License](https://spdx.org/licenses/Apache-2.0.html).  All included or referenced third party libraries are subject to the licenses set forth below.\n",
    "\n",
    "|Library Name|Library License|Library License URL|Library Source URL|\n",
    "| :-: | :-:| :-: | :-:|\n",
    "|Pandas |BSD 3-Clause License| https://github.com/pandas-dev/pandas/blob/master/LICENSE | https://github.com/pandas-dev/pandas|\n",
    "|Numpy |BSD 3-Clause License| https://github.com/numpy/numpy/blob/main/LICENSE.txt | https://github.com/numpy/numpy|\n",
    "|Apache Spark |Apache License 2.0| https://github.com/apache/spark/blob/master/LICENSE | https://github.com/apache/spark/tree/master/python/pyspark|\n",
    "|BeautifulSoup|MIT License|https://www.crummy.com/software/BeautifulSoup/#Download|https://www.crummy.com/software/BeautifulSoup/bs4/download/|\n",
    "|Requests|Apache License 2.0|https://github.com/psf/requests/blob/main/LICENSE|https://github.com/psf/requests|\n",
    "|Spark NLP Display|Apache License 2.0|https://github.com/JohnSnowLabs/spark-nlp-display/blob/main/LICENSE|https://github.com/JohnSnowLabs/spark-nlp-display|\n",
    "|Spark NLP |Apache License 2.0| https://github.com/JohnSnowLabs/spark-nlp/blob/master/LICENSE | https://github.com/JohnSnowLabs/spark-nlp|\n",
    "|Spark NLP for Healthcare|[Proprietary license - John Snow Labs Inc.](https://www.johnsnowlabs.com/spark-nlp-health/) |NA|NA|\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "|Author|\n",
    "|-|\n",
    "|Databricks Inc.|\n",
    "|John Snow Labs Inc.|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b78c2c87-8c24-47a6-9a6c-23f0b424785c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Disclaimers\n",
    "Databricks Inc. (“Databricks”) does not dispense medical, diagnosis, or treatment advice. This Solution Accelerator (“tool”) is for informational purposes only and may not be used as a substitute for professional medical advice, treatment, or diagnosis. This tool may not be used within Databricks to process Protected Health Information (“PHI”) as defined in the Health Insurance Portability and Accountability Act of 1996, unless you have executed with Databricks a contract that allows for processing PHI, an accompanying Business Associate Agreement (BAA), and are running this notebook within a HIPAA Account.  Please note that if you run this notebook within Azure Databricks, your contract with Microsoft applies."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "01-jsl-entity-extraction",
   "notebookOrigID": 713863765819897,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
