{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bb6b049-8d42-469d-beab-2fea1fd6346e",
   "metadata": {},
   "source": [
    "\n",
    "   The Elliptic Data Set maps Bitcoin transactions to real entities belonging to licit categories (exchanges,\n",
    "   wallet providers, miners, licit services, etc.) versus illicit ones (scams, malware, terrorist organizations, \n",
    "   ransomware, Ponzi schemes, etc.). \n",
    "   \n",
    "   The task on the dataset is to classify the illicit and licit nodes in the graph.\n",
    "   \n",
    "   \n",
    "   We will use Graph Neural Networks (GraphSAGE) to perform the node classification task. This demo shows the\n",
    "   end-to-end pipeline that can be done directly on the Katana Graph platform. \n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5063e7a2-2173-4071-b37b-b6d1f891f315",
   "metadata": {},
   "source": [
    "\n",
    "   Original data located at,\n",
    "      https://www.kaggle.com/datasets/ellipticco/elliptic-data-set?resource=download\n",
    "\n",
    "\n",
    "      203769   10_elliptic_txs_features.csv    (nodes)\n",
    "      234356   11_elliptic_txs_edgelist.csv    (edges)\n",
    "      203770   14_elliptic_txs_classes.csv \n",
    "      -------------------------------------\n",
    "      641895   (total)\n",
    "\n",
    "\n",
    "   This anonymized data set is a transaction graph collected from the Bitcoin blockchain. A node in the graph\n",
    "   represents a transaction, an edge can be viewed as a flow of Bitcoins between one transaction and the other. \n",
    "    \n",
    "   Each node has 166 features (property keys) and has been labeled as being created by a \"licit\", \"illicit\" or\n",
    "   \"unknown\" entity.\n",
    "\n",
    "   Two percent (4,545) of the nodes are labelled class1 (illicit). Twenty-one percent (42,019) are labelled\n",
    "   class2 (licit). The remaining transactions are not labelled with regard to licit versus illicit.\n",
    "\n",
    "   There are 166 property keys associated with each node. This data is unlabeled and obfuscated to protect\n",
    "   privacy.\n",
    "   \n",
    "   \n",
    "   There is a time step associated to each node, representing a measure of the time when a transaction was\n",
    "   broadcasted to the Bitcoin network. The time steps, running from 1 to 49, are evenly spaced with an interval\n",
    "   of about two weeks. Each time step contains a single connected component of transactions that appeared on\n",
    "   the blockchain within less than three hours between each other; there are no edges connecting the different\n",
    "   time steps.\n",
    "\n",
    "   The first 94 features represent local information about the transaction â€“ including the time step described\n",
    "   above, number of inputs/outputs, transaction fee, output volume and aggregated figures such as average BTC \n",
    "   received (spent) by the inputs/outputs and average number of incoming (outgoing) transactions associated with\n",
    "   the inputs/outputs. \n",
    "   \n",
    "   The remaining 72 features are aggregated features, obtained using transaction information one-hop backward/\n",
    "   forward from the center node - giving the maximum, minimum, standard deviation and correlation coefficients\n",
    "   of the neighbour transactions for the same information data (number of inputs/outputs, transaction fee, etc.).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4665495f-c756-4237-8a49-24838c5fbdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6712b29-d4d1-409b-a6c5-7ca1f04f3f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#  The source data files proper ..\n",
    "\n",
    "tx_features   = \"gs://katana-demo-datasets/fsi/solution_raw_data/elliptic/elliptic_txs_features.csv\"\n",
    "tx_edges      = \"gs://katana-demo-datasets/fsi/solution_raw_data/elliptic/elliptic_txs_edgelist.csv\"\n",
    "tx_classes    = \"gs://katana-demo-datasets/fsi/solution_raw_data/elliptic/elliptic_txs_classes.csv\"\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "441d663f-b9f1-499c-8048-22e60d0ca118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#  Creating column names for nodes ..\n",
    "\n",
    "feat_col_names    = [\"txId\", \"timestamp\"]\n",
    "local_feats_name  = [f\"local_feat_{i}\" for i in range(2,95)]\n",
    "agg_feats_name    = [f\"agg_feat_{i}\" for i in range(95,167)]\n",
    "\n",
    "\n",
    "#  display(print(f\"Number of rows, feat_col_names: {len(feat_col_names)}\"))\n",
    "#  display(print(f\"Number of rows, local_feat_name: {len(local_feats_name)}\"))\n",
    "#  display(print(f\"Number of rows, agg_feats_name: {len(agg_feats_name)}\"))\n",
    "\n",
    "#  Number of rows, feat_col_names:    2\n",
    "#  Number of rows, local_feat_name:  93\n",
    "#  Number of rows, agg_feats_name:   72\n",
    "#                                  ----\n",
    "#                                   167\n",
    "\n",
    "\n",
    "feat_col_names.extend(local_feats_name)\n",
    "feat_col_names.extend(agg_feats_name  )\n",
    "\n",
    "print(\"--\")\n",
    "\n",
    "\n",
    "#  display(print(f\"Number of rows, feat_col_names: {len(feat_col_names)}\"))\n",
    "\n",
    "#  Number of rows, feat_col_names: 167\n",
    "\n",
    "\n",
    "#  print(feat_col_names)\n",
    "\n",
    "#  ['txId', 'timestamp', 'local_feat_2', 'local_feat_3', 'local_feat_4', 'local_feat_5', 'local_feat_6', 'local_feat_7', \n",
    "#         ...\n",
    "#     'local_feat_90', 'local_feat_91', 'local_feat_92', 'local_feat_93', 'local_feat_94', 'agg_feat_95',\n",
    "#\n",
    "#     'agg_feat_96', 'agg_feat_97', 'agg_feat_98', 'agg_feat_99', 'agg_feat_100', 'agg_feat_101', 'agg_feat_102',\n",
    "#         ...\n",
    "#      'agg_feat_161', 'agg_feat_162', 'agg_feat_163', 'agg_feat_164', 'agg_feat_165', 'agg_feat_166']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c779fe14-7ac4-47c7-ab91-70222895e5fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#  Creating column data types for nodes ..\n",
    "\n",
    "feat_types   = {\n",
    "   \"class\": \"string\",\n",
    "   \"timestamp\": \"string\", \n",
    "   \"target\": \"float\",\n",
    "   \"node_type\": \"string\"\n",
    "   }\n",
    "\n",
    "\n",
    "local_cols   = {}\n",
    "   #\n",
    "for i in range(2,95):\n",
    "   local_cols[f\"local_feat_{i}\"] = \"float\"\n",
    "    \n",
    "agg_cols     = {}\n",
    "   #\n",
    "for i in range(95,167):\n",
    "   agg_cols[f\"agg_feat_{i}\"] = \"float\"\n",
    "\n",
    "    \n",
    "feat_types.update(local_cols)\n",
    "feat_types.update(agg_cols)\n",
    "\n",
    "\n",
    "print(\"--\")\n",
    "\n",
    "#  print(feat_types)\n",
    "\n",
    "#  {'class': 'string', 'timestamp': 'string', 'target': 'float', 'node_type': 'string', 'local_feat_2': 'float',\n",
    "#     'local_feat_3': 'float', 'local_feat_4': 'float', 'local_feat_5': 'float', 'local_feat_6': 'float',\n",
    "#         ...\n",
    "#     'local_feat_92': 'float', 'local_feat_93': 'float', 'local_feat_94': 'float', 'agg_feat_95': 'float',\n",
    "#\n",
    "#     'agg_feat_96': 'float', 'agg_feat_97': 'float', 'agg_feat_98': 'float', 'agg_feat_99': 'float',\n",
    "#        ...\n",
    "#     'agg_feat_163': 'float', 'agg_feat_164': 'float', 'agg_feat_165': 'float', 'agg_feat_166': 'float'}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb42eb2-45f8-4252-8f24-fd7ce0414521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d3c40c1-0ecc-41c0-b3f1-b1ca2df9a88b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#  Create Python Dask DataFrames ..\n",
    "#\n",
    "#  Dont forget; Lazy evaluation, so we'll add an optional compute ..\n",
    "#     (This cell will take a while because of the reading of a GS/S3 file.)\n",
    "\n",
    "import dask.dataframe as dd\n",
    "\n",
    "\n",
    "features   = dd.read_csv(tx_features, header=None, names=feat_col_names).compute()\n",
    "edges      = dd.read_csv(tx_edges).compute()\n",
    "classes    = dd.read_csv(tx_classes).compute()\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e72671b3-a5a8-4b16-838b-f2bcabd39965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#  Setting display width ..\n",
    "\n",
    "import pandas as pd\n",
    "   #\n",
    "pd.set_option('display.max_columns', 500 )\n",
    "pd.set_option('display.width'      , 1000)\n",
    "    \n",
    "print(\"--\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e94694-85a7-4e99-baa4-8e852e2092fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Non-functional: Just looking at the data ..\n",
    "\n",
    "#  display(print(f\"Number of rows, features: {len(features)}\"))\n",
    "#  display(print(f\"Sample data, features: {features.head(5)}\"))\n",
    "#      \n",
    "#  display(print(f\"Number of rows, edges: {len(edges)}\"))\n",
    "#  display(print(f\"Sample data, edges: {edges.head(5)}\"))\n",
    "#      \n",
    "#  display(print(f\"Number of rows, classes: {len(classes)}\"))\n",
    "#  display(print(f\"Sample data, classes: {classes.head(5)}\"))\n",
    "#      \n",
    "#  print(\"--\")    \n",
    "\n",
    "\n",
    "#  Number of rows, features: 203769\n",
    "#  Sample data, features:    txId        timestamp  local_feat_2  local_feat_3   ...   local_feat_93  local_feat_94     agg_feat_95  agg_feat_96  agg_feat_97   ...    agg_feat_164  agg_feat_165  agg_feat_166  \n",
    "#                         0  230425980   1          -0.171469     -0.184668      ...    1.135523        1.135279        -0.169160    -0.201584    -0.116817   ...      -0.097524     -0.120613     -0.119792  \n",
    "#                         1    5530458   1          -0.171484     -0.184668      ...   -1.084907       -1.084845        -0.170113    -0.202332    -0.116817   ...      -0.097524     -0.120613     -0.119792  \n",
    "#                         2  232022460   1          -0.172107     -0.184668      ...   -1.084907       -1.084845        -0.170528    -0.202658    -0.116817   ...      -0.183671     -0.120613     -0.119792  \n",
    "#                         3  232438397   1           0.163054      1.963790      ...    0.025308        0.025217        -0.171098     0.266450     0.159432   ...       0.677799     -0.120613     -0.119792  \n",
    "#                         4  230460314   1           1.011523     -0.081127      ...   -0.487315       -0.563089        -0.162974     0.844932     1.723414   ...       1.293750      0.178136      0.179117   \n",
    "\n",
    "#  Number of rows, edges: 234355\n",
    "#  Sample data, edges:     txId1      txId2\n",
    "#                       0  230425980    5530458\n",
    "#                       1  232022460  232438397\n",
    "#                       2  230460314  230459870\n",
    "#                       3  230333930  230595899\n",
    "#                       4  232013274  232029206\n",
    "\n",
    "#  Number of rows, classes: 203769\n",
    "#  Sample data, classes:    txId    class\n",
    "#                        0  230425980  unknown\n",
    "#                        1    5530458  unknown\n",
    "#                        2  232022460  unknown\n",
    "#                        3  232438397        2\n",
    "#                        4  230460314  unknown\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084c7dc0-cea4-42e9-bbbd-8c57d47605f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Mutation to classes ..\n",
    "\n",
    "classes['target']    = classes['class'].map({'unknown': -1.0, '1': 1.0, '2': 0.0})\n",
    "classes['node_type'] = classes['class'].map({'unknown': 'Unclassified_Txn', '1': 'Classified_Txn', '2': 'Classified_Txn'})\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9246c2df-ea0c-406e-97df-05c9f209216f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Non-functional: Just looking at the data ..\n",
    "\n",
    "#  display(print(f\"Number of rows, classes: {len(classes)}\"))\n",
    "#  display(print(f\"Sample data, classes: {classes.head(5)}\"))\n",
    "\n",
    "\n",
    "#  Number of rows, classes: 203769\n",
    "#  Sample data, classes:     txId       class      target       node_type\n",
    "#                         0  230425980  unknown    -1.0         Unclassified_Txn\n",
    "#                         1    5530458  unknown    -1.0         Unclassified_Txn\n",
    "#                         2  232022460  unknown    -1.0         Unclassified_Txn\n",
    "#                         3  232438397        2     0.0           Classified_Txn\n",
    "#                         4  230460314  unknown    -1.0         Unclassified_Txn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45d0ee2-8d82-4d4b-aaf8-8a8b5ef792f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  New DataFrame, nodes; features merged with classess ..\n",
    "#\n",
    "#     (Number of columns; the two source DataFrames share a key column name.)\n",
    "\n",
    "\n",
    "#  display(print(f\"Number of columns, features: {len(features.columns)}\"))\n",
    "#  display(print(f\"Number of columns, classes: {len(classes.columns)}\"))\n",
    "\n",
    "\n",
    "nodes = features.merge(classes)\n",
    "\n",
    "\n",
    "#  display(print(f\"Number of columns, nodes: {len(nodes.columns)}\"))\n",
    "\n",
    "print(\"--\")\n",
    "\n",
    "\n",
    "#  Number of columns, features: 167\n",
    "#  Number of columns, classes: 4\n",
    "#  Number of columns, nodes: 170\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec01026-8892-4050-87b3-79c477ee97a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f31fa37-c0e2-4ac0-9836-04b8531be8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Get a KatanaGraph Connection Handle ..\n",
    "\n",
    "from katana import remote\n",
    "from katana.remote import import_data\n",
    "\n",
    "\n",
    "NUM_PARTITIONS = 8\n",
    "   #\n",
    "DB_NAME        = \"my_db\"\n",
    "GRAPH_NAME     = \"my_graph\"\n",
    "\n",
    "\n",
    "my_client = remote.Client()\n",
    "\n",
    "print(my_client)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e76bc86-c75e-4694-9a55-fe75b3a9a99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Create KatanaGraph Graph object ..\n",
    "\n",
    "my_graph = my_client.get_database(name=DB_NAME).create_graph(name=GRAPH_NAME, num_partitions=NUM_PARTITIONS)\n",
    "\n",
    "print(my_graph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ad0cca-9b84-41ae-87a8-7c19d63f6973",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  CONNECT TO GRAPH\n",
    "\n",
    "for l_graph in my_client.graphs():\n",
    "   if (l_graph.name == GRAPH_NAME):\n",
    "      my_graph=my_client.get_database(name=DB_NAME).get_graph_by_id(id=l_graph.graph_id)\n",
    "        \n",
    "print(my_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9552b515-e2bf-48a2-9e64-fed0c3e7f363",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Load Graph ..\n",
    "\n",
    "\n",
    "REVERSE_EDGES = True\n",
    "\n",
    "\n",
    "with import_data.DataFrameImporter(my_graph) as df_importer:   \n",
    "    \n",
    "   df_importer.nodes_dataframe(nodes,\n",
    "      id_column             = \"txId\",\n",
    "      id_space              = \"transaction\", \n",
    "      property_columns      = feat_types,\n",
    "         #\n",
    "      label_column          = \"node_type\"\n",
    "      )\n",
    "    \n",
    "   df_importer.edges_dataframe(edges,\n",
    "      source_id_space       = \"transaction\",\n",
    "      destination_id_space  = \"transaction\",\n",
    "         #\n",
    "      source_column         = \"txId1\",\n",
    "      destination_column    = \"txId2\",\n",
    "         #\n",
    "      type                  = \"tx_flow\"\n",
    "      )\n",
    "\n",
    "   if (REVERSE_EDGES):\n",
    "      df_importer.edges_dataframe(edges,\n",
    "         source_id_space        = \"transaction\",\n",
    "         destination_id_space   = \"transaction\",\n",
    "            #\n",
    "         source_column          = \"txId2\",\n",
    "         destination_column     = \"txId1\",\n",
    "            #\n",
    "         type                   = \"rev_tx_flow\"\n",
    "         )\n",
    "    \n",
    "        \n",
    "print(\"--\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff08930-554c-4e3f-a046-8b62b922911d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Checking number of partitions in the graph ..\n",
    "\n",
    "my_graph.num_partitions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97c7dea-c574-4cba-b7f4-b17b8b6c7ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Check our work from above ..\n",
    "\n",
    "display(print(f\"Number of nodes: {my_graph.num_nodes()}\"))\n",
    "display(print(f\"Number of edges: {my_graph.num_edges()}\"))\n",
    "\n",
    "\n",
    "#  Number of nodes: 203769\n",
    "#  Number of edges: 468710\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963f414f-5d1b-4f7c-96ad-e3b336625650",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Check the schema and counts ..\n",
    "\n",
    "\n",
    "l_result1 = my_graph.query(\"\"\"\n",
    "\n",
    "   MATCH (a) WITH DISTINCT LABELS(a) AS temp, COUNT(a) AS tempCnt\n",
    "   UNWIND temp AS label\n",
    "   RETURN label, SUM(tempCnt) AS cnt\n",
    "   ORDER BY label\n",
    "   \n",
    "   \"\"\")\n",
    "display(print(l_result1))\n",
    "\n",
    "\n",
    "l_result2 = my_graph.query(\"\"\"\n",
    "\n",
    "   MATCH (m)-[r]->(n) \n",
    "   WITH DISTINCT TYPE(r) AS temp, COUNT(r) AS tempCnt\n",
    "   RETURN temp, tempCnt\n",
    "   ORDER BY temp\n",
    "\n",
    "   \"\"\")\n",
    "display(print(l_result2))\n",
    "\n",
    "\n",
    "#        cnt        label\n",
    "#  0  203769  transaction\n",
    "#  \n",
    "#            temp  tempCnt\n",
    "#  0  rev_tx_flow   234355\n",
    "#  1      tx_flow   234355\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c9a76e-da54-4a12-91c4-487c8492cbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Compared to above; pure schema ..\n",
    "\n",
    "\n",
    "my_graph.query(\"CALL graph.schema() RETURN *\")\n",
    "\n",
    "\n",
    "#        neighbor                         nodeType         properties\n",
    "#        -----------------------------------------------------------------------------------------\n",
    "#    0                                    [transaction]    agg_feat_100,agg_feat_101,agg_feat_102,agg_fea...\n",
    "#    1   ([transaction]::[rev_tx_flow])   [transaction] \t\n",
    "#    2   ([transaction]::[tx_flow])       [transaction] \t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc475788-b950-479b-ada9-406407d14be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Ability to set labels and more on the visualization ..\n",
    "\n",
    "from katana_visualization_widget import GraphVisOptions, NodeVisOption, EdgeVisOption, ANY\n",
    "\n",
    "options = GraphVisOptions(\n",
    "   node_options = [\n",
    "      NodeVisOption(ANY, label = \"title\")\n",
    "   ])\n",
    "\n",
    "print(\"--\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae5c58c-2212-47be-80e3-630f07e1b09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Visualize the graph;  using a small sample ..\n",
    "\n",
    "l_result = my_graph.query(\"\"\"\n",
    "\n",
    "   MATCH (n: transaction )  - [ r ] ->  (m: transaction)\n",
    "   RETURN n, m, r\n",
    "   LIMIT 1000\n",
    "   \n",
    "   \"\"\", contextualize=True)\n",
    "\n",
    "l_result.view()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33068ed9-81f8-41ed-b74c-29492bae35ba",
   "metadata": {
    "tags": []
   },
   "source": [
    "#  Result set ..\n",
    "\n",
    "<img src=\"./01_Images/result4.png\" alt=\"Result\" style=\"width: 5000px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fceb7d0e-281f-440d-9ef3-c144eab0a1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0879927c-154c-442c-9cea-9c53794067ab",
   "metadata": {},
   "source": [
    "\n",
    "   To initialize features for the GNN, combine all node properties into a feature vector and save as a new feature on the graph. \n",
    "\n",
    "   Katana Graph supports saving binary feature vectors as individual properties on the graph. In this case, we save 3 different feature vectors: \n",
    "\n",
    "      1. local_feats   - raw features provided for each transaction\n",
    "      2. agg_feats     - aggregated features from each node's neighborhood\n",
    "    \n",
    "      3. h_init        - both (local_feats + agg_feats) combined into one feature. \n",
    "        \n",
    "         This will be the starting point for our GNN\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1a336b-f06f-42e8-84ea-024975ac64ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Check the given property keys ..\n",
    "\n",
    "l_result = my_graph.query(\"\"\"\n",
    "\n",
    "   MATCH (n: transaction )\n",
    "   RETURN n.h_init, n.local_feats, n.agg_feats\n",
    "   LIMIT 5\n",
    "   \n",
    "   \"\"\", contextualize=True)\n",
    "\n",
    "l_result.view()\n",
    "\n",
    "\n",
    "#  Count: 5 rows\n",
    "#  n.agg_feats   n.h_init   n.local_feats\n",
    "#  None\t         None       None\n",
    "#  None\t         None       None\n",
    "#  None\t         None       None\n",
    "#  None\t         None       None\n",
    "#  None\t         None       None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bcd1b9-d232-4688-be0f-3d24a6f28ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  This next step requires katana_ai.py be placed o nthe worker nodes,\n",
    "#\n",
    "#     .  Get the name of just one worker node. \n",
    "#        Program 70* will do this.\n",
    "#\n",
    "#     .  Sample SCP,\n",
    "#\n",
    "#        First, on worker node, make a folder,  (ssh in and),\n",
    "#           mkdir 05_Packages\n",
    "#\n",
    "#        Then,\n",
    "#           gcloud compute scp  katana_ai.py  known-mongrel-compute-0:/home/farrell_katanagraph_com/05_Packages\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8af7b8-a560-41ae-909d-4d2d79cee2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Last of data mung; mutate the graph for GNN operation ..\n",
    "\n",
    "def run_feature_init(g): \n",
    "    \n",
    "   import os\n",
    "   import sys\n",
    "      #\n",
    "   import numpy as np\n",
    "\n",
    "    \n",
    "   #  Must copy katana_ai.py to Workder node, $HOME/05_Packages\n",
    "   #     (This seems finicky; finding the file.)\n",
    "   #\n",
    "   # print(os.getcwd())\n",
    "   # sys.path.append(os.path.join(\"/home/farrell_katanagraph_com/05_Packages\"))\n",
    "   #\n",
    "   sys.path.insert(1, \"/home/farrell_katanagraph_com/05_Packages\")\n",
    "    \n",
    "   from katana_ai import get_node_property_list, visualize_embeddings, train_test_split_mask, save_features_to_graph\n",
    "\n",
    "    \n",
    "    \n",
    "   # extract features\n",
    "   #\n",
    "   local_feats = get_node_property_list(g, property_list=local_feats_name)\n",
    "   agg_feats   = get_node_property_list(g, property_list=agg_feats_name)\n",
    "      #\n",
    "   feat_vec    = np.concatenate([local_feats, agg_feats], axis=-1)\n",
    "    \n",
    "    \n",
    "   # save new features vector to graph\n",
    "   #\n",
    "   g = save_features_to_graph(g, feat_vec,    feature_name=\"h_init\"     )\n",
    "   g = save_features_to_graph(g, local_feats, feature_name=\"local_feats\")\n",
    "   g = save_features_to_graph(g, agg_feats,   feature_name=\"agg_feats\"  )\n",
    "    \n",
    "    \n",
    "   # create train/test split mask\n",
    "   #\n",
    "   g = train_test_split_mask(g, train_test_validation_split=[0.8, 0.15, 0.05])\n",
    "   g.write()\n",
    "\n",
    "    \n",
    "my_graph.run(lambda g: run_feature_init(g))\n",
    "\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05214c4-2fa1-466a-b65d-eb51171b524d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Check the given property keys ..\n",
    "\n",
    "l_result = my_graph.query(\"\"\"\n",
    "\n",
    "   MATCH (n: transaction )\n",
    "   RETURN n.h_init, n.local_feats, n.agg_feats\n",
    "   LIMIT 5\n",
    "   \n",
    "   \"\"\", contextualize=True)\n",
    "\n",
    "l_result.view()\n",
    "\n",
    "\n",
    "#  Count: 5 rows\n",
    "#  n.agg_feats                                                                n.h_init                                                                 n.local_feats\n",
    "#  [-0.1687650829553604, -0.20127403736114502, -0.11681671440601349, ... ]    [-0.16940271854400635, -0.18466755747795105, -1.201368808746338, ... ]   [-0.16940271854400635, -0.18466755747795105, -1.201368808746338, ...\n",
    "#        ...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb419b7-ed3d-48f9-b8a7-798b2a5a86c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Generate run ID for Tensorboard ..\n",
    "\n",
    "import uuid\n",
    "\n",
    "run_id = uuid.uuid4().hex\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d26572f-9d3a-4e02-8b2d-edc33d5e3ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b3c2d8-b4a7-452f-aab7-9d7fbe724994",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Need to place a script on the worker nodes that install TensorFlow and related-\n",
    "#\n",
    "#  Full script at,\n",
    "#     https://github.com/KatanaGraph/solutions/blob/main/fsi/setup_scripts/gcp_dgl_worker_install.sh\n",
    "#\n",
    "#  need lines 1-6, 23-33, which are now in 05_Packages/install_tensor.sh\n",
    "#\n",
    "#  From, ccd,\n",
    "\n",
    "   . ./20_Defaults.sh\n",
    "      #\n",
    "   FILE_NAME=\"/workbook/04_Version04/N1_GNN_FinTech/05_Packages/install_tensor.sh\"\n",
    "   l_zone=`70* | grep \"compute\\-\" | head -1 | awk '{print $2}'`\n",
    "    \n",
    "   for l_host in `70* | grep \"compute\\-\" | awk '{print $1}'`\n",
    "      do\n",
    "      gcloud compute scp  ${MY_CLUSTER_DIR}/${FILE_NAME}  ${l_host}:/tmp \n",
    "      gcloud compute ssh --zone ${l_zone} --project ${CLOUDSDK_CORE_PROJECT} ${l_host} --  \". /tmp/install_tensor.sh\"\n",
    "      done\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ac027a-1f2e-4555-a5dc-809f633611fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c2973a-329d-4d93-a671-19c4dd3f85be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def analyze_features(g): \n",
    "    \n",
    "   import os\n",
    "   import sys\n",
    "\n",
    "    \n",
    "   #  Must copy katana_ai.py to Workder node, $HOME/05_Packages\n",
    "   #\n",
    "   sys.path.append(os.path.join(\"/home/farrell_katanagraph_com/05_Packages\"))\n",
    "\n",
    "   from katana_ai import visualize_embeddings\n",
    "    \n",
    "   from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "    \n",
    "   writer = SummaryWriter(f\"/tmp/tensorboard/elliptic-embed-init-{run_id}\")\n",
    "    \n",
    "   # analyze features in tensorboard\n",
    "   #\n",
    "   visualize_embeddings(g, writer, feature_name=\"h_init\", target_name=\"target\", filter_node_type=\"Classified_Txn\", sample_size=2000)\n",
    "   visualize_embeddings(g, writer, feature_name=\"local_feats\", target_name=\"target\", filter_node_type=\"Classified_Txn\", sample_size=2000)\n",
    "   visualize_embeddings(g, writer, feature_name=\"agg_feats\", target_name=\"target\", filter_node_type=\"Classified_Txn\", sample_size=2000)\n",
    "    \n",
    "    \n",
    "my_graph.run(lambda g: analyze_features(g))\n",
    "\n",
    "\n",
    "print(f\"See results at https://demo-finance-tensorboard.katanagraph.com/\")\n",
    "print(f\"Run ID: elliptic-embed-init-{run_id}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef71081-4856-484a-8939-df160297c067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45eae002-d27f-412e-84cf-28473a6137ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Check the given property keys ..\n",
    "\n",
    "l_result = my_graph.query(\"\"\"\n",
    "\n",
    "   MATCH (n: transaction )\n",
    "   RETURN n.target\n",
    "   LIMIT 5\n",
    "   \n",
    "   \"\"\", contextualize=True)\n",
    "\n",
    "l_result.view()\n",
    "\n",
    "\n",
    "#  Count: 5 rows\n",
    "#  n.target\n",
    "#  -1.0\n",
    "#  -1.0\n",
    "#  -1.0\n",
    "#  -1.0\n",
    "#  -1.0\n",
    "\n",
    "l_result = my_graph.query(\"\"\"\n",
    "\n",
    "   MATCH (n: transaction )\n",
    "   RETURN AVG(n.target)\n",
    "   \n",
    "   \"\"\", contextualize=True)\n",
    "\n",
    "l_result.view()\n",
    "\n",
    "\n",
    "#  Count: 1 rows\n",
    "#  AVG(n.target)\n",
    "#  -0.7491816714024214\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1834d123-6a49-49e0-a2b6-27599a0daae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  GNN Training ..\n",
    "\n",
    "\n",
    "import argparse\n",
    "import numpy\n",
    "\n",
    "\n",
    "args = argparse.Namespace(\n",
    "   feat_name             = \"h_init\",\n",
    "    \n",
    "   label_name            = \"target\",\n",
    "    \n",
    "   label_dtype           = numpy.float32,\n",
    "   split_name            = \"train_test_val_mask\",\n",
    "   distributed_execution = True,\n",
    "   tensorboard_dir       = f\"/tmp/tensorboard/elliptic-remote-{run_id}\",\n",
    "   model_dir             = \"/tmp/models\",\n",
    "    \n",
    "   # katana_ai_dir       = \"/home/gsteck_katanagraph_com/solutions/fsi/src\",\n",
    "   katana_ai_dir         = \"/home/farrell_katanagraph_com/05_Packages \",\n",
    "    \n",
    "   pred_node_label       = \"Classified_Txn\",\n",
    "   pred_node_label_prop  = \"node_type\",\n",
    "   pos_weight            = 8,\n",
    "   in_dim                = 165,\n",
    "   hidden_dim            = 256,\n",
    "   train_fan_in          = \"100,100,100,100\",\n",
    "   test_fan_in           = \"100,100,100,100\",\n",
    "   num_layers            = 4,\n",
    "   out_dim               = 1,\n",
    "   minibatch_size        = 1024,\n",
    "   max_minibatches       = 20,\n",
    "   lr                    = 0.001,\n",
    "   dropout               = 0.2,\n",
    "   num_epochs            = 100\n",
    ")\n",
    "\n",
    "print(\"--\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d3f5cc-91ac-4124-96cc-8f36912ce745",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def run_gnn(graph, args):\n",
    "    \n",
    "    from calendar import c\n",
    "    import torch\n",
    "    import numpy\n",
    "    import katana\n",
    "    from katana_enterprise.distributed.pytorch import init_workers\n",
    "    from katana_enterprise.ai.data import PyGNodeSubgraphSampler, SampledSubgraphConfig \n",
    "    from katana_enterprise.ai.data import NodeDataLoader\n",
    "    from torch_geometric.nn import SAGEConv\n",
    "    from torch.nn.parallel import DistributedDataParallel as torch_DDP\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "    import sys, os\n",
    "    sys.path.append(os.path.join(args.katana_ai_dir))\n",
    "    from katana_ai import get_split, train_model\n",
    "    \n",
    "    os.environ['MODIN_ENGINE']='python'\n",
    "    #katana.distributed.initialize()\n",
    "\n",
    "    katana.set_active_threads(32)\n",
    "    exec_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # model definition\n",
    "    class DistSAGE(torch.nn.Module):\n",
    "        def __init__(self, in_dim, hidden_dim, out_dim, num_layers,\n",
    "                     dropout):\n",
    "            super(DistSAGE, self).__init__()\n",
    "\n",
    "            self.convs = torch.nn.ModuleList()\n",
    "            self.convs.append(SAGEConv(in_dim, hidden_dim))\n",
    "            self.bns = torch.nn.ModuleList()\n",
    "            self.bns.append(torch.nn.BatchNorm1d(hidden_dim))\n",
    "            for _ in range(num_layers - 2):\n",
    "                self.convs.append(SAGEConv(hidden_dim, hidden_dim))\n",
    "                self.bns.append(torch.nn.BatchNorm1d(hidden_dim))\n",
    "            self.convs.append(SAGEConv(hidden_dim, out_dim))\n",
    "            self.activation = torch.nn.functional.relu\n",
    "            self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "        def reset_parameters(self):\n",
    "            for conv in self.convs:\n",
    "                conv.reset_parameters()\n",
    "            for bn in self.bns:\n",
    "                bn.reset_parameters()\n",
    "\n",
    "        def forward(self, data):\n",
    "            # unpack data loader\n",
    "            x, edges = data.x, data.adjs\n",
    "            #x.to(exec_device)\n",
    "            #edges.to(exec_device)\n",
    "\n",
    "            for i, conv in enumerate(self.convs):\n",
    "                # for multilayer, set x_target for each layer\n",
    "                x_target = x[:data.dest_count[i]]\n",
    "                x = conv((x, x_target), edges[i])\n",
    "                if i != len(self.convs) - 1:\n",
    "                    x = self.bns[i](x)\n",
    "                    x = self.activation(x)\n",
    "                    x = self.dropout(x) \n",
    "                    embed = x\n",
    "            return x, embed\n",
    "    \n",
    "    # initialize torch mpi process\n",
    "    main_start = time.time()\n",
    "    if args.distributed_execution:\n",
    "        init_workers()\n",
    "\n",
    "    # tensorboard writer\n",
    "    writer = SummaryWriter(args.tensorboard_dir)\n",
    "\n",
    "    # split train / test node idx\n",
    "    train_nodes = get_split(graph, 0, split_name=args.split_name, node_label=args.pred_node_label, node_label_prop=args.pred_node_label_prop)\n",
    "    test_nodes = get_split(graph, 1, split_name=args.split_name, node_label=args.pred_node_label, node_label_prop=args.pred_node_label_prop)\n",
    "\n",
    "    # initialize the multiminibatch sampler\n",
    "    train_sampler = PyGNodeSubgraphSampler(\n",
    "        graph, \n",
    "        SampledSubgraphConfig(\n",
    "        layer_fan=[int(fan_in) for fan_in in args.train_fan_in.split(',')], \n",
    "            max_minibatches=args.max_minibatches, \n",
    "            batch_props_to_pull=args.max_minibatches,\n",
    "            feat_prop_name=args.feat_name,\n",
    "            label_prop_name=args.label_name,\n",
    "            label_dtype=args.label_dtype,\n",
    "            multilayer_export=True\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # test sampler used for evaluation; it samples 100s per hop to simulate getting\n",
    "    test_sampler = PyGNodeSubgraphSampler(\n",
    "        graph, \n",
    "        SampledSubgraphConfig(\n",
    "        layer_fan=[int(fan_in) for fan_in in args.train_fan_in.split(',')], \n",
    "            max_minibatches=args.max_minibatches, \n",
    "            batch_props_to_pull=args.max_minibatches,\n",
    "            feat_prop_name=args.feat_name,\n",
    "            label_prop_name=args.label_name,\n",
    "            label_dtype=args.label_dtype,\n",
    "            pull_edge_types=args.load_edge_types,\n",
    "            multilayer_export=True\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # shuffle seeds between epochs + balance seed nodes across hosts\n",
    "    train_dataloader = NodeDataLoader(\n",
    "        train_sampler, \n",
    "        local_batch_size=args.minibatch_size, \n",
    "        node_ids=train_nodes,  \n",
    "        shuffle=True, \n",
    "        drop_last=True,\n",
    "        balance_seeds=True)\n",
    "    test_dataloader = NodeDataLoader(\n",
    "        test_sampler, \n",
    "        local_batch_size=args.minibatch_size, \n",
    "        node_ids=test_nodes, \n",
    "        balance_seeds=True)\n",
    "\n",
    "    # model initialization\n",
    "    model = DistSAGE(\n",
    "        in_dim=args.in_dim, \n",
    "        hidden_dim=args.hidden_dim, \n",
    "        out_dim=args.out_dim, \n",
    "        num_layers=args.num_layers,\n",
    "        dropout=args.dropout\n",
    "    ).to(exec_device)\n",
    "\n",
    "    if args.distributed_execution:\n",
    "        model = torch_DDP(model)\n",
    "    \n",
    "    # optimizer and loss fn\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "    loss_function = torch.nn.BCEWithLogitsLoss(pos_weight=torch.Tensor([args.pos_weight]))\n",
    "    \n",
    "    # train model\n",
    "    train_model(model, loss_function, optimizer, writer, train_dataloader, test_dataloader, args)\n",
    "    \n",
    "    # save model\n",
    "    #ts = time.time()\n",
    "    #torch.save(model.state_dict(), os.path.join(args.model_dir, 'graph_sage.'+str(ts)+'.pth'))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850a5f33-fe8f-4a98-8ee8-73f9d5a51fc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a2e2f3-32d3-45b7-ac36-383ce12df632",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c58edde-ff67-4c23-8adc-32e0657b9f2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
