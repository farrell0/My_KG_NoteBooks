{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c54fd830-1317-4a52-b610-ef9716e5d744",
   "metadata": {
    "tags": []
   },
   "source": [
    "#  Setup: Display options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a38ebd7-d1d9-47a8-955c-a8270b024543",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Setting display options \n",
    "\n",
    "import pandas as pd\n",
    "   #\n",
    "pd.set_option(\"display.width\", 480)\n",
    "\n",
    "#  Sets horizontal scroll for wide outputs\n",
    "#\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"\"))\n",
    "\n",
    "from tabulate import tabulate\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab7ce90-67cf-462c-94a6-3c8c9d0754b3",
   "metadata": {},
   "source": [
    "# Setup: Connectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87b75ba-c072-4111-8cca-4b4f394a0fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from katana import remote\n",
    "\n",
    "my_client = remote.Client()\n",
    "\n",
    "print(my_client)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a77f99-f11b-4105-ab43-17ce605b15e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "NUM_PARTITIONS  = 3\n",
    "   #\n",
    "DB_NAME         = \"my_db\"\n",
    "GRAPH_NAME      = \"my_graph\"\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2292b8-7df0-49a0-b3da-523f56cfe4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  DELETE ALL GRAPHS\n",
    "\n",
    "for l_database in my_client.databases():\n",
    "   for l_graph in my_client.get_database(name=l_database.name).graphs_in_database():\n",
    "      l_handle=my_client.get_database(name=l_database.name).get_graph_by_id(id=l_graph.graph_id)\n",
    "      l_handle.delete()\n",
    "\n",
    "for l_graph in my_client.graphs():\n",
    "   print(\"GRAPH ID: \", l_graph.graph_id, \"      GRAPH Version: \", l_graph.version)\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f69a0e-451b-4679-b49c-5db1208d4c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  DELETE ALL DATABASES\n",
    "\n",
    "for l_database in my_client.databases():\n",
    "   if (l_database.name != \"default\"):\n",
    "      my_client.get_database(name=l_database.name).delete_database()\n",
    "      print(\"--\")\n",
    "\n",
    "for l_database in my_client.databases():\n",
    "   print(\"DB ID: \", l_database.database_id, \"     DB Name: \", l_database.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b3bc00-fafb-481d-ba5a-8b52ae7d6e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  CREATE DATABASE\n",
    "\n",
    "my_database = my_client.create_database(name=DB_NAME)\n",
    "\n",
    "print(my_database.database_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ba088b-2ec5-4b2a-8171-3c87ddffb47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  CREATE GRAPH\n",
    "\n",
    "my_graph = my_client.get_database(name=DB_NAME).create_graph(name=GRAPH_NAME, num_partitions=NUM_PARTITIONS)\n",
    "\n",
    "print(my_graph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863f948b-cec2-42fb-9287-2a3bcf3e8b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  CONNECT TO GRAPH\n",
    "\n",
    "my_graph, *_ = my_client.get_database(name=DB_NAME).find_graphs_by_name(GRAPH_NAME)\n",
    "\n",
    "print(my_graph)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3685a36-d987-4ad8-a209-7115d726ac03",
   "metadata": {},
   "source": [
    "# Step 01:  Process First CSV file .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0351b0fd-6237-4d7a-99f1-dec2150ec4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  This data was sourced from,\n",
    "#     https://mtsamples.com/site/pages/browse.asp?type=96-Hematology%20-%20Oncology\n",
    "#\n",
    "\n",
    "#  Column headers,\n",
    "#     id,description,medical_specialty,sample_name,transcription,keywords\n",
    "#\n",
    "df_PatientVisitNodes = pd.read_csv(\"./10_Data/33_32_Processed.txt\", header = \"infer\", delimiter = \",\")\n",
    "\n",
    "\n",
    "print(\"Number of CSV input lines: %d\" % (len(df_PatientVisitNodes)))\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "#  And add a \"LABEL\" property\n",
    "#\n",
    "df_PatientVisitNodes[\"LABEL\"]      = df_PatientVisitNodes.id.map(lambda x: \"PatientVisit\"           )\n",
    "\n",
    "\n",
    "#  The original/imported \"id\" column is Null/None for some rows. Fix that-\n",
    "#\n",
    "df_PatientVisitNodes = df_PatientVisitNodes.rename(columns={\"id\": \"id_original\"})\n",
    "   #\n",
    "l_initial = 4000 \n",
    "   #\n",
    "df_PatientVisitNodes[\"id\"] = range(l_initial, len(df_PatientVisitNodes) + l_initial)\n",
    "df_PatientVisitNodes[\"id\"] = df_PatientVisitNodes.id.map(lambda x: \"PV-\" + str(x) )\n",
    "\n",
    "\n",
    "for l_each in df_PatientVisitNodes.head(5).itertuples():\n",
    "   print(\"Id: %s   Desc: %-32s   Specialty: %-32s   Name: %-32s   Transcription: %-32s   Keywords: %s\" % (\n",
    "      l_each.id,\n",
    "      l_each.description[0:31],\n",
    "      l_each.medical_specialty[0:31],\n",
    "      l_each.sample_name[0:31],\n",
    "      l_each.transcription[0:31],\n",
    "      l_each.keywords[0:31],\n",
    "      ))\n",
    "        \n",
    "        \n",
    "print(\"\")\n",
    "   #\n",
    "print(\"--\")\n",
    "\n",
    "#  Sample output,\n",
    "#\n",
    "#     Number of CSV input lines: 4999\n",
    "#     \n",
    "#     Id: PV-4000   Desc:  A 23-year-old white female pre    Specialty:  Allergy / Immunology              Name:  Allergic Rhinitis                 Transcription: SUBJECTIVE:,  This 23-year-old     Keywords: allergy / immunology, allergic \n",
    "#     Id: PV-4001   Desc:  Consult for laparoscopic gastr    Specialty:  Bariatrics                        Name:  Laparoscopic Gastric Bypass Co    Transcription: PAST MEDICAL HISTORY:, He has d    Keywords: bariatrics, laparoscopic gastri\n",
    "#     Id: PV-4002   Desc:  Consult for laparoscopic gastr    Specialty:  Bariatrics                        Name:  Laparoscopic Gastric Bypass Co    Transcription: HISTORY OF PRESENT ILLNESS: , I    Keywords: bariatrics, laparoscopic gastri\n",
    "#     Id: PV-4003   Desc:  2-D M-Mode. Doppler.              Specialty:  Cardiovascular / Pulmonary        Name:  2-D Echocardiogram - 1            Transcription: 2-D M-MODE: , ,1.  Left atrial     Keywords: cardiovascular / pulmonary, 2-d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e95864-6b0a-44f4-a3df-1222910cd783",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  From the sample above,\n",
    "#\n",
    "#     .  Check for any Null/None values, and fix those\n",
    "#\n",
    "\n",
    "def f_check(i_arg1):\n",
    "   l_return = ( i_arg1.isnull().sum() / i_arg1.index.size * 100)\n",
    "      #\n",
    "   print(l_return)\n",
    "   print(\"\")\n",
    "\n",
    "f_check(df_PatientVisitNodes)\n",
    "\n",
    "#  Sample output,\n",
    "#\n",
    "#     id_original          0.000000\n",
    "#     description          0.000000\n",
    "#     medical_specialty    0.000000\n",
    "#     sample_name          0.000000\n",
    "#     transcription        0.660132\n",
    "#     keywords             0.000000\n",
    "#     LABEL                0.000000\n",
    "#     id                   0.000000\n",
    "#     dtype: float64\n",
    "\n",
    "\n",
    "df_PatientVisitNodes[\"transcription_clean\"] = df_PatientVisitNodes[\"transcription\"].fillna(\"None\"[0])\n",
    "   #\n",
    "f_check(df_PatientVisitNodes)\n",
    "\n",
    "#  Sample output,\n",
    "#\n",
    "#     id_original            0.000000\n",
    "#     description            0.000000\n",
    "#     medical_specialty      0.000000\n",
    "#     sample_name            0.000000\n",
    "#     transcription          0.660132\n",
    "#     keywords               0.000000\n",
    "#     LABEL                  0.000000\n",
    "#     id                     0.000000\n",
    "#     transcription_clean    0.000000\n",
    "#     dtype: float64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d3ca45-ae29-41f1-b191-3e6803a54330",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  From the sample above,\n",
    "#\n",
    "#     .  Check both medical_specialty and sample_name for uniquess\n",
    "#\n",
    "\n",
    "print(\"Total rows: %d   Unique-Specialty: %d   Unique-Name: %d\" % (\n",
    "   len(df_PatientVisitNodes),\n",
    "   len(df_PatientVisitNodes.medical_specialty.unique()),\n",
    "   len(df_PatientVisitNodes.sample_name.unique()),\n",
    "   ))\n",
    "\n",
    "#  Sample output\n",
    "#\n",
    "#     Total rows: 4999   Unique-Specialty: 40   Unique-Name: 2377\n",
    "\n",
    "\n",
    "#  Check ths distribution for medical_specialty\n",
    "#\n",
    "print(df_PatientVisitNodes.value_counts(subset=[\"medical_specialty\"]))\n",
    "\n",
    "#  Sample output,\n",
    "#\n",
    "#     medical_specialty             \n",
    "#      Surgery                          1103\n",
    "#      Consult - History and Phy.        516\n",
    "#      Cardiovascular / Pulmonary        372\n",
    "#      Orthopedic                        355\n",
    "#      Radiology                         273\n",
    "#      General Medicine                  259\n",
    "#      Gastroenterology                  230\n",
    "#      Neurology                         223\n",
    "#      SOAP / Chart / Progress Notes     166\n",
    "#      Obstetrics / Gynecology           160\n",
    "#      Urology                           158\n",
    "#      Discharge Summary                 108\n",
    "#      ENT - Otolaryngology               98\n",
    "#      Neurosurgery                       94\n",
    "#      Hematology - Oncology              90\n",
    "#      Ophthalmology                      83\n",
    "#      Nephrology                         81\n",
    "#      Emergency Room Reports             75\n",
    "#      Pediatrics - Neonatal              70\n",
    "#      Pain Management                    62\n",
    "#      Psychiatry / Psychology            53\n",
    "#      Office Notes                       51\n",
    "#      Podiatry                           47\n",
    "#      Dermatology                        29\n",
    "#      Dentistry                          27\n",
    "#      Cosmetic / Plastic Surgery         27\n",
    "#      Letters                            23\n",
    "#      Physical Medicine - Rehab          21\n",
    "#      Sleep Medicine                     20\n",
    "#      Endocrinology                      19\n",
    "#      Bariatrics                         18\n",
    "#      IME-QME-Work Comp etc.             16\n",
    "#      Chiropractic                       14\n",
    "#      Diets and Nutritions               10\n",
    "#      Rheumatology                       10\n",
    "#      Speech - Language                   9\n",
    "#      Autopsy                             8\n",
    "#      Lab Medicine - Pathology            8\n",
    "#      Allergy / Immunology                7\n",
    "#      Hospice - Palliative Care           6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206461e9-fd75-43d9-9f66-b36027593a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#   Build a set of Nodes and Edges/Relationships from \"medical_specialty\" ..\n",
    "#\n",
    "\n",
    "l_MedicalSpecialtyNodes_Set                = set()\n",
    "l_MedicalSpecialtyNodes                    = []\n",
    "   #\n",
    "l_PatientVisitToMedicalSpecialtyEdges_N    = []\n",
    "l_PatientVisitToMedicalSpecialtyEdges_S    = []\n",
    "\n",
    "\n",
    "#  Start with just the nodes\n",
    "#\n",
    "#     .  Use a set() to generate a unique list of values\n",
    "#     .  Convert to an array of records\n",
    "#     .  And convert that to a DataFrame\n",
    "#\n",
    "for l_each in df_PatientVisitNodes.itertuples():\n",
    "   #\n",
    "   #  strip(),  there was leading whitespace on our column value\n",
    "   #\n",
    "   l_MedicalSpecialtyNodes_Set.add(l_each.medical_specialty.strip())\n",
    "      #\n",
    "for l_each in zip( range(len(l_MedicalSpecialtyNodes_Set)), l_MedicalSpecialtyNodes_Set):\n",
    "   l_recd = { \"id\": (\"MS-\" + str(l_each[0])), \"medical_specialty\" : l_each[1], \"LABEL\": \"MedicalSpecialty\" }\n",
    "      # \n",
    "   l_MedicalSpecialtyNodes.append(l_recd)\n",
    "      #\n",
    "df_MedicalSpecialtyNodes = pd.DataFrame.from_records(l_MedicalSpecialtyNodes)\n",
    "   #\n",
    "print(\"Number of Medical Specialities: %d   %d   %d\" % (\n",
    "   len(l_MedicalSpecialtyNodes_Set),\n",
    "   len(l_MedicalSpecialtyNodes),\n",
    "   len(df_MedicalSpecialtyNodes),\n",
    "   ))\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "#  Check our work\n",
    "#\n",
    "for l_each in df_MedicalSpecialtyNodes.head(5).itertuples():\n",
    "   print(\"Id: %-8s   Medical Specialty: %-32s   LABEL: %s\" % (\n",
    "      l_each.id,\n",
    "      l_each.medical_specialty,\n",
    "      l_each.LABEL,\n",
    "      ))\n",
    "\n",
    "\n",
    "#  And now create the edges\n",
    "#\n",
    "for l_each in df_PatientVisitNodes.itertuples():\n",
    "   #\n",
    "   #  Get the id from the medical_specialty data set\n",
    "   #\n",
    "   #     .  Again we strip because of the leading whitespace in the source data\n",
    "   #     .  This form of equality returns a Series, hence the to_list() and [0]\n",
    "   #\n",
    "   l_msid = df_MedicalSpecialtyNodes.loc[df_MedicalSpecialtyNodes[\"medical_specialty\"] == l_each.medical_specialty.strip()].id.to_list()[0]\n",
    "      # \n",
    "   l_recd_N = { \"start_id\": l_each.id, \"end_id\"  : l_msid   , \"TYPE\": \"IS_OF_SPECIALTY\" }\n",
    "   l_recd_S = { \"start_id\": l_msid   , \"end_id\"  : l_each.id, \"TYPE\": \"IS_OF_SPECIALTY\" }\n",
    "      #\n",
    "   l_PatientVisitToMedicalSpecialtyEdges_N.append(l_recd_N)\n",
    "   l_PatientVisitToMedicalSpecialtyEdges_S.append(l_recd_S)\n",
    "\n",
    "\n",
    "df_PatientVisitToMedicalSpecialtyEdges_N = pd.DataFrame.from_records(l_PatientVisitToMedicalSpecialtyEdges_N)\n",
    "df_PatientVisitToMedicalSpecialtyEdges_S = pd.DataFrame.from_records(l_PatientVisitToMedicalSpecialtyEdges_S)\n",
    "   #\n",
    "print(\"\")\n",
    "print(\"Number of Edges,  PatientVisit --> MedicalSpecialty: %d   %d\" % (\n",
    "   len(df_PatientVisitToMedicalSpecialtyEdges_N),\n",
    "   len(df_PatientVisitToMedicalSpecialtyEdges_S),\n",
    "))\n",
    "\n",
    "\n",
    "#  Check our work\n",
    "#\n",
    "print(\"\")\n",
    "   #\n",
    "for l_each in df_PatientVisitToMedicalSpecialtyEdges_N.head(5).itertuples():\n",
    "   print(\"Start Id: %-8s   End Id: %-8s   TYPE: %s\" % (l_each.start_id, l_each.end_id, l_each.TYPE))\n",
    "        \n",
    "print(\"\")\n",
    "   #\n",
    "for l_each in df_PatientVisitToMedicalSpecialtyEdges_S.head(5).itertuples():\n",
    "   print(\"Start Id: %-8s   End Id: %-8s   TYPE: %s\" % (l_each.start_id, l_each.end_id, l_each.TYPE))\n",
    "        \n",
    "        \n",
    "#  Sample output,\n",
    "#\n",
    "#     Number of Medical Specialities: 40   40   40\n",
    "#     \n",
    "#     Id: MS-0       Medical Specialty: Surgery                            LABEL: MedicalSpecialty\n",
    "#     Id: MS-1       Medical Specialty: Neurosurgery                       LABEL: MedicalSpecialty\n",
    "#     Id: MS-2       Medical Specialty: Orthopedic                         LABEL: MedicalSpecialty\n",
    "#     Id: MS-3       Medical Specialty: SOAP / Chart / Progress Notes      LABEL: MedicalSpecialty\n",
    "#     \n",
    "#     Number of Edges,  PatientVisit --> MedicalSpecialty: 4999   4999\n",
    "#     \n",
    "#     Start Id: PV-4000    End Id: MS-9       TYPE: IS_OF_SPECIALTY\n",
    "#     Start Id: PV-4001    End Id: MS-16      TYPE: IS_OF_SPECIALTY\n",
    "#     Start Id: PV-4002    End Id: MS-16      TYPE: IS_OF_SPECIALTY\n",
    "#     Start Id: PV-4003    End Id: MS-10      TYPE: IS_OF_SPECIALTY\n",
    "#     \n",
    "#     Start Id: MS-9       End Id: PV-4000    TYPE: IS_OF_SPECIALTY\n",
    "#     Start Id: MS-16      End Id: PV-4001    TYPE: IS_OF_SPECIALTY\n",
    "#     Start Id: MS-16      End Id: PV-4002    TYPE: IS_OF_SPECIALTY\n",
    "#     Start Id: MS-10      End Id: PV-4003    TYPE: IS_OF_SPECIALTY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acc60bc-e32d-44a5-96c2-aa32c86aab29",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#\n",
    "#  **  This cell takes 1-2 minutes to complete ..\n",
    "#\n",
    "\n",
    "\n",
    "#  Same basic function as above; split out \"keywords\"\n",
    "#\n",
    "\n",
    "l_KeywordNodes_Set                = set()\n",
    "l_KeywordNodes                    = []\n",
    "   #\n",
    "l_PatientVisitToKeywordEdges_N    = []\n",
    "l_PatientVisitToKeywordEdges_S    = []\n",
    "\n",
    "\n",
    "#  Start with just the nodes\n",
    "#\n",
    "#     .  Use a set() to generate a unique list of values\n",
    "#     .  Convert to an array of records\n",
    "#     .  And convert that to a DataFrame\n",
    "#\n",
    "for l_each in df_PatientVisitNodes.itertuples():\n",
    "   #\n",
    "   #  Doing this a little differently than above because lists are mutable\n",
    "   #\n",
    "   l_KeywordNodes_Set.update(l_each.keywords.split(\",\"))\n",
    "      #\n",
    "for l_each in zip( range(len(l_KeywordNodes_Set)), l_KeywordNodes_Set):\n",
    "   if (len(l_each[1]) > 2):                                                   #  Some of the keywords were (Null/None)\n",
    "      l_recd = { \"id\": (\"KW-\" + str(l_each[0])),\n",
    "         \"keyword\" : l_each[1].strip(), \"LABEL\": \"Keyword\" }                  #  strip(), Removing leading whitespace\n",
    "            # \n",
    "      l_KeywordNodes.append(l_recd)\n",
    "         #\n",
    "df_KeywordNodes = pd.DataFrame.from_records(l_KeywordNodes)\n",
    "   #\n",
    "print(\"Number of Keywords: %d   %d   %d\" % (\n",
    "   len(l_KeywordNodes_Set),\n",
    "   len(l_KeywordNodes),\n",
    "   len(df_KeywordNodes),\n",
    "   ))    \n",
    "print(\"\")\n",
    "\n",
    "\n",
    "#  Check our work\n",
    "#\n",
    "for l_each in df_KeywordNodes.head(5).itertuples():\n",
    "   print(\"Id: %-8s   Keyword: %-32s   LABEL: %s\" % (\n",
    "      l_each.id,\n",
    "      l_each.keyword,\n",
    "      l_each.LABEL,\n",
    "      ))\n",
    "print(\"\")\n",
    "\n",
    "    \n",
    "#  And now create the edges\n",
    "#\n",
    "#  A little different than cells above because we have keywords is an array\n",
    "#\n",
    "for l_each1 in df_PatientVisitNodes.itertuples():\n",
    "   for l_each2 in l_each1.keywords.split(\",\"):   \n",
    "      #\n",
    "      #  Get the id from the keywords data set.\n",
    "      #  What we get back from this operation is a Series, hence, to_list()\n",
    "      #\n",
    "      l_kwid =  df_KeywordNodes.loc[df_KeywordNodes[\"keyword\"] == l_each2.strip()].id.to_list()\n",
    "         #\n",
    "      if (len(l_kwid) > 0):\n",
    "            # \n",
    "         l_recd_N = { \"start_id\": l_each1.id, \"end_id\" : l_kwid[0] , \"TYPE\": \"IS_OF_KEYWORD\" }\n",
    "         l_recd_S = { \"start_id\": l_kwid[0] , \"end_id\" : l_each1.id, \"TYPE\": \"IS_OF_KEYWORD\" }\n",
    "            #\n",
    "         l_PatientVisitToKeywordEdges_N.append(l_recd_N)\n",
    "         l_PatientVisitToKeywordEdges_S.append(l_recd_S)\n",
    "            #\n",
    "df_PatientVisitToKeywordEdges_N = pd.DataFrame.from_records(l_PatientVisitToKeywordEdges_N)\n",
    "df_PatientVisitToKeywordEdges_S = pd.DataFrame.from_records(l_PatientVisitToKeywordEdges_S)\n",
    "   #\n",
    "print(\"Number of Edges,  PatientVisit --> Keywords: %d   %d\" % (\n",
    "   len(df_PatientVisitToKeywordEdges_N),\n",
    "   len(df_PatientVisitToKeywordEdges_S),\n",
    "))\n",
    "    \n",
    "    \n",
    "#  Check our work\n",
    "#\n",
    "print(\"\")\n",
    "   #\n",
    "for l_each in df_PatientVisitToKeywordEdges_N.head(5).itertuples():\n",
    "   print(\"Start Id: %-8s   End Id: %-8s   TYPE: %s\" % (l_each.start_id, l_each.end_id, l_each.TYPE))\n",
    "        \n",
    "print(\"\")\n",
    "   #\n",
    "for l_each in df_PatientVisitToKeywordEdges_S.head(5).itertuples():\n",
    "   print(\"Start Id: %-8s   End Id: %-8s   TYPE: %s\" % (l_each.start_id, l_each.end_id, l_each.TYPE))\n",
    "\n",
    "\n",
    "#  Sample output\n",
    "#\n",
    "#     Number of Keywords: 10445   10443   10443\n",
    "#     \n",
    "#     Id: KW-1       Keyword: heart tones                        LABEL: Keyword\n",
    "#     Id: KW-2       Keyword: mastopexy                          LABEL: Keyword\n",
    "#     Id: KW-3       Keyword: temporal lobes                     LABEL: Keyword\n",
    "#     Id: KW-4       Keyword: heart attacks                      LABEL: Keyword\n",
    "#     Id: KW-5       Keyword: digital laceration                 LABEL: Keyword\n",
    "#     \n",
    "#     Number of Edges,  PatientVisit --> Keywords: 52838   52838\n",
    "#     \n",
    "#     Start Id: PV-4000    End Id: KW-10014   TYPE: IS_OF_KEYWORD\n",
    "#     Start Id: PV-4000    End Id: KW-5669    TYPE: IS_OF_KEYWORD\n",
    "#     Start Id: PV-4000    End Id: KW-3019    TYPE: IS_OF_KEYWORD\n",
    "#     Start Id: PV-4000    End Id: KW-9372    TYPE: IS_OF_KEYWORD\n",
    "#     Start Id: PV-4000    End Id: KW-7627    TYPE: IS_OF_KEYWORD\n",
    "#     \n",
    "#     Start Id: KW-10014   End Id: PV-4000    TYPE: IS_OF_KEYWORD\n",
    "#     Start Id: KW-5669    End Id: PV-4000    TYPE: IS_OF_KEYWORD\n",
    "#     Start Id: KW-3019    End Id: PV-4000    TYPE: IS_OF_KEYWORD\n",
    "#     Start Id: KW-9372    End Id: PV-4000    TYPE: IS_OF_KEYWORD\n",
    "#     Start Id: KW-7627    End Id: PV-4000    TYPE: IS_OF_KEYWORD\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9a456b-e425-454d-83a2-5b2c3e7fa666",
   "metadata": {},
   "source": [
    "# Step 00:  Save our work .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffb25ab-3473-4b13-a0f5-c4f6477e55d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  We've created several sets of Nodes, and some bi-directional edges.\n",
    "#  Save these to file.\n",
    "#\n",
    "\n",
    "\n",
    "#  For Patient Visit, we'll take this chance to drop some columns\n",
    "#\n",
    "l_file = \"./10_Data/N_01_00_PatientVist.txt\"\n",
    "   #\n",
    "df_PatientVisitNodes[[\"id\", \"description\", \"sample_name\", \"transcription\"]].to_csv(l_file, sep = \"|\", encoding = \"utf-8\", index = False)\n",
    "\n",
    "\n",
    "#  For Medical Specialty, we'll keep all columns\n",
    "#\n",
    "l_file = \"./10_Data/N_02_00_MedicalSpecialty.txt\"\n",
    "   #\n",
    "df_MedicalSpecialtyNodes.to_csv(l_file, sep = \"|\", encoding = \"utf-8\", index = False)\n",
    "\n",
    "\n",
    "#  For Keyword, we'll keep all columns\n",
    "#\n",
    "l_file = \"./10_Data/N_03_00_Keyword.txt\"\n",
    "   #\n",
    "df_KeywordNodes.to_csv(l_file, sep = \"|\", encoding = \"utf-8\", index = False)\n",
    "\n",
    "\n",
    "   ###\n",
    "    \n",
    "\n",
    "#  Our Edges\n",
    "#\n",
    "l_file = \"./10_Data/E_01_00_N_PatientVistToMedicalSpecialty.txt\"\n",
    "   #\n",
    "df_PatientVisitToMedicalSpecialtyEdges_N.to_csv(l_file, sep = \"|\", encoding = \"utf-8\", index = False)\n",
    "   #\n",
    "l_file = \"./10_Data/E_01_00_S_PatientVistToMedicalSpecialty.txt\"\n",
    "   #\n",
    "df_PatientVisitToMedicalSpecialtyEdges_S.to_csv(l_file, sep = \"|\", encoding = \"utf-8\", index = False)\n",
    "\n",
    "\n",
    "l_file = \"./10_Data/E_02_00_N_PatientVistToKeyword.txt\"\n",
    "   #\n",
    "df_PatientVisitToKeywordEdges_N.to_csv(l_file, sep = \"|\", encoding = \"utf-8\", index = False)\n",
    "   #\n",
    "l_file = \"./10_Data/E_02_00_S_PatientVistToKeyword.txt\"\n",
    "   #\n",
    "df_PatientVisitToKeywordEdges_S.to_csv(l_file, sep = \"|\", encoding = \"utf-8\", index = False)\n",
    "\n",
    "\n",
    "print(\"--\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec1c889-3b18-44f7-adcf-b9b5eca13dd5",
   "metadata": {},
   "source": [
    "#  Step 02: Enrich the above from a Google Web service .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6491d824-e497-4548-8ace-659c3657662a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Google has a Web service to convert text into usable UMLS codes. See,\n",
    "#        https://cloud.google.com/healthcare-api/docs/how-tos/nlp\n",
    "#\n",
    "#  In this cell, we begin to invoke this service on the text from the\n",
    "#  cell above.\n",
    "\n",
    "#  See also,\n",
    "#     https://stackoverflow.com/questions/53472429/how-to-get-a-gcp-bearer-token-programmatically-with-python\n",
    "\n",
    "#  Google:\n",
    "#\n",
    "#     .  We had to create an Auth Token, which produced a JSON file.\n",
    "#        (Instruction in Url above.)\n",
    "#\n",
    "#     .  Our JSON file is at,\n",
    "#              export GOOGLE_APPLICATION_CREDENTIALS=\"/mnt/hgfs/My.20/MyShare_1/46 Topics 2022/91 KG, All Prospects/13 KG, DataBricks, Google/10_Data/05_katana-clusters-beta-d8605ac248e7.json\"\n",
    "#              export GOOGLE_APPLICATION_CREDENTIALS=\"/home/jovyan/work/My_KG_NoteBooks/P1_Prospects/10_DataBricks_Google/10_Data/05_katana-clusters-beta-d8605ac248e7.json\"\n",
    "#\n",
    "#     .  To extract the Auth Token, set the above, then run\n",
    "#           gcloud auth application-default print-access-token\n",
    "\n",
    "\n",
    "import google.auth\n",
    "import google.auth.transport.requests\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "\n",
    "#  This token times out often; you must rerun this block from time to time\n",
    "#\n",
    "l_credentials = service_account.Credentials.from_service_account_file(\n",
    "   \"/home/jovyan/work/My_KG_NoteBooks/P1_Prospects/13_DataBricks_Google/10_Data/05_katana-clusters-beta-d8605ac248e7.json\",\n",
    "   scopes=['https://www.googleapis.com/auth/cloud-platform'])\n",
    "l_auth_req = google.auth.transport.requests.Request()\n",
    "l_credentials.refresh(l_auth_req)\n",
    "   #\n",
    "l_token = l_credentials.token\n",
    "    \n",
    "print(\"\")\n",
    "print(\"Token: \" + l_token[0:120] + \" ...\")\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "#  Sample output,\n",
    "#\n",
    "#     Token: ya29.c.b0Aa9VdylvtWgGXBZyFkW5mADfcFiyBVitZsvkoKbHpCuXU7zGkgANRcho_ax5_SWWbiXfQj6cprlobWUlHnPkYEoKBCRw6   ...\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c425da-e8f3-4a68-bbff-323a002693cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Cool page; convert Curl command to Python,\n",
    "#     https://reqbin.com/curl\n",
    "\n",
    "#  Run the Google Web service, capture results\n",
    "#\n",
    "\n",
    "import requests\n",
    "from requests.structures import CaseInsensitiveDict\n",
    "   #\n",
    "import json\n",
    "\n",
    "    \n",
    "#  Function, because we will loop on this below-\n",
    "#\n",
    "def f_enrich(i_arg1):\n",
    "    \n",
    "   url = \"https://healthcare.googleapis.com/v1/projects/katana-clusters-beta/locations/us-central1/services/nlp:analyzeEntities\"\n",
    "   \n",
    "   l_headers = CaseInsensitiveDict()\n",
    "      #\n",
    "   l_headers[\"Authorization\"] = \"Bearer \" + l_token\n",
    "   l_headers[\"Content-Type\"]  = \"application/json\"\n",
    "       \n",
    "    \n",
    "   l_data = \"\"\"\n",
    "      {{\n",
    "      'nlpService':'projects/katana-clusters-beta/locations/us-central1/services/nlp',\n",
    "      'documentContent':'{0}'\n",
    "      }}\n",
    "      \"\"\".format(i_arg1)\n",
    "         #\n",
    "   l_resp = requests.post(url, headers = l_headers, data = l_data)\n",
    "      #\n",
    "   return l_resp\n",
    "\n",
    "\n",
    "\n",
    "l_response = f_enrich(\"Insulin regimen human 5 units IV administered.\")\n",
    "   #\n",
    "\n",
    "print(l_response.status_code)\n",
    "print(\"\")\n",
    "   #\n",
    "l_data_asjson = json.loads(l_response.content)                       #  Get the response in json\n",
    "print(json.dumps(l_data_asjson, indent = 3))                         #  This gives us a pretty print (easier to read)\n",
    "\n",
    "\n",
    "#  Sample data after this cell-,\n",
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8409d8-ea25-47ea-b911-d3bd7643e9a4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#  Step 00:  Sample data from above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c91100-2973-4f3f-9f3e-a28b53a1da20",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Sample data from above,\n",
    "#\n",
    "#     200\n",
    "#     \n",
    "#     {\n",
    "#        \"entityMentions\": [\n",
    "#           {\n",
    "#              \"mentionId\": \"1\",\n",
    "#              \"type\": \"MEDICINE\",\n",
    "#              \"text\": {\n",
    "#                 \"content\": \"Insulin regimen\",\n",
    "#                 \"beginOffset\": 0\n",
    "#              },\n",
    "#              \"linkedEntities\": [\n",
    "#                 {\n",
    "#                    \"entityId\": \"UMLS/C0021641\"\n",
    "#                 },\n",
    "#                 {\n",
    "#                    \"entityId\": \"UMLS/C0795635\"\n",
    "#                 },\n",
    "#                 {\n",
    "#                    \"entityId\": \"UMLS/C1533581\"\n",
    "#                 },\n",
    "#                 {\n",
    "#                    \"entityId\": \"UMLS/C3537244\"\n",
    "#                 },\n",
    "#                 {\n",
    "#                    \"entityId\": \"UMLS/C3714501\"\n",
    "#                 }\n",
    "#              ],\n",
    "#              \"temporalAssessment\": {\n",
    "#                 \"value\": \"CURRENT\",\n",
    "#                 \"confidence\": 0.8573660850524902\n",
    "#              },\n",
    "#              \"certaintyAssessment\": {\n",
    "#                 \"value\": \"LIKELY\",\n",
    "#                 \"confidence\": 0.9751282930374146\n",
    "#              },\n",
    "#              \"subject\": {\n",
    "#                 \"value\": \"PATIENT\",\n",
    "#                 \"confidence\": 0.9995787739753723\n",
    "#              },\n",
    "#              \"confidence\": 0.6379408836364746\n",
    "#           },\n",
    "#           {\n",
    "#              \"mentionId\": \"2\",\n",
    "#              \"type\": \"MED_DOSE\",\n",
    "#              \"text\": {\n",
    "#                 \"content\": \"5 units\",\n",
    "#                 \"beginOffset\": 22\n",
    "#              },\n",
    "#              \"confidence\": 0.7443782091140747\n",
    "#           },\n",
    "#           {\n",
    "#              \"mentionId\": \"3\",\n",
    "#              \"type\": \"MED_ROUTE\",\n",
    "#              \"text\": {\n",
    "#                 \"content\": \"IV\",\n",
    "#                 \"beginOffset\": 30\n",
    "#              },\n",
    "#              \"linkedEntities\": [\n",
    "#                 {\n",
    "#                    \"entityId\": \"UMLS/C0348016\"\n",
    "#                 }\n",
    "#              ],\n",
    "#              \"confidence\": 0.779011607170105\n",
    "#           }\n",
    "#        ],\n",
    "#        \"entities\": [\n",
    "#           {\n",
    "#              \"entityId\": \"UMLS/C0021641\",\n",
    "#              \"preferredTerm\": \"Insulin\",\n",
    "#              \"vocabularyCodes\": [\n",
    "#                 \"FMA/83365\",\n",
    "#                 \"LNC/LA15805-7\",\n",
    "#                 \"LNC/LP14676-8\",\n",
    "#                 \"LNC/LP16325-0\",\n",
    "#                 \"LNC/LP32542-0\",\n",
    "#                 \"LNC/LP70329-5\",\n",
    "#                 \"LNC/MTHU002108\",\n",
    "#                 \"LNC/MTHU019392\",\n",
    "#                 \"MSH/D007328\",\n",
    "#                 \"MTH/NOCODE\"\n",
    "#              ]\n",
    "#           },\n",
    "#           {\n",
    "#              \"entityId\": \"UMLS/C0348016\",\n",
    "#              \"preferredTerm\": \"Intravenous\",\n",
    "#              \"vocabularyCodes\": [\n",
    "#                 \"LNC/LA9437-0\",\n",
    "#                 \"LNC/LP32453-0\",\n",
    "#                 \"MTH/NOCODE\",\n",
    "#                 \"NCI/C13346\"\n",
    "#              ]\n",
    "#           },\n",
    "#           {\n",
    "#              \"entityId\": \"UMLS/C0795635\",\n",
    "#              \"preferredTerm\": \"insulin, regular, human\",\n",
    "#              \"vocabularyCodes\": [\n",
    "#                 \"LNC/LP17001-6\",\n",
    "#                 \"MSH/D061386\",\n",
    "#                 \"MTH/NOCODE\",\n",
    "#                 \"NCI/C29125\",\n",
    "#                 \"RXNORM/253182\",\n",
    "#                 \"VANDF/4017559\",\n",
    "#                 \"VANDF/4017569\",\n",
    "#                 \"VANDF/4019786\"\n",
    "#              ]\n",
    "#           },\n",
    "#           {\n",
    "#              \"entityId\": \"UMLS/C1533581\",\n",
    "#              \"preferredTerm\": \"Therapeutic Insulin\",\n",
    "#              \"vocabularyCodes\": [\n",
    "#                 \"MTH/NOCODE\",\n",
    "#                 \"NCI/C581\"\n",
    "#              ]\n",
    "#           },\n",
    "#           {\n",
    "#              \"entityId\": \"UMLS/C3537244\",\n",
    "#              \"preferredTerm\": \"Insulins\",\n",
    "#              \"vocabularyCodes\": [\n",
    "#                 \"MSH/D061385\",\n",
    "#                 \"MTH/NOCODE\"\n",
    "#              ]\n",
    "#           },\n",
    "#           {\n",
    "#              \"entityId\": \"UMLS/C3714501\",\n",
    "#              \"preferredTerm\": \"Insulin Drug Class\",\n",
    "#              \"vocabularyCodes\": [\n",
    "#                 \"MTH/NOCODE\",\n",
    "#                 \"VANDF/4021631\"\n",
    "#              ]\n",
    "#           }\n",
    "#        ],\n",
    "#        \"relationships\": [\n",
    "#           {\n",
    "#              \"subjectId\": \"1\",\n",
    "#              \"objectId\": \"2\",\n",
    "#              \"confidence\": 0.9996469616889954\n",
    "#           },\n",
    "#           {\n",
    "#              \"subjectId\": \"1\",\n",
    "#              \"objectId\": \"3\",\n",
    "#              \"confidence\": 0.9995671510696411\n",
    "#           }\n",
    "#        ]\n",
    "#     }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07bdf14-0a1b-48db-a0f8-9d9604c16a31",
   "metadata": {},
   "source": [
    "#  Step 02:  (continued)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b11d557b-990a-4e72-bcb1-3cbcfdcbf6a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "............................................................................................................................................................................."
     ]
    },
    {
     "ename": "UnicodeEncodeError",
     "evalue": "'latin-1' codec can't encode character '\\u2019' in position 2693: Body ('’') is not valid Latin-1. Use body.encode('utf-8') if you want to send it encoded in UTF-8.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeEncodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [60], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m    \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m l_response \u001b[38;5;241m=\u001b[39m \u001b[43mf_enrich\u001b[49m\u001b[43m(\u001b[49m\u001b[43ml_each\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranscription\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m l_data_asjson \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(l_response\u001b[38;5;241m.\u001b[39mcontent) \n\u001b[1;32m     21\u001b[0m df_enriched \u001b[38;5;241m=\u001b[39m df_enriched\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m : l_each\u001b[38;5;241m.\u001b[39mid, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menriched_from_Google\u001b[39m\u001b[38;5;124m\"\u001b[39m : l_data_asjson}, ignore_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn [59], line 32\u001b[0m, in \u001b[0;36mf_enrich\u001b[0;34m(i_arg1)\u001b[0m\n\u001b[1;32m     25\u001b[0m l_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;124m   \u001b[39m\u001b[38;5;124m{{\u001b[39m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;124m   \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnlpService\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprojects/katana-clusters-beta/locations/us-central1/services/nlp\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;124m   \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdocumentContent\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;124m   }}\u001b[39m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124m   \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(i_arg1)\n\u001b[1;32m     31\u001b[0m       \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m l_resp \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43ml_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43ml_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m    \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m l_resp\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/requests/api.py:115\u001b[0m, in \u001b[0;36mpost\u001b[0;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(url, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, json\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \n\u001b[1;32m    106\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/requests/sessions.py:587\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    582\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    583\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    584\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    585\u001b[0m }\n\u001b[1;32m    586\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 587\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/requests/sessions.py:701\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    698\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    700\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    704\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/requests/adapters.py:489\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    487\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    488\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m chunked:\n\u001b[0;32m--> 489\u001b[0m         resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m            \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m            \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m            \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m            \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m            \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    502\u001b[0m     \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[1;32m    503\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    504\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(conn, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproxy_pool\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/urllib3/connectionpool.py:703\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_proxy(conn)\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    704\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    706\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    711\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    713\u001b[0m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    714\u001b[0m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    715\u001b[0m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    716\u001b[0m \u001b[38;5;66;03m# mess.\u001b[39;00m\n\u001b[1;32m    717\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/urllib3/connectionpool.py:398\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    396\u001b[0m         conn\u001b[38;5;241m.\u001b[39mrequest_chunked(method, url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhttplib_request_kw)\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 398\u001b[0m         \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhttplib_request_kw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[38;5;66;03m# We are swallowing BrokenPipeError (errno.EPIPE) since the server is\u001b[39;00m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;66;03m# legitimately able to close the connection after sending a valid response.\u001b[39;00m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;66;03m# With this behaviour, the received response is still readable.\u001b[39;00m\n\u001b[1;32m    403\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBrokenPipeError\u001b[39;00m:\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;66;03m# Python 3\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/urllib3/connection.py:239\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[0;34m(self, method, url, body, headers)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser-agent\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (six\u001b[38;5;241m.\u001b[39mensure_str(k\u001b[38;5;241m.\u001b[39mlower()) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m headers):\n\u001b[1;32m    238\u001b[0m     headers[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser-Agent\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m _get_default_user_agent()\n\u001b[0;32m--> 239\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mHTTPConnection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/http/client.py:1256\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1253\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\u001b[38;5;28mself\u001b[39m, method, url, body\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, headers\u001b[38;5;241m=\u001b[39m{}, \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   1254\u001b[0m             encode_chunked\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   1255\u001b[0m     \u001b[38;5;124;03m\"\"\"Send a complete request to the server.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1256\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/http/client.py:1301\u001b[0m, in \u001b[0;36mHTTPConnection._send_request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1297\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mputheader(hdr, value)\n\u001b[1;32m   1298\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(body, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;66;03m# RFC 2616 Section 3.7.1 says that text default has a\u001b[39;00m\n\u001b[1;32m   1300\u001b[0m     \u001b[38;5;66;03m# default charset of iso-8859-1.\u001b[39;00m\n\u001b[0;32m-> 1301\u001b[0m     body \u001b[38;5;241m=\u001b[39m \u001b[43m_encode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbody\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1302\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mendheaders(body, encode_chunked\u001b[38;5;241m=\u001b[39mencode_chunked)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/http/client.py:164\u001b[0m, in \u001b[0;36m_encode\u001b[0;34m(data, name)\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlatin-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mUnicodeEncodeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m--> 164\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mUnicodeEncodeError\u001b[39;00m(\n\u001b[1;32m    165\u001b[0m         err\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[1;32m    166\u001b[0m         err\u001b[38;5;241m.\u001b[39mobject,\n\u001b[1;32m    167\u001b[0m         err\u001b[38;5;241m.\u001b[39mstart,\n\u001b[1;32m    168\u001b[0m         err\u001b[38;5;241m.\u001b[39mend,\n\u001b[1;32m    169\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m%.20r\u001b[39;00m\u001b[38;5;124m) is not valid Latin-1. Use \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.encode(\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    170\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mif you want to send it encoded in UTF-8.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m    171\u001b[0m         (name\u001b[38;5;241m.\u001b[39mtitle(), data[err\u001b[38;5;241m.\u001b[39mstart:err\u001b[38;5;241m.\u001b[39mend], name)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
      "\u001b[0;31mUnicodeEncodeError\u001b[0m: 'latin-1' codec can't encode character '\\u2019' in position 2693: Body ('’') is not valid Latin-1. Use body.encode('utf-8') if you want to send it encoded in UTF-8."
     ]
    }
   ],
   "source": [
    "\n",
    "#\n",
    "#  **  This cell takes minutes to complete ..\n",
    "#\n",
    "\n",
    "#  Run the above Web service against our first DataFrame\n",
    "#\n",
    "#  Recall df_PatientVisitNodes contains,\n",
    "#\n",
    "#     id|description|sample_name|transcription\n",
    "#\n",
    "\n",
    "df_enriched = pd.DataFrame(columns = [\"id\", \"enrich_from_Google\"])\n",
    "   #\n",
    "print(\"\")\n",
    "print(\"Process nearly 50,000 rows ..\")\n",
    "\n",
    "l_cntr = 0\n",
    "   #\n",
    "for l_each in df_PatientVisitNodes.itertuples():\n",
    "   l_cntr += 1\n",
    "      #\n",
    "   if (l_cntr % 5000 == 0):\n",
    "      print(\"\")\n",
    "      print(\"Processed so far: \" %d (l_cntr))\n",
    "   else:\n",
    "      print(\".\", end = \"\")\n",
    "         #\n",
    "   l_tosend = bytes(l_each.transcription).decode(\"utf-8\")    \n",
    "       #\n",
    "   l_response = f_enrich(l_tosend)\n",
    "   l_data_asjson = json.loads(l_response.content) \n",
    "\n",
    "   df_enriched = df_enriched.append({\"id\" : l_each.id, \"enriched_from_Google\" : l_data_asjson}, ignore_index = True)\n",
    "\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "for l_each in df_enriched.head(5).itertuples():\n",
    "   print(\"Record number: %d   Short text: %s\" % (l_each.id, str(l_each.enriched_from_Google)[0:60]))\n",
    "        \n",
    "        \n",
    "print(\"\")\n",
    "print(\"--\")\n",
    "\n",
    "#  Sample output,\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4ad011-1bf5-4242-b93a-7480745e6cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Write this out to a file\n",
    "#\n",
    "\n",
    "l_file = \"./10_Data/34_33_PlusGoogle.txt\"\n",
    "\n",
    "df_enriched.to_csv(l_file, sep = \"|\", encoding = \"utf-8\", index = False)\n",
    "\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824e4efe-569f-4b16-82aa-a4ada5bfb099",
   "metadata": {},
   "source": [
    "# Step 03:  Extract actual field data from the JSON Google gave us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2583d8-e087-440c-9ffb-9916bb5e6e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#  The model for the df_enriched DataFrame is,\n",
    "#     id|enrich_from_Google\n",
    "#\n",
    "#     with the second column being a JSON encoded string with its own model.\n",
    "#\n",
    "#  Here we loop thru said DataFrme, and make our resultant Nodes and Edges\n",
    "#  for our graph.\n",
    "#\n",
    "\n",
    "#  import numpy as np\n",
    "\n",
    "\n",
    "l_NumOfBadJson                = 0\n",
    "l_NumOfRootKeyNotFound        = 0\n",
    "   #\n",
    "l_UmlsEntityNodes             = []\n",
    "l_UmlsVocabularyNodes         = []\n",
    "   #\n",
    "l_PatientVisitToEntityEdge_N  = []\n",
    "l_PatientVisitToEntityEdge_S  = []\n",
    "l_EntityToVocabularyEdge_N    = []\n",
    "l_EntityToVocabularyEdge_S    = []\n",
    "\n",
    "\n",
    "for l_each in df_enriched.itertuples():\n",
    "    \n",
    "   #  Before we cleaned up all of the JSON (random single quotes for possesive nouns,\n",
    "   #  other), we would get errors similar to,\n",
    "   #\n",
    "   #      4001|\"{'error': {'code': 400, 'message': \"\"Invalid JSON payload received. Expected , or } after key:value pair.\\naced\n",
    "   #         on the patient's skin or on the imm\\n                    ^\"\", 'status': 'INVALID_ARGUMENT'}}\"\n",
    "   #\n",
    "   #  Upon receipt of this condition, a root level key with the title \"error\" would be\n",
    "   #  present. If those return, filter those out-\n",
    "   #\n",
    "   if (\"error\" in l_each.enrich_from_Google):\n",
    "      l_NumOfBadJson += 1\n",
    "    \n",
    "   else:\n",
    "      #\n",
    "      #  \"entities\" should be a root level key to this dictionary\n",
    "      #\n",
    "      if (\"entities\" in l_each.enrich_from_Google):\n",
    "         #\n",
    "         #  Loop thru these\n",
    "         #\n",
    "         for l_entity in l_each.enrich_from_Google[\"entities\"]:\n",
    "            if (\"entityId\" in l_entity):\n",
    "               #\n",
    "               #  Build a dictionary that we will append to an array\n",
    "               #\n",
    "               l_recd1 = { \"id\": l_entity[\"entityId\"], \"entityId\" : l_entity[\"entityId\"], \"LABEL\": \"UmlsEntity\" }\n",
    "               #\n",
    "               #  If this key is present, add it to the dictionary\n",
    "               #\n",
    "               if (\"preferredTerm\" in l_entity):\n",
    "                  #\n",
    "                  #  We have an additional key, add to the record and add to our array\n",
    "                  #\n",
    "                  l_recd1.update( {\"preferredTerm\": l_entity[\"preferredTerm\"]} )\n",
    "                     #\n",
    "               l_UmlsEntityNodes.append(l_recd1)\n",
    "               #\n",
    "               #  Above was our list of Nodes of LABEL \"UmlsEntity\"\n",
    "               #  \n",
    "               #  Here we make our Edge list from;  PatientVisit --> UmlsEntity\n",
    "               #\n",
    "               #  We make all Edges to be bi-directional. As a heterogeneous relationship,\n",
    "               #  we need two arrays.\n",
    "               #\n",
    "               l_recd2a = { \"start_id\": str(l_each.id)           , \"end_id\":   str(l_entity[\"entityId\"]), \"TYPE\": \"VISIT_CONTAINS\" }\n",
    "               l_recd2b = { \"start_id\": str(l_entity[\"entityId\"]), \"end_id\":   str(l_each.id)           , \"TYPE\": \"VISIT_CONTAINS\" }\n",
    "                  #\n",
    "               l_PatientVisitToEntityEdge_N.append(l_recd2a)\n",
    "               l_PatientVisitToEntityEdge_S.append(l_recd2b)\n",
    "               #\n",
    "               #  We are done with UmlsEntity and its Edge to PatientVisit\n",
    "               #\n",
    "               #  Also in \"entities\" is another array, \"vocabularyCodes\"\n",
    "               #\n",
    "               if (\"vocabularyCodes\" in l_entity):\n",
    "                  for l_vocab in l_entity[\"vocabularyCodes\"]:\n",
    "                     #\n",
    "                     #  Add to our set of Vocabulary Nodes\n",
    "                     #\n",
    "                     l_recd3 = { \"id\": l_vocab, \"vocabularyCode\": l_vocab, \"LABEL\": \"UmlsVocabulary\" }\n",
    "                        #\n",
    "                     l_UmlsVocabularyNodes.append(l_recd3)\n",
    "                     #\n",
    "                     #  And create the Edge from UmlsEntity --> UmlsVocabulary\n",
    "                     #\n",
    "                     l_recd4a = { \"start_id\": str(l_entity[\"entityId\"]), \"end_id\": str(l_vocab             ), \"TYPE\": \"ALSO_CODED_AS\" }\n",
    "                     l_recd4b = { \"start_id\": str(l_vocab             ), \"end_id\": str(l_entity[\"entityId\"]), \"TYPE\": \"ALSO_CODED_AS\" }\n",
    "                        #\n",
    "                     l_EntityToVocabularyEdge_N.append(l_recd4a)\n",
    "                     l_EntityToVocabularyEdge_S.append(l_recd4b)\n",
    "            else:\n",
    "               #\n",
    "               #  No \"entityId\" in our record. This has never happened.\n",
    "               #  We wont report, just pass.\n",
    "               #\n",
    "               pass\n",
    "            \n",
    "      else:\n",
    "         l_NumOfRootKeyNotFound += 1\n",
    "            \n",
    "            \n",
    "   ###\n",
    "\n",
    "\n",
    "print(\"Number of 'Error' input records: %d   Number of 'No Root Key' input records: %d\" % ( l_NumOfBadJson, l_NumOfRootKeyNotFound) )\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "#  l_UmlsEntityNodes, l_UmlsVocabularyNodes, l_PatientVisitToEntityEdge, and l_EntityToVocabularyEdge\n",
    "#     are currently arrays, and have duplicate records.\n",
    "#\n",
    "#  Arguably; these should have been sets(), [ then ] made into arrays or DataFrames.\n",
    "#     (Memory versus CPU. And .. .. two processing loops versus one.)\n",
    "#\n",
    "#  Convert these to DataFrames and remove duplicates\n",
    "#\n",
    " \n",
    "df_UmlsEntityNodes             = pd.DataFrame.from_records(l_UmlsEntityNodes           ).drop_duplicates()\n",
    "df_UmlsVocabularyNodes         = pd.DataFrame.from_records(l_UmlsVocabularyNodes       ).drop_duplicates()\n",
    "   #\n",
    "df_PatientVisitToEntityEdge_N  = pd.DataFrame.from_records(l_PatientVisitToEntityEdge_N).drop_duplicates()\n",
    "df_PatientVisitToEntityEdge_S  = pd.DataFrame.from_records(l_PatientVisitToEntityEdge_S).drop_duplicates()\n",
    "df_EntityToVocabularyEdge_N    = pd.DataFrame.from_records(l_EntityToVocabularyEdge_N  ).drop_duplicates()\n",
    "df_EntityToVocabularyEdge_S    = pd.DataFrame.from_records(l_EntityToVocabularyEdge_S  ).drop_duplicates()\n",
    "\n",
    "\n",
    "print(\"Number of PatientVisit nodes: %d   UmlsEntity nodes: %d   UmlsVocabulary nodes: %d\" % ( len(df_PatientVisitNodes), len(df_UmlsEntityNodes), len(df_UmlsVocabularyNodes) ))\n",
    "print(\"\")\n",
    "print(\"Edges PatientVisit --> UmlsEntity: %d   %s   UmlsEntity --> UmlsVocabulary: %d   %s\" % (\n",
    "   len(df_PatientVisitToEntityEdge_N), len(df_PatientVisitToEntityEdge_S),\n",
    "   len(df_EntityToVocabularyEdge_N  ), len(df_EntityToVocabularyEdge_S  ) ))\n",
    "print(\"\")\n",
    "\n",
    "print(\"--\")    \n",
    "\n",
    "#  Sample output,\n",
    "#\n",
    "#     Number of 'Error' input records: 0   Number of 'No Root Key' input records: 0\n",
    "#     \n",
    "#     Number of PatientVisit nodes: 90   UmlsEntity nodes: 3115   UmlsVocabulary nodes: 8860\n",
    "#     \n",
    "#     Edges PatientVisit --> UmlsEntity: 8479   8479   UmlsEntity --> UmlsVocabulary: 10976   10976\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61d95ad-27ae-4dfa-ad2d-27ba92d2dbb8",
   "metadata": {},
   "source": [
    "#  Step 04: Create the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adb1735-4a5d-496a-bab4-da806a7624b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  At this point we have several Pandas DataFrames; import them into the graph-\n",
    "#\n",
    "#     Why use Pandas DataFrames versus Dask ?\n",
    "#     \n",
    "#     .  The data is small enough\n",
    "#     .  Dask DataFrames currently (beta) have to be sourced from a shared/public drive\n",
    "#\n",
    "\n",
    "\n",
    "from katana.remote import import_data\n",
    "\n",
    "\n",
    "#  Just nodes\n",
    "#\n",
    "with import_data.DataFrameImporter(my_graph) as df_importer:   \n",
    "   df_importer.nodes_dataframe(\n",
    "      df_PatientVisitNodes,\n",
    "      id_column             = \"id\",\n",
    "      id_space              = \"PatientVisit\",  \n",
    "      label                 = \"PatientVisit\",  \n",
    "      ) \n",
    "   df_importer.insert()\n",
    "      #\n",
    "with import_data.DataFrameImporter(my_graph) as df_importer:   \n",
    "   df_importer.nodes_dataframe(\n",
    "      df_UmlsEntityNodes,\n",
    "      id_column             = \"id\",\n",
    "      id_space              = \"UmlsEntity\",  \n",
    "      label                 = \"UmlsEntity\",  \n",
    "      ) \n",
    "   df_importer.insert()\n",
    "      #\n",
    "with import_data.DataFrameImporter(my_graph) as df_importer:   \n",
    "   df_importer.nodes_dataframe(\n",
    "      df_UmlsVocabularyNodes,\n",
    "      id_column             = \"id\",\n",
    "      id_space              = \"UmlsVocabulary\",  \n",
    "      label                 = \"UmlsVocabulary\",  \n",
    "      ) \n",
    "   df_importer.insert()\n",
    "    \n",
    "\n",
    "#  Just edges\n",
    "#\n",
    "with import_data.DataFrameImporter(my_graph) as df_importer:   \n",
    "   df_importer.edges_dataframe(\n",
    "      df_PatientVisitToEntityEdge_N, \n",
    "      source_id_space       = \"PatientVisit\", \n",
    "      destination_id_space  = \"UmlsEntity\",   \n",
    "      source_column         = \"start_id\",\n",
    "      destination_column    = \"end_id\",\n",
    "      type                  = \"VISIT_CONTAINS\"\n",
    "      )\n",
    "   df_importer.node_id_property_name(\"id\")                        #  This line is required when you are not inserting any Nodes, only Edges\n",
    "   df_importer.insert()                                           #  A heterogeneous relationship, we need two inserts for each edge, because\n",
    "      #                                                           #  the source and dest id spaces are different.\n",
    "with import_data.DataFrameImporter(my_graph) as df_importer:   \n",
    "   df_importer.edges_dataframe(\n",
    "      df_PatientVisitToEntityEdge_S, \n",
    "      source_id_space       = \"UmlsEntity\", \n",
    "      destination_id_space  = \"PatientVisit\",   \n",
    "      source_column         = \"start_id\",\n",
    "      destination_column    = \"end_id\",\n",
    "      type                  = \"VISIT_CONTAINS\"\n",
    "      )\n",
    "   df_importer.node_id_property_name(\"id\")\n",
    "   df_importer.insert()\n",
    "      #\n",
    "with import_data.DataFrameImporter(my_graph) as df_importer:   \n",
    "   df_importer.edges_dataframe(\n",
    "      df_EntityToVocabularyEdge_N, \n",
    "      source_id_space       = \"UmlsEntity\", \n",
    "      destination_id_space  = \"UmlsVocabulary\",   \n",
    "      source_column         = \"start_id\",\n",
    "      destination_column    = \"end_id\",\n",
    "      type                  = \"ALSO_CODED_AS\"\n",
    "      )\n",
    "   df_importer.node_id_property_name(\"id\")\n",
    "   df_importer.insert()\n",
    "      #\n",
    "with import_data.DataFrameImporter(my_graph) as df_importer:   \n",
    "   df_importer.edges_dataframe(\n",
    "      df_EntityToVocabularyEdge_S, \n",
    "      source_id_space       = \"UmlsVocabulary\", \n",
    "      destination_id_space  = \"UmlsEntity\",   \n",
    "      source_column         = \"start_id\",\n",
    "      destination_column    = \"end_id\",\n",
    "      type                  = \"ALSO_CODED_AS\"\n",
    "      )\n",
    "   df_importer.node_id_property_name(\"id\")\n",
    "   df_importer.insert()\n",
    "\n",
    "\n",
    "      ###\n",
    "\n",
    "\n",
    "display(\"Number of nodes: %d   Numbers of edges: %d\" % ( my_graph.num_nodes(), my_graph.num_edges() ))\n",
    "\n",
    "\n",
    "display(\"--\")\n",
    "\n",
    "\n",
    "#  Sample output,\n",
    "#\n",
    "#     'Number of nodes: 12065   Numbers of edges: 38910'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e721f96b-147e-4ed6-ad3c-d4621dca8565",
   "metadata": {},
   "source": [
    "#  Step 00:  Checking Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30796d4e-2cba-46da-bf2d-8cf3ce54cfaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Produce a visual graph\n",
    "\n",
    "from katana_visualization_widget import GraphVisOptions, NodeVisOption, EdgeVisOption, ANY\n",
    "\n",
    "l_options = GraphVisOptions(\n",
    "   node_options = [\n",
    "      NodeVisOption(\"PatientVisit\",   label=\"sample_name\"),\n",
    "      NodeVisOption(\"UmlsEntity\",     label=\"id\"         ),\n",
    "      NodeVisOption(\"UmlsVocabulary\", label=\"id\"         ),\n",
    "   ],\n",
    "   #  edge_options = [\n",
    "   #     EdgeVisOption([\"VISIT_CONTAINS\"], label=\"start_id\"),\n",
    "   #     EdgeVisOption([\"ALSO_CODED_AS\" ], label=\"start_id\"),\n",
    "   #  ]\n",
    "   )\n",
    "    \n",
    "    \n",
    "l_result = my_graph.query(\"\"\"\n",
    "\n",
    "   MATCH (n) - [r] -> (m)\n",
    "   RETURN n, r, m\n",
    "   LIMIT 1000 \n",
    "   \n",
    "   \"\"\",\n",
    "   contextualize=True)\n",
    "\n",
    "l_result.view(graph_vis_options = l_options)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e978423-cbc7-4012-bf72-4f8d74b918b9",
   "metadata": {},
   "source": [
    "<div> \n",
    "<img src=\"./01_Images/Results_01.png\" alt=\"Drawing\" style=\"width: 800px;\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b4a44c-9d89-4306-b8c0-67156a79a537",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Produce a visual graph\n",
    "\n",
    "from katana_visualization_widget import GraphVisOptions, NodeVisOption, EdgeVisOption, ANY\n",
    "\n",
    "l_options = GraphVisOptions(\n",
    "   node_options = [\n",
    "      NodeVisOption(\"PatientVisit\",   label=\"sample_name\"),\n",
    "      NodeVisOption(\"UmlsEntity\",     label=\"id\"         ),\n",
    "      NodeVisOption(\"UmlsVocabulary\", label=\"id\"         ),\n",
    "   ],\n",
    "   #  edge_options = [\n",
    "   #     EdgeVisOption([\"VISIT_CONTAINS\"], label=\"start_id\"),\n",
    "   #     EdgeVisOption([\"ALSO_CODED_AS\" ], label=\"start_id\"),\n",
    "   #  ]\n",
    "   )\n",
    "    \n",
    "    \n",
    "l_result = my_graph.query(\"\"\"\n",
    "\n",
    "   MATCH (n: PatientVisit) - [r: VISIT_CONTAINS] - (m: UmlsEntity) - [s: ALSO_CODED_AS] -> (t: UmlsVocabulary)\n",
    "   WHERE n.id = \"4001\"\n",
    "   RETURN n, r, m, s, t\n",
    "   \n",
    "   \"\"\",\n",
    "   contextualize=True)\n",
    "\n",
    "l_result.view(graph_vis_options = l_options)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b4c131-62ef-4449-8674-7e0109b9c8c5",
   "metadata": {},
   "source": [
    "<div> \n",
    "<img src=\"./01_Images/Results_02.png\" alt=\"Drawing\" style=\"width: 800px;\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89bebcb-a530-4a0a-b4a8-3a38926dfbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Checking the edges specifically ..\n",
    "#\n",
    "\n",
    "l_result = my_graph.query(\"\"\"\n",
    "   MATCH (n) - [r: VISIT_CONTAINS] - (m)\n",
    "   RETURN COUNT(r)\n",
    "   \"\"\",\n",
    "   contextualize=False)\n",
    "      #    \n",
    "print(l_result)\n",
    "\n",
    "l_result = my_graph.query(\"\"\"\n",
    "   MATCH (n: PatientVisit) - [r: VISIT_CONTAINS] -> (m: UmlsEntity)\n",
    "   RETURN COUNT(r)\n",
    "   \"\"\",\n",
    "   contextualize=False)\n",
    "      #    \n",
    "print(l_result)\n",
    "\n",
    "l_result = my_graph.query(\"\"\"\n",
    "   MATCH (m: UmlsEntity) - [r: VISIT_CONTAINS] -> (n: PatientVisit)\n",
    "   RETURN COUNT(r)\n",
    "   \"\"\",\n",
    "   contextualize=False)\n",
    "      #    \n",
    "print(l_result)\n",
    "\n",
    "#  Sample output,    (I don't think this is correct)\n",
    "#\n",
    "#        COUNT(r)\n",
    "#     0     33916              #  This is 4x  8479\n",
    "#     \n",
    "#        COUNT(r)\n",
    "#     0      8479\n",
    "#     \n",
    "#        COUNT(r)\n",
    "#     0      8479\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d607abb-6e9b-4dfd-9b10-302c237556d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "l_result = my_graph.query(\"\"\"\n",
    "   MATCH (m) - [s: ALSO_CODED_AS] - (t)\n",
    "   RETURN COUNT(s)\n",
    "   \"\"\",\n",
    "   contextualize=False)\n",
    "      #\n",
    "print(l_result)\n",
    "\n",
    "l_result = my_graph.query(\"\"\"\n",
    "   MATCH (m: UmlsEntity) - [s: ALSO_CODED_AS] -> (t: UmlsVocabulary)\n",
    "   RETURN COUNT(s)\n",
    "   \"\"\",\n",
    "   contextualize=False)\n",
    "      #\n",
    "print(l_result)\n",
    "\n",
    "l_result = my_graph.query(\"\"\"\n",
    "   MATCH (t: UmlsVocabulary) - [s: ALSO_CODED_AS] -> (m: UmlsEntity)\n",
    "   RETURN COUNT(s)\n",
    "   \"\"\",\n",
    "   contextualize=False)\n",
    "      #\n",
    "print(l_result)\n",
    "\n",
    "#  Sample output,    (I don't think this is correct)\n",
    "#\n",
    "#     0     33916              #  This is 4x  8479\n",
    "#        COUNT(s)\n",
    "#     0     43904              #  This is 4x  10976\n",
    "#     \n",
    "#        COUNT(s)\n",
    "#     0     10976\n",
    "#     \n",
    "#        COUNT(s)\n",
    "#     0     10976\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
