{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064cde52-a7e1-486e-a7c4-3ce3aaaa01da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  This NoteBook contains code to run classic ML routines against a \n",
    "#  number of familiar data swets ..\n",
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d433bb83-5f71-406e-ab1c-1b0ef02583f1",
   "metadata": {},
   "source": [
    "#  Step 00: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba7589c-6f1e-4f5c-a37b-21f8944c2499",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Largely code to control how print statements and related work\n",
    "#\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%xmode Minimal\n",
    "\n",
    "\n",
    "#  Setting display options \n",
    "#\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.width\", 640)\n",
    "   #\n",
    "import numpy as np\n",
    "np.set_printoptions(edgeitems = 30, linewidth = 100000, \n",
    "   formatter = dict(float = lambda x: \"%.3g\" % x))\n",
    "\n",
    "#  Sets horizontal scroll for wide outputs\n",
    "#\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"\"))\n",
    "\n",
    "   ###\n",
    "    \n",
    "from tabulate import tabulate\n",
    "#\n",
    "#  How to use tabulate-\n",
    "#\n",
    "#  l_result = [{ \"col1\": 20, \"col2\": 30}]\n",
    "#  #\n",
    "#  print(tabulate(l_result, headers='keys', tablefmt='psql', showindex=False))\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a747a0-4aec-4cd2-bf38-8849deee7ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Helper functions for what we want to do below-\n",
    "#\n",
    "\n",
    "#  We use these objects to store the history of results; display only\n",
    "#\n",
    "class HistoryIterator:\n",
    "   def __init__(self, history):\n",
    "       self._history = history\n",
    "       self._index = 0\n",
    "\n",
    "   def __next__(self):\n",
    "       if (self._index < len(self._history._events)):\n",
    "           result = (self._history._events[self._index][\"event\"] , self._history._events[self._index][\"measure\"])\n",
    "           self._index +=1\n",
    "           return result\n",
    "       raise StopIteration\n",
    "\n",
    "class History:\n",
    "   def __init__(self):\n",
    "      self._events = list()\n",
    "\n",
    "   def clear(self):\n",
    "      self._events = list()\n",
    "    \n",
    "   def add(self, event, measure):\n",
    "      self._events.append({\"event\": event, \"measure\": measure})\n",
    "\n",
    "   def __iter__(self):\n",
    "      return HistoryIterator(self)\n",
    "\n",
    "\n",
    "l_history = History()\n",
    "\n",
    "\n",
    "#  The sklearn ML routines follow a very consistent pattern. As such, we\n",
    "#  put these in a function, reduce redundant code below-\n",
    "#\n",
    "\n",
    "def do_model(i_routine, i_train_data, i_train_labels, i_test_data, i_test_labels, i_name_of_test):\n",
    "\n",
    "   #  Train whatever model\n",
    "   #\n",
    "   i_routine.fit(i_train_data, i_train_labels)\n",
    "   \n",
    "   #  Predict on the test data\n",
    "   #\n",
    "   l_predicted_labels = i_routine.predict(i_test_data)\n",
    "   l_accuracy         = (i_routine.score(i_test_data, i_test_labels) * 100)\n",
    "      #\n",
    "   l_history.add(event = i_name_of_test, measure = l_accuracy)\n",
    "   \n",
    "   #  Output results\n",
    "   #\n",
    "   print(i_name_of_test + \" ...\")\n",
    "   print(\"   Actual    labels from test......... %s\" % (i_test_labels     ) )\n",
    "   print(\"   Predicted labels from test......... %s\" % (l_predicted_labels) )\n",
    "   print(   \"   ###\")\n",
    "   print(\"   Accuracy: %0.4f %%\" % (l_accuracy))\n",
    "\n",
    "\n",
    "print(\"--\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e65a137-2d39-411c-9316-aff11a9b6108",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Sometimes we want to clear history-\n",
    "#\n",
    "\n",
    "l_history.clear()\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebe2eac-169f-4068-b5a4-d70e1e2856a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  To add a blank line to history-\n",
    "#\n",
    "\n",
    "l_history.add(event = \"\", measure = \"\")\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317e2979-c711-419c-91b4-e035bafbe79c",
   "metadata": {},
   "source": [
    "#  Step A1: Iris Data load, encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d1be95-f9df-4435-ab56-add6aa68b8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Read the Iris data into a Pandas Dataframe\n",
    "#\n",
    "#     Features\n",
    "#     1. sepal length in cm\n",
    "#     2. sepal width in cm\n",
    "#     3. petal length in cm\n",
    "#     4. petal width in cm\n",
    "#     5. class: \n",
    "#        Iris-setosa\n",
    "#        Iris-versicolour\n",
    "#        Iris-virginica\n",
    "#\n",
    "#  To convert class into a numeric, we use sklearn.preprocessing.LabelEncoder\n",
    "#  See,\n",
    "#     https://www.turing.com/kb/convert-categorical-data-in-pandas-and-scikit-learn\n",
    "#\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "my_le = LabelEncoder()\n",
    "   #\n",
    "l_folder = \"20_Data\"\n",
    "l_file   = \"11_iris.data.txt\"\n",
    "\n",
    "\n",
    "pd_iris  = pd.read_csv((l_folder + \"/\" + l_file), header = 0, sep = \",\",\n",
    "   names = [\"sl\", \"sw\", \"pl\", \"pw\", \"class\"],\n",
    "   dtype = {\"sl\": \"float\", \"sw\": \"float\", \"pl\": \"float\", \"pw\": \"float\", \"class\": \"string\"} )\n",
    "      #\n",
    "pd_iris[\"class_encoded\"]  =  my_le.fit_transform(pd_iris[\"class\"])\n",
    "   #\n",
    "pd_iris = pd_iris.drop([\"class\"], axis = 1)\n",
    "    \n",
    "    \n",
    "#  Pandas.Dataframe.sample() returns a randomized set of rows, versus\n",
    "#  say head(), which always returns the first n ..\n",
    "#\n",
    "print(tabulate(pd_iris.sample(5), headers='keys', tablefmt='psql', showindex=False))\n",
    "print(\"Number of rows: %d\" % (len(pd_iris)))\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd64b3f-30aa-4680-b983-462d5da51304",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Split data into training and test.\n",
    "#  Convert the data into numpy arrays, since the ml libraries we use later expect that.\n",
    "#\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np_iris = {}\n",
    "   #\n",
    "np_iris[\"train\"], np_iris[\"test\"] = train_test_split(pd_iris.to_numpy(),              #  random_state calls to shuffle the data,\n",
    "   test_size = 0.20, random_state = 40)                                               #    which had arrived sorted\n",
    "                                                                                      #  10% yields way too high of an accuracy\n",
    "                                                                                      #    far below\n",
    "print(\"Number of total rows: %d   Training rows: %d   Test rows: %d\" %\n",
    "  (len(pd_iris), len(np_iris[\"train\"]), len(np_iris[\"test\"])) )\n",
    "\n",
    "print()\n",
    "print(\"Train data:\")\n",
    "print(\"%s\" % (np_iris[\"train\"][0:5]))\n",
    "print()\n",
    "print(\"Test  data:\")\n",
    "print(\"%s\" % (np_iris[\"test\" ][0:5]))\n",
    "print()\n",
    "   #\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62191e70-57c7-49d1-ba47-cca3517b0d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Boxplot, and normalize ..\n",
    "#\n",
    "#  Normalize from,\n",
    "#     https://datascience.stackexchange.com/questions/39142/normalize-matrix-in-python-numpy\n",
    "#\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "   #\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "print(type(np_iris[\"train\"]))\n",
    "\n",
    "plt.boxplot(np_iris[\"train\"])\n",
    "plt.show()\n",
    "\n",
    "   ###\n",
    "\n",
    "def my_normalize(X, x_min, x_max):\n",
    "   nom = (X-X.min(axis=0))*(x_max-x_min)\n",
    "   denom = X.max(axis=0) - X.min(axis=0)\n",
    "   denom[denom==0] = 1\n",
    "   return x_min + nom/denom \n",
    "\n",
    "#  If we normalize the \"class\" column, we lose the categorical nature\n",
    "#  of that data. So, create a deep copy, then just normalize the non-\n",
    "#  class columns.\n",
    "#\n",
    "np_iris[\"train_norm\"] = np.copy(np_iris[\"train\"])\n",
    "np_iris[\"test_norm\" ] = np.copy(np_iris[\"test\" ])\n",
    "   #\n",
    "np_iris[\"train_norm\"][:, :4] = my_normalize(np_iris[\"train_norm\"][:, :4], 0, 1)\n",
    "np_iris[\"test_norm\" ][:, :4] = my_normalize(np_iris[\"test_norm\" ][:, :4], 0, 1)\n",
    "\n",
    "plt.boxplot(np_iris[\"train_norm\"])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5642d7fa-e89f-47c8-90c4-13e33fc2106c",
   "metadata": {},
   "source": [
    "#  Step A2: Iris Data train, test .. NearestCentroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d15b429-f361-434a-a3c4-f91f48d61824",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "\n",
    "#  Our numpy array has 5 columns, with the last column being the class.\n",
    "#  To review numpy array slicing,\n",
    "#\n",
    "#     To get the first 4 columns use,\n",
    "#        np_iris[\"train\"][:, :4]\n",
    "#     To get the last column use,\n",
    "#        np_iris[\"train\"][:, -1]\n",
    "#\n",
    "\n",
    "do_model(NearestCentroid(), np_iris[\"train\"][:, :4], np_iris[\"train\"][:, -1], np_iris[\"test\"][:, :4], np_iris[\"test\"][:, -1], \"Iris: Centroid\") \n",
    "print()\n",
    "print(tabulate(l_history, headers=[\"Category\", \"Accuracy\"], tablefmt='psql', showindex=False))\n",
    "\n",
    "do_model(NearestCentroid(), np_iris[\"train_norm\"][:, :4], np_iris[\"train_norm\"][:, -1], np_iris[\"test_norm\"][:, :4], np_iris[\"test_norm\"][:, -1], \"Iris: Centroid Normalized\") \n",
    "print()\n",
    "print(tabulate(l_history, headers=[\"Category\", \"Accuracy\"], tablefmt='psql', showindex=False))\n",
    "\n",
    "print(\"--\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c25a3d4-cf32-4503-8c86-e14146b7fa9d",
   "metadata": {},
   "source": [
    "#  Step A3: Iris Data train, test .. kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c1202c-c630-4e27-aea8-4ed425b139ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#  Our numpy array has 5 columns, with the last column being the class.\n",
    "#  To review numpy array slicing,\n",
    "#\n",
    "#     To get the first 4 columns use,\n",
    "#        np_iris[\"train\"][:, :4]\n",
    "#     To get the last column use,\n",
    "#        np_iris[\"train\"][:, -1]\n",
    "#\n",
    "\n",
    "do_model(KNeighborsClassifier(n_neighbors = 3), np_iris[\"train\"][:, :4], np_iris[\"train\"][:, -1], np_iris[\"test\"][:, :4], np_iris[\"test\"][:, -1], \"Iris: kNN=3\" ) \n",
    "print()\n",
    "print(tabulate(l_history, headers=[\"Category\", \"Accuracy\"], tablefmt='psql', showindex=False))\n",
    "\n",
    "do_model(KNeighborsClassifier(n_neighbors = 3), np_iris[\"train_norm\"][:, :4], np_iris[\"train_norm\"][:, -1], np_iris[\"test_norm\"][:, :4], np_iris[\"test_norm\"][:, -1], \"Iris: kNN=3 Normalized\" ) \n",
    "print()\n",
    "print(tabulate(l_history, headers=[\"Category\", \"Accuracy\"], tablefmt='psql', showindex=False))\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5328044-4f54-41b6-a37a-0f96a11e2250",
   "metadata": {},
   "source": [
    "#  Step A4: Iris Data train, test .. Naive Bayes, Gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c1740a-f656-4d6d-bdf3-cb2f053fef60",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "#  Naive Bayes, Gaussian\n",
    "#\n",
    "#     Gaussian usually does better than the Multinomial below because,\n",
    "#        Gaussian expects continuous values\n",
    "#        Multinomial expects discreet values\n",
    "#\n",
    "#     And our values are continuous\n",
    "#\n",
    "\n",
    "do_model(GaussianNB(), np_iris[\"train\"][:, :4], np_iris[\"train\"][:, -1], np_iris[\"test\"][:, :4], np_iris[\"test\"][:, -1], \"Iris: GaussianNB\" ) \n",
    "print()\n",
    "print(tabulate(l_history, headers=[\"Category\", \"Accuracy\"], tablefmt='psql', showindex=False))\n",
    "\n",
    "do_model(GaussianNB(), np_iris[\"train_norm\"][:, :4], np_iris[\"train_norm\"][:, -1], np_iris[\"test_norm\"][:, :4], np_iris[\"test_norm\"][:, -1], \"Iris: GaussianNB Normalized\" ) \n",
    "print()\n",
    "print(tabulate(l_history, headers=[\"Category\", \"Accuracy\"], tablefmt='psql', showindex=False))\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f815d9-6eb7-4aaa-863c-3c7ce613d127",
   "metadata": {},
   "source": [
    "#  Step A5: Iris Data train, test .. Naive Bayes, Multinomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de942e0e-31d0-44c1-bdb3-0ee3c5ca3315",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "#  Naive Bayes, Multinomial\n",
    "#\n",
    "\n",
    "do_model(MultinomialNB(), np_iris[\"train\"][:, :4], np_iris[\"train\"][:, -1], np_iris[\"test\"][:, :4], np_iris[\"test\"][:, -1], \"Iris: MultinomialNB\" ) \n",
    "print()\n",
    "print(tabulate(l_history, headers=[\"Category\", \"Accuracy\"], tablefmt='psql', showindex=False))\n",
    "\n",
    "do_model(MultinomialNB(), np_iris[\"train_norm\"][:, :4], np_iris[\"train_norm\"][:, -1], np_iris[\"test_norm\"][:, :4], np_iris[\"test_norm\"][:, -1], \"Iris: MultinomialNB Normalized\" ) \n",
    "print()\n",
    "print(tabulate(l_history, headers=[\"Category\", \"Accuracy\"], tablefmt='psql', showindex=False))\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662897e8-2b96-4d7a-bf24-74aa2da5c006",
   "metadata": {},
   "source": [
    "#  Step A6: Iris Data train, test .. Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4adb7874-3d2c-4f27-83db-147b953228c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "#  Decision Tree\n",
    "#\n",
    "\n",
    "do_model(DecisionTreeClassifier(), np_iris[\"train\"][:, :4], np_iris[\"train\"][:, -1], np_iris[\"test\"][:, :4], np_iris[\"test\"][:, -1], \"Iris: DecisionTree\" ) \n",
    "print()\n",
    "print(tabulate(l_history, headers=[\"Category\", \"Accuracy\"], tablefmt='psql', showindex=False))\n",
    "\n",
    "do_model(DecisionTreeClassifier(), np_iris[\"train_norm\"][:, :4], np_iris[\"train_norm\"][:, -1], np_iris[\"test_norm\"][:, :4], np_iris[\"test_norm\"][:, -1], \"Iris: DecisionTree Normalized\" ) \n",
    "print()\n",
    "print(tabulate(l_history, headers=[\"Category\", \"Accuracy\"], tablefmt='psql', showindex=False))\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f5d6be-ecfa-4ca1-b518-0b37274c04de",
   "metadata": {},
   "source": [
    "#  Step A7: Iris Data train, test .. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0a74fa-57c5-44ef-ade3-dd5438751064",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#  Random Forest\n",
    "#\n",
    "\n",
    "#  n_estimators, number of random trees created and trained\n",
    "#\n",
    "\n",
    "do_model(RandomForestClassifier(n_estimators = 5), np_iris[\"train\"][:, :4], np_iris[\"train\"][:, -1], np_iris[\"test\"][:, :4], np_iris[\"test\"][:, -1], \"Iris: RandomForest\" ) \n",
    "print()\n",
    "print(tabulate(l_history, headers=[\"Category\", \"Accuracy\"], tablefmt='psql', showindex=False))\n",
    "\n",
    "do_model(RandomForestClassifier(n_estimators = 5), np_iris[\"train_norm\"][:, :4], np_iris[\"train_norm\"][:, -1], np_iris[\"test_norm\"][:, :4], np_iris[\"test_norm\"][:, -1], \"Iris: RandomForest Normalized\" ) \n",
    "print()\n",
    "print(tabulate(l_history, headers=[\"Category\", \"Accuracy\"], tablefmt='psql', showindex=False))\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7973bf-5a1a-4999-810d-91e52c13bd0b",
   "metadata": {},
   "source": [
    "#  Step A8: Iris Data train, test .. Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2dc0262-190c-455d-b2f9-99056c1bea18",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "#  Support Vector Machine\n",
    "#\n",
    "#  We run this one with a number of configurations ..\n",
    "#\n",
    "#     C      ==  margin constant\n",
    "#     gamma  ==  used by the Gaussian kernel\n",
    "#\n",
    "\n",
    "do_model(SVC(kernel = \"linear\", C = 1.0), np_iris[\"train\"][:, :4], np_iris[\"train\"][:, -1], np_iris[\"test\"][:, :4], np_iris[\"test\"][:, -1], \"Iris: SVC/Linear\" ) \n",
    "print()\n",
    "print(tabulate(l_history, headers=[\"Category\", \"Accuracy\"], tablefmt='psql', showindex=False))\n",
    "print()\n",
    "             \n",
    "do_model(SVC(kernel = \"linear\", C = 1.0), np_iris[\"train_norm\"][:, :4], np_iris[\"train_norm\"][:, -1], np_iris[\"test_norm\"][:, :4], np_iris[\"test_norm\"][:, -1], \"Iris: SVC/Linear Normalized\" ) \n",
    "print()\n",
    "print(tabulate(l_history, headers=[\"Category\", \"Accuracy\"], tablefmt='psql', showindex=False))\n",
    "print()\n",
    "\n",
    "             \n",
    "do_model(SVC(kernel = \"rbf\", C = 1.0, gamma = 0.25), np_iris[\"train\"][:, :4], np_iris[\"train\"][:, -1], np_iris[\"test\"][:, :4], np_iris[\"test\"][:, -1], \"Iris: RBF\" ) \n",
    "print()\n",
    "print(tabulate(l_history, headers=[\"Category\", \"Accuracy\"], tablefmt='psql', showindex=False))\n",
    "print()\n",
    "             \n",
    "do_model(SVC(kernel = \"rbf\", C = 1.0, gamma = 0.25), np_iris[\"train_norm\"][:, :4], np_iris[\"train_norm\"][:, -1], np_iris[\"test_norm\"][:, :4], np_iris[\"test_norm\"][:, -1], \"Iris: RBF Normalized\" ) \n",
    "print()\n",
    "print(tabulate(l_history, headers=[\"Category\", \"Accuracy\"], tablefmt='psql', showindex=False))\n",
    "print()\n",
    "\n",
    "             \n",
    "do_model(SVC(kernel = \"rbf\", C = 1.0, gamma = 0.001), np_iris[\"train\"][:, :4], np_iris[\"train\"][:, -1], np_iris[\"test\"][:, :4], np_iris[\"test\"][:, -1], \"Iris: RBF 2\" ) \n",
    "print()\n",
    "print(tabulate(l_history, headers=[\"Category\", \"Accuracy\"], tablefmt='psql', showindex=False))\n",
    "print()\n",
    "\n",
    "do_model(SVC(kernel = \"rbf\", C = 1.0, gamma = 0.001), np_iris[\"train_norm\"][:, :4], np_iris[\"train_norm\"][:, -1], np_iris[\"test_norm\"][:, :4], np_iris[\"test_norm\"][:, -1], \"Iris: RBF 2 Normalized\" ) \n",
    "print()\n",
    "print(tabulate(l_history, headers=[\"Category\", \"Accuracy\"], tablefmt='psql', showindex=False))\n",
    "print()\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b45c758-b0db-43dd-9aa7-832598413085",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "####################################################################\n",
    "####################################################################\n",
    "####################################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc4cf6a-434a-4fcf-bfdb-edc3308c5252",
   "metadata": {},
   "source": [
    "#  Step B1:  Breast Cancer Data load, encode, normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cafe262a-4fb5-4954-9d6f-c8cbea09d763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------+--------+---------+---------+---------+---------+--------+---------+--------+--------+-------+-------+----------+----------+----------+----------+---------+-----------+-------+-------+--------+--------+--------+---------+---------+---------+--------+---------+-----------------+\n",
      "|   f01 |   f02 |    f03 |    f04 |     f05 |     f06 |     f07 |     f08 |    f09 |     f10 |    f11 |    f12 |   f13 |   f14 |      f15 |      f16 |      f17 |      f18 |     f19 |       f20 |   f21 |   f22 |    f23 |    f24 |    f25 |     f26 |     f27 |     f28 |    f29 |     f30 |   class_encoded |\n",
      "|-------+-------+--------+--------+---------+---------+---------+---------+--------+---------+--------+--------+-------+-------+----------+----------+----------+----------+---------+-----------+-------+-------+--------+--------+--------+---------+---------+---------+--------+---------+-----------------|\n",
      "| 15.78 | 17.89 | 103.6  |  781   | 0.0971  | 0.1292  | 0.09954 | 0.06606 | 0.1842 | 0.06082 | 0.5058 | 0.9849 | 3.564 | 54.16 | 0.005771 | 0.04061  | 0.02791  | 0.01282  | 0.02008 | 0.004144  | 20.42 | 27.28 | 136.5  | 1299   | 0.1396 | 0.5609  | 0.3965  | 0.181   | 0.3792 | 0.1048  |               1 |\n",
      "| 11.95 | 14.96 |  77.23 |  426.7 | 0.1158  | 0.1206  | 0.01171 | 0.01787 | 0.2459 | 0.06581 | 0.361  | 1.05   | 2.455 | 26.65 | 0.0058   | 0.02417  | 0.007816 | 0.01052  | 0.02734 | 0.003114  | 12.81 | 17.72 |  83.09 |  496.2 | 0.1293 | 0.1885  | 0.03122 | 0.04766 | 0.3124 | 0.0759  |               0 |\n",
      "| 20.48 | 21.46 | 132.5  | 1306   | 0.08355 | 0.08348 | 0.09042 | 0.06022 | 0.1467 | 0.05177 | 0.6874 | 1.041  | 5.144 | 83.5  | 0.007959 | 0.03133  | 0.04257  | 0.01671  | 0.01341 | 0.003933  | 24.22 | 26.17 | 161.7  | 1750   | 0.1228 | 0.2311  | 0.3158  | 0.1445  | 0.2238 | 0.07127 |               1 |\n",
      "| 14.95 | 18.77 |  97.84 |  689.5 | 0.08138 | 0.1167  | 0.0905  | 0.03562 | 0.1744 | 0.06493 | 0.422  | 1.909  | 3.271 | 39.43 | 0.00579  | 0.04877  | 0.05303  | 0.01527  | 0.03356 | 0.009368  | 16.25 | 25.47 | 107.1  |  809.7 | 0.0997 | 0.2521  | 0.25    | 0.08405 | 0.2852 | 0.09218 |               0 |\n",
      "| 14.61 | 15.69 |  92.68 |  664.9 | 0.07618 | 0.03515 | 0.01447 | 0.01877 | 0.1632 | 0.05255 | 0.316  | 0.9115 | 1.954 | 28.9  | 0.005031 | 0.006021 | 0.005325 | 0.006324 | 0.01494 | 0.0008948 | 16.46 | 21.75 | 103.7  |  840.8 | 0.1011 | 0.07087 | 0.04746 | 0.05813 | 0.253  | 0.05695 |               0 |\n",
      "+-------+-------+--------+--------+---------+---------+---------+---------+--------+---------+--------+--------+-------+-------+----------+----------+----------+----------+---------+-----------+-------+-------+--------+--------+--------+---------+---------+---------+--------+---------+-----------------+\n",
      "Number of rows: 568\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#  Read the Breast Cancer data into a Pandas Dataframe\n",
    "#\n",
    "#     Features\n",
    "#     1)     ID number\n",
    "#     2)     Diagnosis (M = malignant, B = benign)\n",
    "#     3-32)\n",
    "#       Ten real-valued features are computed for each cell nucleus:\n",
    "#     \n",
    "#     \ta) radius (mean of distances from center to points on the perimeter)\n",
    "#     \tb) texture (standard deviation of gray-scale values)\n",
    "#     \tc) perimeter\n",
    "#     \td) area\n",
    "#     \te) smoothness (local variation in radius lengths)\n",
    "#     \tf) compactness (perimeter^2 / area - 1.0)\n",
    "#     \tg) concavity (severity of concave portions of the contour)\n",
    "#     \th) concave points (number of concave portions of the contour)\n",
    "#     \ti) symmetry \n",
    "#     \tj) fractal dimension (\"coastline approximation\" - 1)\n",
    "#\n",
    "#  Sample data line,\n",
    "#     842302,M,\n",
    "#     17.99,    10.38,    122.8,    1001,    0.1184,    0.2776,    0.3001,    0.1471,    0.2419,    0.07871,         #  10 count\n",
    "#     1.095,    0.9053,   8.589,    153.4,   0.006399,  0.04904,   0.05373,   0.01587,   0.03003,   0.006193,\n",
    "#     25.38,    17.33,    184.6,    2019,    0.1622,    0.6656,    0.7119,    0.2654,    0.4601     ,0.1189\n",
    "#\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "my_le = LabelEncoder()\n",
    "   #\n",
    "l_folder = \"20_Data\"\n",
    "l_file   = \"22_wdbc.data.txt\"\n",
    "\n",
    "\n",
    "pd_bc  = pd.read_csv((l_folder + \"/\" + l_file), header = 0, sep = \",\",\n",
    "   names = [\"id\", \"class\",\n",
    "            \"f01\", \"f02\", \"f03\", \"f04\", \"f05\", \"f06\", \"f07\", \"f08\", \"f09\", \"f10\", \n",
    "            \"f11\", \"f12\", \"f13\", \"f14\", \"f15\", \"f16\", \"f17\", \"f18\", \"f19\", \"f20\", \n",
    "            \"f21\", \"f22\", \"f23\", \"f24\", \"f25\", \"f26\", \"f27\", \"f28\", \"f29\", \"f30\", \n",
    "           ],\n",
    "   dtype = {\"id\": \"int\", \"class\": \"string\",\n",
    "            \"f01\": \"float\", \"f02\": \"float\", \"f03\": \"float\", \"f04\": \"float\", \"f05\": \"float\", \"f06\": \"float\", \"f07\": \"float\", \"f08\": \"float\", \"f09\": \"float\", \"f10\": \"float\", \n",
    "            \"f11\": \"float\", \"f12\": \"float\", \"f13\": \"float\", \"f14\": \"float\", \"f15\": \"float\", \"f16\": \"float\", \"f17\": \"float\", \"f18\": \"float\", \"f19\": \"float\", \"f20\": \"float\", \n",
    "            \"f21\": \"float\", \"f22\": \"float\", \"f23\": \"float\", \"f24\": \"float\", \"f25\": \"float\", \"f26\": \"float\", \"f27\": \"float\", \"f28\": \"float\", \"f29\": \"float\", \"f30\": \"float\", \n",
    "           } )\n",
    "      #\n",
    "pd_bc[\"class_encoded\"]  =  my_le.fit_transform(pd_bc[\"class\"])\n",
    "   #\n",
    "pd_bc = pd_bc.drop([\"class\", \"id\"], axis = 1)\n",
    "    \n",
    "    \n",
    "#  Pandas.Dataframe.sample() returns a randomized count of rows\n",
    "#\n",
    "print(tabulate(pd_bc.sample(5), headers='keys', tablefmt='psql', showindex=False))\n",
    "print(\"Number of rows: %d\" % (len(pd_bc)))\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "afe05409-32ac-4da0-bd05-6d97b35df9c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of total rows: 568   Training rows: 454   Test rows: 114\n",
      "\n",
      "Train data:\n",
      "[[12.1 14.6 78 449 0.103 0.0909 0.0659 0.0275 0.168 0.0604 0.264 0.729 1.85 19.9 0.00549 0.0143 0.0232 0.00566 0.0143 0.00242 13.8 20.7 89.9 583 0.149 0.216 0.305 0.0655 0.275 0.083 0]\n",
      " [15.7 11.3 103 747 0.104 0.13 0.119 0.0621 0.178 0.0626 0.163 0.387 1.14 13.9 0.00603 0.0182 0.0334 0.0107 0.0118 0.00226 17 14.2 112 854 0.154 0.298 0.4 0.145 0.256 0.0818 0]\n",
      " [14.3 16.8 90.3 633 0.0643 0.0267 0.00725 0.00625 0.151 0.0538 0.13 0.72 0.844 10.8 0.00349 0.00371 0.00483 0.00361 0.0154 0.00138 14.9 20.6 94.4 685 0.0857 0.0504 0.0387 0.0333 0.246 0.0612 0]\n",
      " [19.4 23.5 129 1.16e+03 0.103 0.156 0.205 0.0889 0.198 0.06 0.524 1.8 4.04 60.4 0.0106 0.0325 0.0391 0.0156 0.0219 0.00395 21.6 30.5 145 1.42e+03 0.146 0.297 0.346 0.156 0.292 0.0761 1]\n",
      " [10.2 17.5 65.1 313 0.106 0.085 0.0177 0.0192 0.191 0.0691 0.247 1.22 1.64 15.1 0.0079 0.014 0.00853 0.00762 0.0264 0.00376 11.2 22.8 71.9 376 0.141 0.144 0.0657 0.0558 0.305 0.088 0]]\n",
      "\n",
      "Test  data:\n",
      "[[10.9 12.3 69.1 364 0.0852 0.0472 0.0124 0.0137 0.145 0.0603 0.175 1.03 1.27 11.1 0.00348 0.0122 0.0107 0.00939 0.0294 0.00343 11.4 14.8 72.4 392 0.0931 0.0751 0.0288 0.0319 0.214 0.0664 0]\n",
      " [16.1 20.7 108 799 0.117 0.202 0.172 0.103 0.216 0.0736 0.569 1.07 3.85 54.2 0.00703 0.025 0.0319 0.013 0.0169 0.00414 21 31.5 137 1.32e+03 0.179 0.423 0.478 0.207 0.371 0.114 1]\n",
      " [20.2 19.5 134 1.25e+03 0.113 0.149 0.213 0.126 0.172 0.0605 0.433 1 3.01 52.5 0.00909 0.0272 0.0555 0.0191 0.0245 0.004 22 25.1 146 1.48e+03 0.167 0.294 0.531 0.217 0.303 0.0808 1]\n",
      " [12.6 17.1 80.6 493 0.0858 0.0543 0.0297 0.0227 0.18 0.0583 0.169 0.667 1.12 13.3 0.00389 0.00854 0.0126 0.00689 0.0161 0.00164 14.3 22.1 91.6 634 0.122 0.152 0.189 0.0985 0.327 0.0733 0]\n",
      " [14.7 20.1 94.7 684 0.0987 0.072 0.074 0.0526 0.159 0.0592 0.473 1.24 3.19 45.4 0.00572 0.0116 0.02 0.0111 0.0141 0.00209 19.1 30.9 123 1.14e+03 0.146 0.187 0.291 0.161 0.303 0.0822 1]]\n",
      "\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#  Split data into training and test.\n",
    "#  Convert the data into numpy arrays, since the ml libraries we use later expect that.\n",
    "#\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np_bc = {}\n",
    "   #\n",
    "np_bc[\"train\"], np_bc[\"test\"] = train_test_split(pd_bc.to_numpy(),                    #  random_state calls to shuffle the data,\n",
    "   test_size = 0.20, random_state = 40)                                               #    which had arrived sorted\n",
    "                                                                                      #  10% yields way too high of an accuracy\n",
    "                                                                                      #    far below\n",
    "print(\"Number of total rows: %d   Training rows: %d   Test rows: %d\" %\n",
    "  (len(pd_bc), len(np_bc[\"train\"]), len(np_bc[\"test\"])) )\n",
    "\n",
    "print()\n",
    "print(\"Train data:\")\n",
    "print(\"%s\" % (np_bc[\"train\"][0:5]))\n",
    "print()\n",
    "print(\"Test  data:\")\n",
    "print(\"%s\" % (np_bc[\"test\" ][0:5]))\n",
    "print()\n",
    "   #\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f885ac6-db00-4fd0-9ef3-c7b5bd130542",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def my_normalize(X, x_min, x_max):\n",
    "   nom = (X-X.min(axis=0))*(x_max-x_min)\n",
    "   denom = X.max(axis=0) - X.min(axis=0)\n",
    "   denom[denom==0] = 1\n",
    "   return x_min + nom/denom \n",
    "\n",
    "#  If we normalize the \"class\" column, we lose the categorical nature\n",
    "#  of that data. So, create a deep copy, then just normalize the non-\n",
    "#  class columns.\n",
    "#\n",
    "np_bc[\"train_norm\"] = np.copy(np_bc[\"train\"])\n",
    "np_bc[\"test_norm\" ] = np.copy(np_bc[\"test\" ])\n",
    "   #\n",
    "np_bc[\"train_norm\"][:, :4] = my_normalize(np_bc[\"train_norm\"][:, :4], 0, 1)\n",
    "np_bc[\"test_norm\" ][:, :4] = my_normalize(np_bc[\"test_norm\" ][:, :4], 0, 1)\n",
    "\n",
    "plt.boxplot(np_bc[\"train_norm\"])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a63d6f-96d6-42c3-9782-8be69c555bef",
   "metadata": {},
   "source": [
    "#  Step B2:  Breast Cancer Data, run against all models .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf36b4ec-23da-4af1-8cb4-fe5669cc5cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  All of our model libraries were imported above, but ..\n",
    "#\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "#  To add a blank line to history-\n",
    "#\n",
    "l_history.add(event = \"\", measure = \"\")\n",
    "\n",
    "\n",
    "#  Our numpy array has many columns, with the last column being the class.\n",
    "#\n",
    "#  To review numpy array slicing,\n",
    "#\n",
    "#     To get the first 30 columns use,\n",
    "#        np_iris[\"train\"][:, :30]\n",
    "#     To get the last column use,\n",
    "#        np_iris[\"train\"][:, -1]\n",
    "#\n",
    "\n",
    "do_model(NearestCentroid(), np_bc[\"train\"][:, :4], np_bc[\"train\"][:, -1], np_bc[\"test\"][:, :4], np_bc[\"test\"][:, -1], \"Breast Cancer: Centroid\") \n",
    "print()\n",
    "\n",
    "do_model(KNeighborsClassifier(n_neighbors = 3), np_bc[\"train\"][:, :4], np_bc[\"train\"][:, -1], np_bc[\"test\"][:, :4], np_bc[\"test\"][:, -1], \"Breast Cancer: kNN=3\") \n",
    "print()\n",
    "\n",
    "do_model(GaussianNB(), np_bc[\"train\"][:, :4], np_bc[\"train\"][:, -1], np_bc[\"test\"][:, :4], np_bc[\"test\"][:, -1], \"Breast Cancer: GaussianNB\") \n",
    "print()\n",
    "\n",
    "do_model(MultinomialNB(), np_bc[\"train\"][:, :4], np_bc[\"train\"][:, -1], np_bc[\"test\"][:, :4], np_bc[\"test\"][:, -1], \"Breast Cancer: MultinomialNB\") \n",
    "print()\n",
    "\n",
    "do_model(DecisionTreeClassifier(), np_bc[\"train\"][:, :4], np_bc[\"train\"][:, -1], np_bc[\"test\"][:, :4], np_bc[\"test\"][:, -1], \"Breast Cancer: DecisionTree\") \n",
    "print()\n",
    "\n",
    "#  n_estimators, number of random trees created and trained\n",
    "#\n",
    "do_model(RandomForestClassifier(n_estimators = 5), np_bc[\"train\"][:, :4], np_bc[\"train\"][:, -1], np_bc[\"test\"][:, :4], np_bc[\"test\"][:, -1], \"Breast Cancer: Random Forest = 5\") \n",
    "print()\n",
    "\n",
    "   ###\n",
    "\n",
    "#  Support Vector Machine\n",
    "#\n",
    "#  We run this one with a number of configurations ..\n",
    "#\n",
    "#     C      ==  margin constant\n",
    "#     gamma  ==  used by the Gaussian kernel\n",
    "#\n",
    "\n",
    "do_model(SVC(kernel = \"linear\", C = 1.0), np_bc[\"train\"][:, :4], np_bc[\"train\"][:, -1], np_bc[\"test\"][:, :4], np_bc[\"test\"][:, -1], \"Breast Cancer: SVC/Linear\") \n",
    "print()\n",
    "\n",
    "do_model(SVC(kernel = \"rbf\", C = 1.0, gamma = 1.0), np_bc[\"train\"][:, :4], np_bc[\"train\"][:, -1], np_bc[\"test\"][:, :4], np_bc[\"test\"][:, -1], \"Breast Cancer: RBF\") \n",
    "print()\n",
    "\n",
    "do_model(SVC(kernel = \"rbf\", C = 1.0, gamma = 0.001), np_bc[\"train\"][:, :4], np_bc[\"train\"][:, -1], np_bc[\"test\"][:, :4], np_bc[\"test\"][:, -1], \"Breast Cancer: RBF 2\") \n",
    "print()\n",
    "\n",
    "\n",
    "print()\n",
    "print(tabulate(l_history, headers=[\"Category\", \"Accuracy\"], tablefmt='psql', showindex=False))\n",
    "print()\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f9a359-02b1-4bb0-aced-216eb7cad1f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c66fc5d-8f1b-4243-8f06-69a91d8ae685",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8916a655-4bb6-4e41-be06-e11b244da0d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d37e56f-cca5-4af4-9bb5-89be55420d0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff57e71-9dc7-4e7c-9e6c-eb447e07867a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
