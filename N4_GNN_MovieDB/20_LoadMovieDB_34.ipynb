{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bfd2274-8be4-4517-a0bb-e5bf3e9370a2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#  Notebook overview .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4f14aa-01d9-4d3c-9986-a42fa4b15d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  This Notebook is part of a set that demonstrate GNN using a movie dataset.\n",
    "#  About this Notebook,\n",
    "#\n",
    "#  .  There was a Kaggle GNN challenge circa 2019 detailed here,\n",
    "#        https://www.kaggle.com/c/movie-genre-classification/data\n",
    "#\n",
    "#     That data is locked down, but a similarly themed dataset also on \n",
    "#     Kaggle is here,\n",
    "#        https://www.kaggle.com/datasets/rounakbanik/the-movies-dataset\n",
    "#\n",
    "#     And this is the data set in use here.\n",
    "#\n",
    "#  .  1 GB, plus or minus. Most of that volume comes from reviews. The\n",
    "#     movies run 30-40 MB, and the cast and crew about 190 MB.\n",
    "#\n",
    "#     The data is CSV, with embedded arrays of JSON.\n",
    "#     To remove dependencies on GS/S3 hosted data, this program expects this\n",
    "#     data to be local to the container hosting this Jupyter Notebook.\n",
    "#\n",
    "#     Since we host on GitHub, and GitHub has a 25 MB file size limit,\n",
    "#     the total data set is now split across multiple files.\n",
    "#\n",
    "#  .  The existing schema for just Movies is listed here,\n",
    "#  \n",
    "#        10_movies_metadata.csv\n",
    "#        -----------------------------------------\n",
    "#           adult                      ..   False\n",
    "#           belongs_to_collection      ..\n",
    "#           budget                     ..   2700000\n",
    "#           genres                     ..   \"[{'id': 35, 'name': 'Comedy'}]\"\n",
    "#           homepage                   ..   http://www.animalhouse.com/\n",
    "#           id                         ..   8469\n",
    "#           imdb_id                    ..   tt0077975\n",
    "#           original_language          ..   en\n",
    "#           original_title             ..   Animal House\n",
    "#           overview                   ..   \"At a 1962 College, Dean Vernon Wormer is determined to expel\n",
    "#                                            the entire Delta Tau Chi Fraternity, but those troublemakers\n",
    "#                                            have other plans for him.\"\n",
    "#           popularity                 ..   7.525382\n",
    "#           poster_path                ..   /AuJkgAh7zAGsm7Oo3CGyDtYvzg0.jpg\n",
    "#           production_companies       ..   \"[{'name': 'Universal Pictures', 'id': 33}, {'name': 'Oregon Film Factory',\n",
    "#                                               'id': 13298}, {'name': 'Stage III Productions', 'id': 13300}]\"\n",
    "#           production_countries       ..   \"[{'iso_3166_1': 'US', 'name': 'United States of America'}]\"\n",
    "#           release_date               ..   1978-07-27\n",
    "#           revenue                    ..   141000000\n",
    "#           runtime                    ..   109.0\n",
    "#           spoken_languages           ..   \"[{'iso_639_1': 'en', 'name': 'English'}]\"\n",
    "#           status                     ..   Released\n",
    "#           tagline                    ..   It was the Deltas against the rules... the rules lost!\n",
    "#           title                      ..   Animal House\n",
    "#           video                      ..   False\n",
    "#           vote_average               ..   7.0\n",
    "#           vote_count                 ..   420\n",
    "#\n",
    "#     From the above, we load the following into a DataFrame of Movies nodes,\n",
    "#\n",
    "#           id                         ..   8469\n",
    "#           title                      ..   Animal House\n",
    "#           genres                     ..   \"[{'id': 35, 'name': 'Comedy'}]\"\n",
    "#           overview                   ..   \"At a 1962 College, Dean Vernon Wormer is determined to expel\n",
    "#                                            the entire Delta Tau Chi Fraternity, but those troublemakers\n",
    "#                                            have other plans for him.\"\n",
    "#           tagline                    ..   It was the Deltas against the rules... the rules lost!\n",
    "#\n",
    "#           popularity                 ..   7.525382\n",
    "#           production_companies       ..   \"[{'name': 'Universal Pictures', 'id': 33}, {'name': 'Oregon Film Factory',\n",
    "#                                               'id': 13298}, {'name': 'Stage III Productions', 'id': 13300}]\"\n",
    "#           release_date               ..   1978-07-27\n",
    "#           revenue                    ..   141000000\n",
    "#           runtime                    ..   109.0\n",
    "#           spoken_languages           ..   \"[{'iso_639_1': 'en', 'name': 'English'}]\"\n",
    "#           vote_average               ..   7.0\n",
    "#           vote_count                 ..   420\n",
    "#\n",
    "#      Notice the following from above,\n",
    "#\n",
    "#         ..  genres is an array of JSON, with each genre being unique identified via a numeric.\n",
    "#             We will take the first genre and put it into a property on each node titled, primary_genre.\n",
    "#         ..  We will leave all remaining JSON untouched, stored as strings.\n",
    "#\n",
    "#  .  The existing schema is for Keywords is listed here,\n",
    "#\n",
    "#        11_keywords.csv\n",
    "#        -----------------------------------------\n",
    "#           id                         ..   8469\n",
    "#           keywords                   ..   \"[{'id': 572, 'name': 'sex'}, {'id': 2483, 'name': 'nudity'},\n",
    "#                                             {'id': 3616, 'name': 'college'}, {'id': 157632, 'name': 'fraternity'},\n",
    "#                                             {'id': 158507, 'name': 'gross out comedy'}, {'id': 160450, 'name': 'dean'},\n",
    "#                                             {'id': 171400, 'name': 'fraternity house'}, {'id': 208983, 'name': 'probation'},\n",
    "#                                             {'id': 208992, 'name': '1960s'}, {'id': 209506, 'name': 'college freshman'},\n",
    "#                                             {'id': 236316, 'name': 'anarchic comedy'}]\"\n",
    "#\n",
    "#      From the above, the following is offered,\n",
    "#   \n",
    "#         ..  id  joins with movie.id\n",
    "#         ..  keywords.id  already enumerates keywords associated with the movies for us.\n",
    "#             Super handy.\n",
    "#\n",
    "#  Below we continue by loading the raw data, and performing some validations on\n",
    "#  statements made, assumptions, and similar.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555a2149-2554-45a3-be06-8e2ebea43932",
   "metadata": {},
   "source": [
    "#  Load just Movies into a DataFrame, perform analysis .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a874df7-993f-404d-aa53-c8ca266f40a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import dask.dataframe as dd\n",
    "import numpy as np\n",
    "   #\n",
    "import json\n",
    "\n",
    "from tabulate import tabulate\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338c62ab-efc7-44de-ac83-4a0cd23f27da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Load DataFrame with raw input data\n",
    "\n",
    "l_InputFiles  = [\n",
    "   \"./02_Files/40_Movies_01.txt\",\n",
    "   \"./02_Files/41_Movies_02.txt\",\n",
    "]\n",
    "\n",
    "df_data = dd.read_csv(\n",
    "   l_InputFiles,\n",
    "   delimiter  = \",\",\n",
    "   skiprows   = 1,                            #  Skip the first line of each file, since it's the column headers\n",
    "   dtype      = {\n",
    "      \"adult\"                     : np.dtype(str),\n",
    "      \"belongs_to_collection\"     : np.dtype(str),\n",
    "      \"budget\"                    : np.dtype(str),\n",
    "      \"genres\"                    : np.dtype(str),\n",
    "      \"homepage\"                  : np.dtype(str),\n",
    "      \"id\"                        : np.dtype(str),\n",
    "      \"imdb_id\"                   : np.dtype(str),\n",
    "      \"original_language\"         : np.dtype(str),\n",
    "      \"original_title\"            : np.dtype(str),\n",
    "      \"overview\"                  : np.dtype(str),\n",
    "      \"popularity\"                : np.dtype(str),\n",
    "      \"poster_path\"               : np.dtype(str),\n",
    "      \"production_companies\"      : np.dtype(str),\n",
    "      \"production_countries\"      : np.dtype(str),\n",
    "      \"release_date\"              : np.dtype(str),\n",
    "      \"revenue\"                   : np.dtype(str),\n",
    "      \"runtime\"                   : np.dtype(str),\n",
    "      \"spoken_languages\"          : np.dtype(str),\n",
    "      \"status\"                    : np.dtype(str),\n",
    "      \"tagline\"                   : np.dtype(str),\n",
    "      \"title\"                     : np.dtype(str),\n",
    "      \"video\"                     : np.dtype(str),\n",
    "      \"vote_average\"              : np.dtype(str),\n",
    "      \"vote_count\"                : np.dtype(str),\n",
    "      },\n",
    "   names      = [\n",
    "      \"adult\", \"belongs_to_collection\", \"budget\", \"genres\", \"homepage\", \"id\", \"imdb_id\",\n",
    "      \"original_language\", \"original_title\", \"overview\", \"popularity\", \"poster_path\",\n",
    "      \"production_companies\", \"production_countries\", \"release_date\", \"revenue\", \"runtime\",\n",
    "      \"spoken_languages\", \"status\", \"tagline\", \"title\", \"video\", \"vote_average\", \"vote_count\",\n",
    "      ]\n",
    "   )   \n",
    "\n",
    "df_data.compute()\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55dab9a4-76a1-400c-83d3-2ead7cfe57e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Initial look at the data, sanity check-\n",
    "#\n",
    "\n",
    "#  print(len(df_data.index))\n",
    "#     #\n",
    "#  print(tabulate(df_data.head(2), headers='keys', tablefmt='psql'))\n",
    "#  \n",
    "#  print(\"--\")\n",
    "\n",
    "#  Sample output,\n",
    "#\n",
    "#  45466\n",
    "#\n",
    "#  +----+---------+----------------------------------------------------------------------------------------------\n",
    "#  |    | adult   | belongs_to_collection                                                                       \n",
    "#  |----+---------+--------------------------------------------------------------------------------------------\n",
    "#  |  0 | False   | {'id': 10194, 'name': 'Toy Story Collection', 'poster_path': '/7G9915LfUQ2lVfwMEEhDsn3kT4B.jpg',\n",
    "#     'backdrop_path': '/9FBwqcd9IRruEDUrTdcaafOMKUq.jpg'} | 30000000 | [{'id': 16, 'name': 'Animation'},\n",
    "#     {'id': 35, 'name': 'Comedy'}, {'id': 10751, 'name': 'Family'}]  | http://toystory.disney.com/toy-story |  862 |\n",
    "#     tt0114709 | en                  | Toy Story        | Led by Woody, Andy's toys live happily in his room until\n",
    "#        Andy's birthday brings Buzz Lightyear onto the scene. Afraid of losing his place in Andy's heart, Woody plots\n",
    "#        against Buzz. But when circumstances separate Buzz and Woody from their owner, the duo eventually learns to put\n",
    "#        aside their differences.                                                                                             \n",
    "#     |      21.9469 | /rhIRbceoE9lR4veEXuwCC2wARtG.jpg | [{'name': 'Pixar Animation Studios', 'id': 3}]                                                                                      | [{'iso_3166_1': 'US', 'name': 'United States of America'}] | 1995-10-30     | 373554033 |        81 | [{'iso_639_1': 'en', 'name': 'English'}]                                          | Released | nan                                       | Toy Story | False   |            7.7 |         5415 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a827014-0911-4eb4-ab37-4fa6bf8453b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Drop unwanted columns\n",
    "#\n",
    "df_data2 = df_data[[\"id\", \"title\", \"overview\", \"tagline\", \"budget\", \"genres\",\n",
    "   \"popularity\", \"production_companies\", \"release_date\", \"revenue\", \"runtime\",\n",
    "   \"vote_average\", \"vote_count\", ]]\n",
    "\n",
    "#  print(tabulate(df_data2.head(2), headers='keys', tablefmt='psql'))\n",
    "\n",
    "print(\"--\")\n",
    "\n",
    "\n",
    "#  Check column type of genres\n",
    "#\n",
    "#  l_cntr = 0\n",
    "#     #\n",
    "#  for l_each in df_data2.iterrows():\n",
    "#     l_cntr += 1\n",
    "#     if (l_cntr < 3):\n",
    "#        print(type(l_each[1][5]))          #  genres\n",
    "#        print(     l_each[1][5] )\n",
    "\n",
    "#  Sample output,\n",
    "#\n",
    "#     <class 'str'>\n",
    "#     [{'id': 16, 'name': 'Animation'}, {'id': 35, 'name': 'Comedy'}, {'id': 10751, 'name': 'Family'}]\n",
    "#     <class 'str'>\n",
    "#     [{'id': 12, 'name': 'Adventure'}, {'id': 14, 'name': 'Fantasy'}, {'id': 10751, 'name': 'Family'}]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c86d12b-b0e7-452b-a5a5-bf105bda91b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Save the original 'genres' string as 'genres_str'.\n",
    "#  Then, convert genres to a array of distionary values.\n",
    "#\n",
    "\n",
    "def f_convert_genres(i_arg1):\n",
    "   try:\n",
    "      l_str1   = str(i_arg1)                                                  #  Needed this, was getting odd  json.loads()  errors otherwise\n",
    "      l_str2   = l_str1.replace(\"'\", \"\\\"\")\n",
    "      l_return = json.loads(l_str2)\n",
    "   except:\n",
    "      l_return = json.loads('[{\"id\": 99999999, \"name\": \"Unknown\"}]')\n",
    "   return l_return\n",
    "\n",
    "\n",
    "#    The reason for the if is to prevent error upon multiple\n",
    "#    executions of this code.\n",
    "#\n",
    "if (\"genres_str\" not in df_data2):\n",
    "   df_data2[\"genres_str\"   ] = df_data2.genres.map(lambda x: x            )\n",
    "   df_data2[\"genres\"       ] = df_data2.genres.map(lambda x: f_convert_genres(x) )\n",
    "   print(\"--\")\n",
    "    \n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63ceeb1-0323-435d-a5eb-40772b238764",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Check column type of genres\n",
    "#\n",
    "#  l_cntr = 0\n",
    "#     #\n",
    "#  for l_each in df_data2.iterrows():\n",
    "#     l_cntr += 1\n",
    "#        #\n",
    "#     if (l_cntr < 3):\n",
    "#        print(type(l_each[1][5]))                     #  genres\n",
    "#        print(     l_each[1][5] )\n",
    "#           #\n",
    "#        print(type(l_each[1][5][0]))                  #  genres, first element\n",
    "#        print(     l_each[1][5][0] )\n",
    "#           #\n",
    "#        print(type(l_each[1][5][0][\"name\"]))          #  genres, first element, given key value\n",
    "#        print(     l_each[1][5][0][\"name\"] )\n",
    "        \n",
    "#  Sample output,\n",
    "#  \n",
    "#     <class 'list'>\n",
    "#     [{'id': 16, 'name': 'Animation'}, {'id': 35, 'name': 'Comedy'}, {'id': 10751, 'name': 'Family'}]\n",
    "#     <class 'dict'>\n",
    "#     {'id': 16, 'name': 'Animation'}\n",
    "#     <class 'str'>\n",
    "#     Animation\n",
    "#     <class 'list'>\n",
    "#     [{'id': 12, 'name': 'Adventure'}, {'id': 14, 'name': 'Fantasy'}, {'id': 10751, 'name': 'Family'}]\n",
    "#     <class 'dict'>\n",
    "#     {'id': 12, 'name': 'Adventure'}\n",
    "#     <class 'str'>\n",
    "#     Adventure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ae8e1f19-e1cd-4668-8e5b-57c72cc3e696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#  Analysis on just genres-\n",
    "#\n",
    "#     .  Copy just genres into a new DataFrame. \n",
    "#     .  Pivot this list of genres into separate rows.\n",
    "\n",
    "\n",
    "df_genres = df_data2[[\"genres\"]]\n",
    "   #\n",
    "df_genres2 = df_genres.explode(\"genres\")\n",
    "\n",
    "print(\"--\")\n",
    "\n",
    "#  l_cntr = 0\n",
    "#     #\n",
    "#  for l_each in df_genres2.iterrows():\n",
    "#     l_cntr += 1\n",
    "#        #\n",
    "#     if (l_cntr < 3):\n",
    "#        print(type(l_each))\n",
    "#        print(     l_each )\n",
    "#           #\n",
    "#        print(type(l_each[1][0]))\n",
    "#        print(     l_each[1][0] )\n",
    "#           #\n",
    "#        print(type(l_each[1][0][\"name\"]))\n",
    "#        print(     l_each[1][0][\"name\"] )\n",
    "#        print(\"\")\n",
    "\n",
    "#  Sample data,\n",
    "#\n",
    "#     <class 'tuple'>\n",
    "#     (0, genres    {'id': 16, 'name': 'Animation'}\n",
    "#     Name: 0, dtype: object)\n",
    "#     <class 'dict'>\n",
    "#     {'id': 16, 'name': 'Animation'}\n",
    "#     <class 'str'>\n",
    "#     Animation\n",
    "#     \n",
    "#     <class 'tuple'>\n",
    "#     (0, genres    {'id': 35, 'name': 'Comedy'}\n",
    "#     Name: 0, dtype: object)\n",
    "#     <class 'dict'>\n",
    "#     {'id': 35, 'name': 'Comedy'}\n",
    "#     <class 'str'>\n",
    "#     Comedy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a5c4ee8d-517a-4df1-ba31-9a86418b0291",
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [63], line 30\u001b[0m\n\u001b[1;32m     28\u001b[0m l_cntr \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     29\u001b[0m    \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m l_each \u001b[38;5;129;01min\u001b[39;00m df_genres2\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[1;32m     31\u001b[0m    l_cntr \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     32\u001b[0m       \u001b[38;5;66;03m#\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/dask/dataframe/core.py:5377\u001b[0m, in \u001b[0;36mDataFrame.iterrows\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   5374\u001b[0m \u001b[38;5;129m@derived_from\u001b[39m(pd\u001b[38;5;241m.\u001b[39mDataFrame)\n\u001b[1;32m   5375\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21miterrows\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   5376\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnpartitions):\n\u001b[0;32m-> 5377\u001b[0m         df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_partition\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5378\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m df\u001b[38;5;241m.\u001b[39miterrows()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/dask/base.py:315\u001b[0m, in \u001b[0;36mDaskMethodsMixin.compute\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;124;03m\"\"\"Compute this dask collection\u001b[39;00m\n\u001b[1;32m    293\u001b[0m \n\u001b[1;32m    294\u001b[0m \u001b[38;5;124;03m    This turns a lazy Dask collection into its in-memory equivalent.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;124;03m    dask.base.compute\u001b[39;00m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 315\u001b[0m     (result,) \u001b[38;5;241m=\u001b[39m \u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraverse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/dask/base.py:598\u001b[0m, in \u001b[0;36mcompute\u001b[0;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[1;32m    595\u001b[0m     keys\u001b[38;5;241m.\u001b[39mappend(x\u001b[38;5;241m.\u001b[39m__dask_keys__())\n\u001b[1;32m    596\u001b[0m     postcomputes\u001b[38;5;241m.\u001b[39mappend(x\u001b[38;5;241m.\u001b[39m__dask_postcompute__())\n\u001b[0;32m--> 598\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mschedule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdsk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    599\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m repack([f(r, \u001b[38;5;241m*\u001b[39ma) \u001b[38;5;28;01mfor\u001b[39;00m r, (f, a) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(results, postcomputes)])\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/dask/threaded.py:89\u001b[0m, in \u001b[0;36mget\u001b[0;34m(dsk, keys, cache, num_workers, pool, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(pool, multiprocessing\u001b[38;5;241m.\u001b[39mpool\u001b[38;5;241m.\u001b[39mPool):\n\u001b[1;32m     87\u001b[0m         pool \u001b[38;5;241m=\u001b[39m MultiprocessingPoolExecutor(pool)\n\u001b[0;32m---> 89\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mget_async\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_max_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdsk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m    \u001b[49m\u001b[43mget_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_thread_get_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpack_exception\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpack_exception\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;66;03m# Cleanup pools associated to dead threads\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pools_lock:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/dask/local.py:511\u001b[0m, in \u001b[0;36mget_async\u001b[0;34m(submit, num_workers, dsk, result, cache, get_id, rerun_exceptions_locally, pack_exception, raise_exception, callbacks, dumps, loads, chunksize, **kwargs)\u001b[0m\n\u001b[1;32m    509\u001b[0m         _execute_task(task, data)  \u001b[38;5;66;03m# Re-execute locally\u001b[39;00m\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         \u001b[43mraise_exception\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    512\u001b[0m res, worker_id \u001b[38;5;241m=\u001b[39m loads(res_info)\n\u001b[1;32m    513\u001b[0m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcache\u001b[39m\u001b[38;5;124m\"\u001b[39m][key] \u001b[38;5;241m=\u001b[39m res\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/dask/local.py:319\u001b[0m, in \u001b[0;36mreraise\u001b[0;34m(exc, tb)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exc\u001b[38;5;241m.\u001b[39m__traceback__ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tb:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mwith_traceback(tb)\n\u001b[0;32m--> 319\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/dask/local.py:224\u001b[0m, in \u001b[0;36mexecute_task\u001b[0;34m(key, task_info, dumps, loads, get_id, pack_exception)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    223\u001b[0m     task, data \u001b[38;5;241m=\u001b[39m loads(task_info)\n\u001b[0;32m--> 224\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_execute_task\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;28mid\u001b[39m \u001b[38;5;241m=\u001b[39m get_id()\n\u001b[1;32m    226\u001b[0m     result \u001b[38;5;241m=\u001b[39m dumps((result, \u001b[38;5;28mid\u001b[39m))\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/dask/core.py:119\u001b[0m, in \u001b[0;36m_execute_task\u001b[0;34m(arg, cache, dsk)\u001b[0m\n\u001b[1;32m    115\u001b[0m     func, args \u001b[38;5;241m=\u001b[39m arg[\u001b[38;5;241m0\u001b[39m], arg[\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# Note: Don't assign the subtask results to a variable. numpy detects\u001b[39;00m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;66;03m# temporaries by their reference count and can execute certain\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;66;03m# operations in-place.\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_execute_task\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ishashable(arg):\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arg\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/dask/optimization.py:990\u001b[0m, in \u001b[0;36mSubgraphCallable.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    988\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minkeys):\n\u001b[1;32m    989\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m args, got \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minkeys), \u001b[38;5;28mlen\u001b[39m(args)))\n\u001b[0;32m--> 990\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdsk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/dask/core.py:149\u001b[0m, in \u001b[0;36mget\u001b[0;34m(dsk, out, cache)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m toposort(dsk):\n\u001b[1;32m    148\u001b[0m     task \u001b[38;5;241m=\u001b[39m dsk[key]\n\u001b[0;32m--> 149\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_execute_task\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m     cache[key] \u001b[38;5;241m=\u001b[39m result\n\u001b[1;32m    151\u001b[0m result \u001b[38;5;241m=\u001b[39m _execute_task(out, cache)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/dask/core.py:119\u001b[0m, in \u001b[0;36m_execute_task\u001b[0;34m(arg, cache, dsk)\u001b[0m\n\u001b[1;32m    115\u001b[0m     func, args \u001b[38;5;241m=\u001b[39m arg[\u001b[38;5;241m0\u001b[39m], arg[\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# Note: Don't assign the subtask results to a variable. numpy detects\u001b[39;00m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;66;03m# temporaries by their reference count and can execute certain\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;66;03m# operations in-place.\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_execute_task\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ishashable(arg):\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arg\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/dask/core.py:119\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    115\u001b[0m     func, args \u001b[38;5;241m=\u001b[39m arg[\u001b[38;5;241m0\u001b[39m], arg[\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# Note: Don't assign the subtask results to a variable. numpy detects\u001b[39;00m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;66;03m# temporaries by their reference count and can execute certain\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;66;03m# operations in-place.\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m(\u001b[43m_execute_task\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m args))\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ishashable(arg):\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arg\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/dask/core.py:119\u001b[0m, in \u001b[0;36m_execute_task\u001b[0;34m(arg, cache, dsk)\u001b[0m\n\u001b[1;32m    115\u001b[0m     func, args \u001b[38;5;241m=\u001b[39m arg[\u001b[38;5;241m0\u001b[39m], arg[\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# Note: Don't assign the subtask results to a variable. numpy detects\u001b[39;00m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;66;03m# temporaries by their reference count and can execute certain\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;66;03m# operations in-place.\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_execute_task\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ishashable(arg):\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arg\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/dask/utils.py:71\u001b[0m, in \u001b[0;36mapply\u001b[0;34m(func, args, kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;124;03m\"\"\"Apply a function given its positional and keyword arguments.\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \n\u001b[1;32m     42\u001b[0m \u001b[38;5;124;03mEquivalent to ``func(*args, **kwargs)``\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;124;03m>>> dsk = {'task-name': task}  # adds the task to a low level Dask task graph\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs:\n\u001b[0;32m---> 71\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/dask/dataframe/core.py:6650\u001b[0m, in \u001b[0;36mapply_and_enforce\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   6648\u001b[0m func \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_func\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6649\u001b[0m meta \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_meta\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 6650\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6651\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_dataframe_like(df) \u001b[38;5;129;01mor\u001b[39;00m is_series_like(df) \u001b[38;5;129;01mor\u001b[39;00m is_index_like(df):\n\u001b[1;32m   6652\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(df):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/dask/utils.py:1100\u001b[0m, in \u001b[0;36mmethodcaller.__call__\u001b[0;34m(self, _methodcaller__obj, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, __obj, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m-> 1100\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m__obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/core/series.py:4357\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4247\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4248\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4249\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4252\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4253\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m FrameOrSeriesUnion:\n\u001b[1;32m   4254\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4255\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4256\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4355\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4356\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4357\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/core/apply.py:1043\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1039\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m   1040\u001b[0m     \u001b[38;5;66;03m# if we are a string, try to dispatch\u001b[39;00m\n\u001b[1;32m   1041\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[0;32m-> 1043\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/core/apply.py:1098\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1092\u001b[0m         values \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m_values\n\u001b[1;32m   1093\u001b[0m         \u001b[38;5;66;03m# error: Argument 2 to \"map_infer\" has incompatible type\u001b[39;00m\n\u001b[1;32m   1094\u001b[0m         \u001b[38;5;66;03m# \"Union[Callable[..., Any], str, List[Union[Callable[..., Any], str]],\u001b[39;00m\n\u001b[1;32m   1095\u001b[0m         \u001b[38;5;66;03m# Dict[Hashable, Union[Union[Callable[..., Any], str],\u001b[39;00m\n\u001b[1;32m   1096\u001b[0m         \u001b[38;5;66;03m# List[Union[Callable[..., Any], str]]]]]\"; expected\u001b[39;00m\n\u001b[1;32m   1097\u001b[0m         \u001b[38;5;66;03m# \"Callable[[Any], Any]\"\u001b[39;00m\n\u001b[0;32m-> 1098\u001b[0m         mapped \u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1099\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1100\u001b[0m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1101\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1102\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1105\u001b[0m     \u001b[38;5;66;03m# GH 25959 use pd.array instead of tolist\u001b[39;00m\n\u001b[1;32m   1106\u001b[0m     \u001b[38;5;66;03m# so extension arrays can be used\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(pd_array(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/_libs/lib.pyx:2859\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn [63], line 25\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     13\u001b[0m    \u001b[38;5;28;01mreturn\u001b[39;00m l_str4\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#  try:\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m#     l_str1 = str(i_arg1)\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m#     l_str2 = l_str1[0][1]\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# df_genres2[\"genres\"] = df_genres2.genres.apply(lambda x: aaa(x), meta(\"genres\", \"tuple\") )\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m df_genres2[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenres\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m df_genres2\u001b[38;5;241m.\u001b[39mgenres\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43maaa\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m, meta\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenres\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m) )\n\u001b[1;32m     28\u001b[0m l_cntr \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     29\u001b[0m    \u001b[38;5;66;03m#\u001b[39;00m\n",
      "Cell \u001b[0;32mIn [63], line 5\u001b[0m, in \u001b[0;36maaa\u001b[0;34m(i_arg1)\u001b[0m\n\u001b[1;32m      3\u001b[0m l_str1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(i_arg1)                                                  \u001b[38;5;66;03m#  Needed this, was getting odd  json.loads()  errors otherwise\u001b[39;00m\n\u001b[1;32m      4\u001b[0m l_str2 \u001b[38;5;241m=\u001b[39m l_str1\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m l_str3 \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43ml_str2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# l_str2 = i_arg1[0]\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# l_str2 = l_str1.replace(\"'\", \"\\\"\")\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# l_str3 = json.loads(str2)\u001b[39;00m\n\u001b[1;32m     11\u001b[0m l_str4 \u001b[38;5;241m=\u001b[39m l_str2\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/json/__init__.py:357\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m kw[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    355\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    356\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 357\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    359\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w\u001b[38;5;241m=\u001b[39mWHITESPACE\u001b[38;5;241m.\u001b[39mmatch):\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03m    containing a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m     end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/json/decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    353\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscan_once(s, idx)\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m--> 355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def aaa(i_arg1):\n",
    "    \n",
    "   l_str1 = str(i_arg1)                                                  #  Needed this, was getting odd  json.loads()  errors otherwise\n",
    "   l_str2 = l_str1.replace(\"'\", \"\\\"\")\n",
    "   l_str3 = json.loads(l_str2)\n",
    "\n",
    "\n",
    "   # l_str2 = i_arg1[0]\n",
    "   # l_str2 = l_str1.replace(\"'\", \"\\\"\")\n",
    "   # l_str3 = json.loads(str2)\n",
    "   l_str4 = l_str2\n",
    "      #\n",
    "   return l_str4\n",
    "\n",
    "#  try:\n",
    "#     l_str1 = str(i_arg1)\n",
    "#     l_str2 = l_str1[0][1]\n",
    "#     l_return = l_str2\n",
    "#  except:\n",
    "#     l_return = \"Unknown2\"\n",
    "#  return l_return\n",
    "\n",
    "    \n",
    "# df_genres2[\"genres\"] = df_genres2.genres.apply(lambda x: aaa(x), meta(\"genres\", \"tuple\") )\n",
    "df_genres2[\"genres\"] = df_genres2.genres.apply(lambda x: aaa(x), meta=(\"genres\", \"object\") )\n",
    "\n",
    " \n",
    "l_cntr = 0\n",
    "   #\n",
    "for l_each in df_genres2.iterrows():\n",
    "   l_cntr += 1\n",
    "      #\n",
    "   if (l_cntr < 3):\n",
    "      print(type(l_each))\n",
    "      print(     l_each )\n",
    "      print(\"\")\n",
    "         #\n",
    "      # print(type(l_each[1][0][\"name\"]))\n",
    "      # print(     l_each[1][0][\"name\"] )\n",
    "\n",
    "        \n",
    "#  After:  .apply(func, meta=('genres', 'object'))\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077172a1-b15b-4d43-b7f0-e70af04ac58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267771f4-fc58-4d37-9a90-095e38d749de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f93e39c-5cba-444f-8cb9-969308215eb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fadf6da-3e86-4d23-89a2-8a7b65b2a661",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c206abe-b7c9-4f24-9c38-bd59fde31526",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159bca8a-48d1-4063-8f8f-2633be1b5be0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ffcbf7-0857-42e2-9a74-2c0c0e355728",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf181b77-904a-4e79-a2b8-da70f48bc4e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750b6557-384c-4dcc-8eff-117f9345c5aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e94ece-8ec6-4e57-adf5-fb82b42e7ad0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6ec14ebd-74bc-40ac-9259-d0c936a2df86",
   "metadata": {},
   "source": [
    "#  Building the DataFrames that will go into the graph .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1aa5d25-c0d8-4a73-bf59-02207b28f659",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  At this point we have one DataFrame with 34,000 rows. A flat model, it has\n",
    "#  the following features,\n",
    "#\n",
    "#     release_year\n",
    "#     title\n",
    "#     country\n",
    "#     director\n",
    "#     cast\n",
    "#     genre\n",
    "#     wiki_url\n",
    "#     plot\n",
    "#\n",
    "#  We will create two graphs from the DataFrame above; \n",
    "#\n",
    "#     .  A standard graph, as you might expect,\n",
    "#           (Genre) - [IN] - (Movies) - [ACTED_IN, DIRECTED] - (Person)\n",
    "#              and\n",
    "#           (Movies) - [FROM] - (Countries)\n",
    "#\n",
    "#     .  And a second graph shaped specifically to service our GNN routine\n",
    "#        for \"node property prediction\"\n",
    "#\n",
    "#        ..  We'll make \"Genre\" a property of the node, Movie\n",
    "#        ..  We'll withhold 10 or so Movies for testing\n",
    "#        ..  We'll simplify the Movie relationship to Person\n",
    "#\n",
    "#  We will wind up with several DataFrames; one for each node and edge above.\n",
    "#  We will organize the activities below into (graph-1) and )graph-2).\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcba0c9b-c187-4cfa-827f-11f112c85cb8",
   "metadata": {},
   "source": [
    "#  Our standard graph, Nodes/Edges .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7b0c91-fde8-4fc0-ac76-9bcba213c989",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Our standard graph, Nodes\n",
    "\n",
    "df_movies    = df_data[[\"release_year\", \"title\", \"wiki_url\", \"plot\"]].compute()\n",
    "print(\"Movies...... \" + str(len(df_movies.index)))\n",
    "   #\n",
    "df_genres    = df_data[[\"genre\"  ]].drop_duplicates().compute()\n",
    "print(\"Genres...... \" + str(len(df_genres.index)))\n",
    "df_countries = df_data[[\"country\"]].drop_duplicates().compute()\n",
    "print(\"Countries... \" + str(len(df_countries.index)))\n",
    "   #\n",
    "df_persons   = df_data[[\"director\"]].drop_duplicates().compute()\n",
    "print(\"Persons..... \" + str(len(df_persons.index)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#  Our standard graph, Edges\n",
    "\n",
    "\n",
    "print(\"--\")\n",
    "\n",
    "#  Sample output\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827bfb01-0b17-48c0-8a63-05bc49f6a0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Genres looks odd at 2000+ unique entries. Look deeper at that-\n",
    "#\n",
    "#  We'll write df_genres to a file local to the Jupyter Docker container, so we can view it in an editor\n",
    "\n",
    "df_genres.to_csv(\"02_Files/40_genres.txt\", index=None, sep=\"|\")\n",
    "\n",
    "print(\"--\")\n",
    "\n",
    "#  Sample output\n",
    "#\n",
    "#  genre\n",
    "#  unknown\n",
    "#  western\n",
    "#  comedy\n",
    "#  short\n",
    "#  short action/crime western\n",
    "#  short film\n",
    "#  biographical\n",
    "#  drama\n",
    "#  adventure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6037f035-2a6a-4c42-af32-a3e9730b834a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a38bd3-1c7e-46ba-9e2c-13b784052efa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05070e73-8c0a-4fef-8bd8-c60e83b22b78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831e6482-1186-4a12-91cc-91580fded193",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27725cb6-5404-4132-95e4-f26ea493e899",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a009883-fadd-4dfe-9f83-efd5e4ef6e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from katana import remote\n",
    "from katana.remote import import_data\n",
    "\n",
    "my_client = remote.Client()\n",
    "\n",
    "print(my_client)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddc3b51-a38f-406d-b4cb-4bcd13c5f327",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "NUM_PARTITIONS  = 3\n",
    "   #\n",
    "DB_NAME         = \"my_db\"\n",
    "GRAPH_NAME      = \"my_graph\"\n",
    "\n",
    "print(\"--\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96e9e29-c90b-4ed6-9997-db56ff225fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##################################################################\n",
    "#\n",
    "#  This section; basic graph and database setup, reset for test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cfb83b-fd1f-46c3-96ad-95ceed14058f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  DELETE ALL GRAPHS\n",
    "\n",
    "for l_database in my_client.databases():\n",
    "   for l_graph in my_client.get_database(name=l_database.name).graphs_in_database():\n",
    "      l_handle=my_client.get_database(name=l_database.name).get_graph_by_id(id=l_graph.graph_id)\n",
    "      l_handle.delete()\n",
    "\n",
    "for l_graph in my_client.graphs():\n",
    "   print(\"GRAPH ID: \", l_graph.graph_id, \"      GRAPH Version: \", l_graph.version)\n",
    "\n",
    "print(\"--\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04909ce0-17bc-4214-84a7-3016cdbf52c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  DELETE ALL DATABASES\n",
    "\n",
    "for l_database in my_client.databases():\n",
    "   if (l_database.name != \"default\"):\n",
    "      my_client.get_database(name=l_database.name).delete_database()\n",
    "      print(\"--\")\n",
    "\n",
    "for l_database in my_client.databases():\n",
    "   print(\"DB ID: \", l_database.database_id, \"     DB Name: \", l_database.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb24521-9a8d-4fad-bdec-52876bc71e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  CREATE DATABASE\n",
    "\n",
    "my_database = my_client.create_database(name=DB_NAME)\n",
    "\n",
    "print(my_database.database_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf6371d-6d53-4052-b895-f61c76b0792a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  CREATE A GRAPH\n",
    "\n",
    "my_graph=my_client.get_database(name=DB_NAME).create_graph(name=GRAPH_NAME, num_partitions=NUM_PARTITIONS)\n",
    "\n",
    "print(my_graph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29f6eb7-59ef-4c9b-813a-e8370b25b7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  CONNECT TO GRAPH\n",
    "\n",
    "for l_graph in my_client.get_database(name=DB_NAME).graphs_in_database():\n",
    "   if (l_graph.name == GRAPH_NAME):\n",
    "      my_graph=my_client.get_database(name=DB_NAME).get_graph_by_id(id=l_graph.graph_id)\n",
    "         #\n",
    "      break\n",
    "\n",
    "# my_graph, *_ = my_client.get_database(name=DB_NAME).find_graphs_by_name(GRAPH_NAME)\n",
    "\n",
    "print(my_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bedb2fa-cec7-474a-acc2-5cefbbb3660e",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(my_graph.num_nodes())\n",
    "display(my_graph.num_edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "badb5a2d-bddb-4bba-9169-e5dfc1d44147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##################################################################\n",
    "#\n",
    "#  Load from source CSV, in this case we are using the Neo4J Movie graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e273ddce-66b8-43a8-b551-3a51a6983449",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import numpy as np\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1259d64d-c879-4e1f-ab10-6cf663b293e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Load a DataFrame from CSV, Nodes/Vertices\n",
    "\n",
    "l_InputFile  = \"./10_NMovieDB/24_nodes.txt\"\n",
    "\n",
    "df_all_nodes1 = dd.read_csv(\n",
    "   l_InputFile,\n",
    "   delimiter = \",\",\n",
    "   dtype = {\n",
    "      # \"id\"        : int,\n",
    "      \"id\"        : np.dtype('O'),\n",
    "      \"_labels\"   : np.dtype('O'),\n",
    "      # \"born\"      : float, \n",
    "      \"born\"      : np.dtype('O'),\n",
    "      \"name\"      : np.dtype('O'),\n",
    "      # \"released\"  : float,\n",
    "      \"released\"  : np.dtype('O'),\n",
    "      \"tagline\"   : np.dtype('O'),\n",
    "      \"title\"     : np.dtype('O')\n",
    "      })\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91245c1-9a0f-455b-b9c1-f975018d98f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Number of rows, two different ways\n",
    "\n",
    "display(len(df_all_nodes1))\n",
    "display(print(\"{} Rows\".format(df_all_nodes1.shape[0].compute())))\n",
    "\n",
    "#  Other output\n",
    "\n",
    "display(df_all_nodes1.head(10))\n",
    "display(df_all_nodes1[[\"born\", \"name\"]].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d07d63f-9c74-43e8-ba54-cdbd6099e8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Print all rows, subsetted columns\n",
    "\n",
    "for l_each in df_all_nodes1.iterrows():\n",
    "   print(l_each[0], \"   \", l_each[1][\"_labels\"], \"   \", l_each[1][\"name\"])  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9490285c-abad-4c3b-804d-2534515c95fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##################################################################\n",
    "#\n",
    "#   https://www.askpython.com/python/examples/subset-a-dataframe\n",
    "#   https://www.codegrepper.com/code-examples/python/convert+float+to+int+python+pandas\n",
    "#   https://docs.dask.org/en/latest/generated/dask.dataframe.DataFrame.assign.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77dc5f0-3ce2-4c47-8db4-0b7438277f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  We have some issues we want to change with our data\n",
    "#\n",
    "#  .  Some of the property names have a leading underscore. Change those.\n",
    "#  .  Some values which should be integer, are float.\n",
    "#  .  The label values are currently \";Person\" and \";Movie\". Let's remove those semicolons.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfbf635-c973-4c1d-91d4-36bc47eb5cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Column renames\n",
    "\n",
    "df_all_nodes2 = df_all_nodes1.rename(columns={\"_id\": \"id\", \"_labels\": \"label\"})\n",
    "\n",
    "print(\"--\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc357f2-0a17-42c9-ab81-3d984a1604b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Change float values to integer, remove leading semicolon from label\n",
    "\n",
    "df_all_nodes3 = df_all_nodes2.assign(\n",
    "   # born     = lambda x: x.born.fillna(0.0).astype(int), \n",
    "   born     = lambda x: x.born.fillna(0.0).astype(str), \n",
    "   # id       = lambda x: x.id.fillna(0.0).astype(int),\n",
    "   id       = lambda x: x.id.fillna(0.0).astype(str),\n",
    "   # released = lambda x: x.released.fillna(0.0).astype(int),\n",
    "   released = lambda x: x.released.fillna(0.0).astype(str),\n",
    "   label    = lambda x: x.label.astype(str).str[1:]\n",
    "   )\n",
    "\n",
    "print(\"--\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6485b323-6335-4dd2-8d63-dfbf7e845f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(len(df_all_nodes3))\n",
    "display(df_all_nodes3.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f520e942-6083-4ddc-a55d-104bd0cf5280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83330545-9507-4cef-831a-bb1ea802029f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Now, populate Persons and Movies\n",
    "\n",
    "df_persons = df_all_nodes3[df_all_nodes3[\"label\"] == \"Person\"][[\"id\", \"label\", \"born\", \"name\"]]\n",
    "\n",
    "df_movies  = df_all_nodes3[df_all_nodes3[\"label\"] == \"Movie\"][[\"id\", \"label\", \"released\", \"tagline\", \"title\"]]\n",
    "\n",
    "print(\"--\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df25c958-2366-45e6-b316-6b30198858d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "display(len(df_persons))\n",
    "display(df_persons.head(10))\n",
    "display(len(df_movies))\n",
    "display(df_movies.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352ec1d4-2f6a-42d7-b026-6d4db9979aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##################################################################\n",
    "#\n",
    "#  Repeat the style of work from above, but now for edges\n",
    "#\n",
    "#  .  Some of the property names have a leading underscore. Change those.\n",
    "#  .  Some values which should be integer, are float.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26bb9803-9e7b-4471-83ad-d279bb733d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Load a DataFrame from CSV, Edges\n",
    "\n",
    "l_InputFile  = \"./10_NMovieDB/25_edges.txt\"\n",
    "\n",
    "df_all_edges1 = dd.read_csv(\n",
    "   l_InputFile,\n",
    "   delimiter = \",\",\n",
    "   dtype = {\n",
    "      # \"_start\"    : float,\n",
    "      \"_start\"    : np.dtype('O'),\n",
    "      # \"_end\"      : float, \n",
    "      \"_end\"      : np.dtype('O'),\n",
    "      \"_type\"     : np.dtype('O'),\n",
    "      # \"rating\"    : float,\n",
    "      \"rating\"    : np.dtype('O'),\n",
    "      \"roles\"     : np.dtype('O'),\n",
    "      \"summary\"   : np.dtype('O')\n",
    "      })\n",
    "\n",
    "print(\"--\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fde9ff-5304-4d2c-b351-84fe21d97f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(len(df_all_edges1))\n",
    "display(df_all_edges1.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9356971-a6d7-4cba-ae07-cc0d693e39f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_edges2 = df_all_edges1.rename(columns={\"_start\": \"START_ID\", \"_end\": \"END_ID\", \"_type\": \"TYPE\",\n",
    "   \"rating\": \"RATING\", \"roles\": \"ROLES\", \"summary\": \"SUMMARY\"})\n",
    "\n",
    "df_all_edges3 = df_all_edges2.assign(\n",
    "   # START_ID = lambda x: x.START_ID.fillna(0.0).astype(int), \n",
    "   START_ID = lambda x: x.START_ID.fillna(0.0).astype(str), \n",
    "   # END_ID   = lambda x: x.END_ID.fillna(0.0).astype(int),\n",
    "   END_ID   = lambda x: x.END_ID.fillna(0.0).astype(str),\n",
    "   # RATING   = lambda x: x.RATING.fillna(0.0).astype(int)\n",
    "   RATING   = lambda x: x.RATING.fillna(0.0).astype(str)\n",
    "   )\n",
    "\n",
    "print(\"--\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddde16b-d104-4e12-aac2-40d6598ffcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(len(df_all_edges3))\n",
    "display(df_all_edges3.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137a838f-d161-4898-95ca-80f608fa82a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Split out the various edges\n",
    "\n",
    "df_reviewed = df_all_edges3[df_all_edges3[\"TYPE\"] == \"REVIEWED\"][[\"START_ID\", \"END_ID\", \"TYPE\", \"RATING\", \"SUMMARY\"]]\n",
    "\n",
    "df_wrote    = df_all_edges3[df_all_edges3[\"TYPE\"] == \"WROTE\"   ][[\"START_ID\", \"END_ID\", \"TYPE\"]]\n",
    "\n",
    "df_produced = df_all_edges3[df_all_edges3[\"TYPE\"] == \"PRODUCED\"][[\"START_ID\", \"END_ID\", \"TYPE\"]]\n",
    "\n",
    "df_directed = df_all_edges3[df_all_edges3[\"TYPE\"] == \"DIRECTED\"][[\"START_ID\", \"END_ID\", \"TYPE\"]]\n",
    "\n",
    "df_follows  = df_all_edges3[df_all_edges3[\"TYPE\"] == \"FOLLOWS\" ][[\"START_ID\", \"END_ID\", \"TYPE\"]]\n",
    "\n",
    "#  \"roles\" is a string similar to  '[ \"a\", \"b\", \"c\" ]'\n",
    "#\n",
    "#  This was automatically coming in as a list-\n",
    "#  Cool\n",
    "\n",
    "df_actedin  = df_all_edges3[df_all_edges3[\"TYPE\"] == \"ACTED_IN\"][[\"START_ID\", \"END_ID\", \"TYPE\", \"ROLES\"]]\n",
    "\n",
    "print(\"--\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950f1fa7-3ac5-4eb9-a749-6f3d89267d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "display(len(df_reviewed))\n",
    "display(df_reviewed.head(2))\n",
    "\n",
    "display(len(df_wrote))\n",
    "display(df_wrote.head(2))\n",
    "\n",
    "display(len(df_produced))\n",
    "display(df_produced.head(2))\n",
    "\n",
    "display(len(df_directed))\n",
    "display(df_directed.head(2))\n",
    "\n",
    "display(len(df_follows))\n",
    "display(df_follows.head(2))\n",
    "\n",
    "display(len(df_actedin))\n",
    "display(df_actedin.head(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e959b0-0040-4556-a486-8267ae32c01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a47455c-f349-4c06-be7f-214e996a33d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Didn't need this; also don't know if it had any effect\n",
    "\n",
    "# from dask.distributed import Client\n",
    "# client = Client(n_workers=4, threads_per_worker=2)\n",
    "\n",
    "# print(\"--\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba15117-b555-4953-8580-38e4222f4864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##################################################################\n",
    "#\n",
    "#  Finally now, load the vertices/nodes into a graph\n",
    "#\n",
    "#  Some hinkiness we need to work around ..\n",
    "#\n",
    "#     .  The Dask DataFrames here were loaded from CSV, and those CSV\n",
    "#        files were found, in scope.\n",
    "#        The KG DataFrame importer will reference that same file\n",
    "#        pathname, and the file will not be in scope. Basically,\n",
    "#        it was expected that these files be on S3/GS all along.\n",
    "#        I hate to have that dependency because, just one more thing\n",
    "#        to have to manage.\n",
    "#\n",
    "#     .  So, we'll copy the DataFrames to Dask arrays, then back into\n",
    "#        a Dask DataFrame.\n",
    "#        Why not just copy the DaskDataFrame ?  Currently there is only \n",
    "#        shallow copies of DataFrames.\n",
    "#\n",
    "#  See,\n",
    "#     https://stackoverflow.com/questions/52119342/how-do-i-convert-a-dask-dataframe-into-a-dask-array\n",
    "#     https://docs.dask.org/en/latest/generated/dask.dataframe.from_dask_array.html\n",
    "\n",
    "\n",
    "da_persons    = df_persons.to_dask_array()\n",
    "da_movies     = df_movies.to_dask_array()\n",
    "   #\n",
    "da_directed   = df_directed.to_dask_array()\n",
    "da_reviewed   = df_reviewed.to_dask_array()\n",
    "da_wrote      = df_wrote.to_dask_array()\n",
    "da_produced   = df_produced.to_dask_array()\n",
    "da_follows    = df_follows.to_dask_array()\n",
    "da_actedin    = df_actedin.to_dask_array()\n",
    "\n",
    "\n",
    "df_persons2   = dd.io.from_dask_array(da_persons,  columns=[\"id\", \"label\", \"born\", \"name\"]).compute()\n",
    "df_movies2    = dd.io.from_dask_array(da_movies,   columns=[\"id\", \"label\", \"released\", \"tagline\", \"title\"]).compute()\n",
    "   #\n",
    "df_directed2  = dd.io.from_dask_array(da_directed, columns=[\"START_ID\", \"END_ID\", \"TYPE\"]).compute()\n",
    "df_reviewed2  = dd.io.from_dask_array(da_reviewed, columns=[\"START_ID\", \"END_ID\", \"TYPE\", \"RATING\", \"SUMMARY\"]).compute()\n",
    "df_wrote2     = dd.io.from_dask_array(da_wrote, columns=[\"START_ID\", \"END_ID\", \"TYPE\"]).compute()\n",
    "df_produced2  = dd.io.from_dask_array(da_produced, columns=[\"START_ID\", \"END_ID\", \"TYPE\"]).compute()\n",
    "df_follows2   = dd.io.from_dask_array(da_follows, columns=[\"START_ID\", \"END_ID\", \"TYPE\"]).compute()\n",
    "df_actedin2   = dd.io.from_dask_array(da_actedin, columns=[\"START_ID\", \"END_ID\", \"TYPE\", \"ROLES\"]).compute()\n",
    "\n",
    "print(\"--\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0060507c-c49f-4cda-92c0-a53752091fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from katana_enterprise.remote import import_data\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7737ce45-e17f-49e4-ac98-af1dc3bf8948",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with import_data.DataFrameImporter(my_graph) as df_importer:\n",
    "    \n",
    "   # Person\n",
    "   #\n",
    "   df_importer.nodes_dataframe(\n",
    "      df_persons2[[\"id\", \"label\", \"born\", \"name\"]],\n",
    "      id_column  = \"id\",\n",
    "      id_space   = \"Person\"\n",
    "      )\n",
    "   #  Movie\n",
    "   #\n",
    "   df_importer.nodes_dataframe(\n",
    "      df_movies2[[\"id\", \"label\", \"title\", \"tagline\"]],\n",
    "      id_column  = \"id\",\n",
    "      id_space   = \"Movie\"\n",
    "      )  \n",
    "    \n",
    "   #  DIRECTED\n",
    "   #\n",
    "   df_importer.edges_dataframe(\n",
    "      df_directed2[[\"START_ID\", \"END_ID\", \"TYPE\"]],\n",
    "      source_id_space      = \"Person\",\n",
    "      destination_id_space = \"Movie\",\n",
    "      source_column        = \"START_ID\",\n",
    "      destination_column   = \"END_ID\",\n",
    "      type                 = \"DIRECTED\"\n",
    "      )\n",
    "   #  REVIEWED\n",
    "   #\n",
    "   df_importer.edges_dataframe(\n",
    "      df_reviewed2[[\"START_ID\", \"END_ID\", \"TYPE\", \"RATING\", \"SUMMARY\"]],\n",
    "      source_id_space      = \"Person\",\n",
    "      destination_id_space = \"Movie\",\n",
    "      source_column        = \"START_ID\",\n",
    "      destination_column   = \"END_ID\",\n",
    "      type                 = \"REVIEWED\"\n",
    "      )\n",
    "   #  WROTE\n",
    "   #\n",
    "   df_importer.edges_dataframe(\n",
    "      df_wrote2[[\"START_ID\", \"END_ID\", \"TYPE\"]],\n",
    "      source_id_space      = \"Person\",\n",
    "      destination_id_space = \"Movie\",\n",
    "      source_column        = \"START_ID\",\n",
    "      destination_column   = \"END_ID\",\n",
    "      type                 = \"WROTE\"\n",
    "      )\n",
    "   #  PRODUCED\n",
    "   #\n",
    "   df_importer.edges_dataframe(\n",
    "      df_produced2[[\"START_ID\", \"END_ID\", \"TYPE\"]],\n",
    "      source_id_space      = \"Person\",\n",
    "      destination_id_space = \"Movie\",\n",
    "      source_column        = \"START_ID\",\n",
    "      destination_column   = \"END_ID\",\n",
    "      type                 = \"PRODUCED\"\n",
    "      )\n",
    "   #  FOLLOWS\n",
    "   #\n",
    "   df_importer.edges_dataframe(\n",
    "      df_follows2[[\"START_ID\", \"END_ID\", \"TYPE\"]],\n",
    "      source_id_space      = \"Person\",\n",
    "      destination_id_space = \"Movie\",\n",
    "      source_column        = \"START_ID\",\n",
    "      destination_column   = \"END_ID\",\n",
    "      type                 = \"FOLLOWS\"\n",
    "      )\n",
    "   #  ACTEDIN\n",
    "   #\n",
    "   df_importer.edges_dataframe(\n",
    "      df_actedin2[[\"START_ID\", \"END_ID\", \"TYPE\", \"ROLES\"]],\n",
    "      source_id_space      = \"Person\",\n",
    "      destination_id_space = \"Movie\",\n",
    "      source_column        = \"START_ID\",\n",
    "      destination_column   = \"END_ID\",\n",
    "      type                 = \"ACTEDIN\"\n",
    "      )\n",
    "\n",
    "   df_importer.execute()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1d9a40-4d2e-4972-9113-3084d2258d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "l_result1 = my_graph.query(\"\"\"\n",
    "\n",
    "   MATCH (a) \n",
    "   WITH DISTINCT LABELS(a) AS temp, COUNT(a) AS tempCnt\n",
    "   UNWIND temp AS label\n",
    "   RETURN label, SUM(tempCnt) AS cnt\n",
    "   ORDER BY label\n",
    "   \n",
    "   \"\"\")\n",
    "\n",
    "display(print(l_result1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ef49e4-30e4-488c-963b-bf13c44e12f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "l_result1 = my_graph.query(\"\"\"\n",
    "\n",
    "   MATCH (m)-[r]->(n) \n",
    "   WITH DISTINCT TYPE(r) AS temp, COUNT(r) AS tempCnt\n",
    "   RETURN temp, tempCnt\n",
    "   ORDER BY temp\n",
    "\n",
    "   \"\"\")\n",
    "\n",
    "display(print(l_result1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da550017-2685-48ed-994b-5a51395ea189",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "result = my_graph.query(\"\"\"\n",
    "\n",
    "   MATCH (x) -[r]-> (a)\n",
    "   RETURN x, r AS rel, a\n",
    "   \n",
    "   \"\"\",\n",
    "   contextualize=True)\n",
    "\n",
    "result.view()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbfb62f6-0d3f-4d22-924f-a2630e3405a7",
   "metadata": {},
   "source": [
    "# Output a graph as a a local file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f800b8bb-11ec-4904-9217-51e56a5b9b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Formatting could use a little work, but the concept is here ..\n",
    "\n",
    "l_result1 = my_graph.query(\"\"\"\n",
    "   MATCH (n: Person) \n",
    "   RETURN n\n",
    "   \"\"\")\n",
    "      #\n",
    "# display(print(l_result1))\n",
    "\n",
    "l_result2 = my_graph.query(\"\"\"\n",
    "   MATCH (n) - [r: WROTE] -> (m)\n",
    "   RETURN r\n",
    "   \"\"\")\n",
    "      #\n",
    "# display(print(l_result2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd62f55-1b27-43f1-bdc4-e1b79de25609",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "l_nodes = []\n",
    "   #\n",
    "for l_node in l_result1.iterrows():\n",
    "   l_nodes.append(l_node)\n",
    "\n",
    "l_file = open(\"nodes.txt\", \"w\")\n",
    "l_file.write(str(l_nodes))\n",
    "l_file.close()\n",
    "\n",
    "\n",
    "l_edges = []\n",
    "   #\n",
    "for l_edge in l_result2.iterrows():\n",
    "   l_edges.append(l_edge)\n",
    "\n",
    "l_file = open(\"edges.txt\", \"w\")\n",
    "l_file.write(str(l_edges))\n",
    "l_file.close()\n",
    "\n",
    "\n",
    "print(\"--\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
