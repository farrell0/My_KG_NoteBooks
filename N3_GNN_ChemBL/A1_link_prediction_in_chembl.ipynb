{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b24734a6-bb32-4111-b5ba-ad8f7c253721",
   "metadata": {},
   "source": [
    "# Drug-Target Interaction Prediction in ChEMBL using Distributed GNN based Link Prediction\n",
    "\n",
    "In this notebook, we demonstrate how to use `Katana Graph Platform` to implement a distributed GNN based model for performing drug-target interactions predictions pipeline. We pose this as a link prediction problem. For this demo, we would be using the `chembl29` dataset which consists of roughly 2.1M drug or compound nodes and 15K target or gene nodes. Overall, there are 3.2M connections between drug and target pairs. The input for the GNN model would be the `chembl29` bipartite graph and it would predict the interaction between the corresponding pairs. We pose this as a `classification` problem in this notebook. \n",
    "\n",
    "There are five distinct steps of a ML pipeline.\n",
    "\n",
    "* **[Step 1: Katana Setup and Data Loading.](#step1)** Set up a `Katana Client` and load the data from a collections of csvs into a `rdg`.\n",
    "* **[Step 2: Data Preprocessing.](#step2)** Generate features for drugs and target data and store them into the `rdg`.\n",
    "* **[Step 3: Data Splitting into Train-Val-Test.](#step3)** Partitioing compounds into disjoint sets for developing generalizable models.\n",
    "* **[Step 4: Setting up Components of AI Training Pipeline.](#step4)** Defining GNN models, dataset abstractions, and trainer abstractions.\n",
    "* **[Step 5: Putting Everything Together.](#step5)** Defining a remote function with all the peices together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75664ca-bf4f-487e-9c38-c1a2c4e25c7e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Requirements\n",
    "\n",
    "Before you begin, make sure you meet these prerequisites:\n",
    "\n",
    "* A running [Katana cluster](../../getting-started/index.rst) (cloud or local deployment).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f374bc0-081f-4a1b-8670-2a301688b147",
   "metadata": {},
   "source": [
    "<a id='step1'></a>\n",
    "## Step 1. Katana Setup and Data Loading\n",
    "\n",
    "Once you have a running katana cluster, you would need to initialize a `Client`. This would require setting up the address of the Katana Server in the current environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc22a0d7-935c-4670-8b74-cf0b8ef75021",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"MODIN_ENGINE\"] = \"python\"\n",
    "\n",
    "# Docker container on macOS or Windows\n",
    "# os.environ[\"KATANA_SERVER_ADDRESS\"] = \"host.docker.internal:8080\"\n",
    "\n",
    "# Docker container on Linux\n",
    "# os.environ[\"KATANA_SERVER_ADDRESS\"] = \"localhost:8080\"\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fba0577-5c9b-4f40-aa69-5f816c9ac44a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load Libraries and Initialize the Katana `Client`\n",
    "\n",
    "Starting a Katana remote Client is required to interface with the Katana remote service and schedule distributed operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11dbde7b-8af7-44d0-be70-aa5c6d9a6058",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Connect to the Katana Server\n",
    "from katana import remote\n",
    "from katana.remote import import_data\n",
    "\n",
    "client = remote.Client()\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb60324f-7ff8-4409-a9aa-8c26e83482c6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load data\n",
    "\n",
    "Create a new graph with four partitions and load the example `chembl29` graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0baf72-7db7-4a66-9843-4364416b9d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "#  MMM\n",
    "# g = client.create_graph(num_partitions=4)       \n",
    "g = client.create_graph(num_partitions=3)\n",
    "\n",
    "import_data.csv(\n",
    "    g,\n",
    "    input_node_path=\"gs://katana-demo-datasets/csv-datasets/chembl29/nodes.txt\",\n",
    "    input_edge_path=\"gs://katana-demo-datasets/csv-datasets/chembl29/edges.txt\",\n",
    "    input_dir=\"gs://katana-demo-datasets/csv-datasets/chembl29/\",\n",
    "    data_delimiter=\",\",\n",
    "    schema_delimiter=\",\",\n",
    ")\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891271d2-f101-4df3-a66d-80dcda9b258d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Inspect the Graph Schema and the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547d3ef7-0049-4669-a064-4763b1d6a64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(f\"The graph has {g.num_nodes()} nodes and {g.num_edges()} edges.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c5de16-3578-4a71-84e2-f5e6f0a3f299",
   "metadata": {},
   "source": [
    "<a id='step2'></a>\n",
    "## Step 2: Data Preprocessing: Feature Generation and Stroing\n",
    "\n",
    "There are two types of nodes in the `chembl29` dataset: `compound` and `targets`. For running ML tasks on this dataset, we first need to add numerical features to the nodes on which the ML models will operate. In this section, we show how to add features to specific node types. \n",
    "\n",
    "We provide the `HlsPreprocessingGraph` class to perform preprocessing and save features as a property on the graph's nodes. We use a custom user defined featurizer that randomly assigns a numpy array as a feature. We use this option in the demo as it is extremeley fast.\n",
    "\n",
    "Note: The custom featurizer function used with upsert_featurizer_feature needs to be deterministic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b1eb5f-cd93-4a6f-a745-c5161869bda3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# pylint: disable=C0412 (ungrouped-imports)\n",
    "import deepchem\n",
    "import numpy\n",
    "from katana.ai.hls.hls_preprocessing_graph import HlsPreprocessingGraph\n",
    "from katana.distributed import Graph\n",
    "\n",
    "\n",
    "def remote_preprocessing(graph: Graph):\n",
    "\n",
    "    feat_obj = HlsPreprocessingGraph(graph)\n",
    "\n",
    "    # Random feature\n",
    "    def random_feature_generator(_):\n",
    "        nr_features = 100\n",
    "        rng = numpy.random.default_rng()\n",
    "        return rng.random(size=(nr_features,), dtype=numpy.float32)\n",
    "\n",
    "    def random_smiles_feature_generator(_):\n",
    "        nr_features = 2\n",
    "        rng = numpy.random.default_rng()\n",
    "        return rng.random(size=(nr_features,), dtype=numpy.float32)\n",
    "\n",
    "    feat_obj.upsert_featurizer_feature(\n",
    "        in_feature_name=\"chembl_id\",\n",
    "        out_feature_name=\"random_100\",\n",
    "        featurizer=random_feature_generator,\n",
    "        node_types=[\"compound\", \"target\"],\n",
    "    )\n",
    "\n",
    "    feat_obj.upsert_featurizer_feature(\n",
    "        in_feature_name=\"canonical_smiles\",\n",
    "        out_feature_name=\"random_2\",\n",
    "        featurizer=random_smiles_feature_generator,\n",
    "        node_types=[\"compound\"],\n",
    "    )\n",
    "\n",
    "\n",
    "g.run(remote_preprocessing)\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929c15aa-0fbe-4490-ae50-9671e046af75",
   "metadata": {},
   "source": [
    "#### Adding embedding features required by the sampler/dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256b97c7-15b3-40db-9cc7-96177a774ab3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# pylint: disable=C0412 (ungrouped-imports), W0404 (reimport)\n",
    "from katana_enterprise.ai.preprocessing.preprocessing_graph import PreprocessingGraph\n",
    "from katana_enterprise.distributed import Graph\n",
    "\n",
    "\n",
    "def generate_features(graph):\n",
    "    feature_obj = PreprocessingGraph(graph)\n",
    "    # Node labels, not used but have to be specified for later in the pipeline.\n",
    "    label_data = numpy.zeros((len(graph.nodes()),)).reshape((-1, 1))\n",
    "    feature_obj.upsert_node_feature(feature_name=\"embedding\", feature_data=label_data)\n",
    "\n",
    "\n",
    "g.run(generate_features)\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eaf6e86-512f-4372-8d16-367b53b97750",
   "metadata": {},
   "source": [
    "#### Testing to make sure the features are correctly added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4d4f68-b25a-489e-b14f-95c7e8b300a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# pylint: disable=C0412 (ungrouped-imports), W0404 (reimport)\n",
    "from katana_enterprise.ai.preprocessing.preprocessing_graph import PreprocessingGraph\n",
    "from katana_enterprise.distributed import Graph\n",
    "\n",
    "\n",
    "def test_featurization(graph: Graph):\n",
    "\n",
    "    preproc_graph = PreprocessingGraph(graph)\n",
    "    random_feat = preproc_graph.get_node_feature(feature_name=\"random_100\")\n",
    "    assert random_feat.shape[1] == 100\n",
    "\n",
    "    embed_feat = preproc_graph.get_node_feature(feature_name=\"embedding\")\n",
    "    assert len(embed_feat.shape) == 1\n",
    "\n",
    "\n",
    "g.run(test_featurization)\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266a74eb-380a-4bf2-b1bf-6837875bcd7b",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='step3'></a>\n",
    "## Step 3: Data Splitting into Train-Val-Test\n",
    "\n",
    "In the Drug-Target Inetraction (DTI) prediction tasks, we extract a set of triplets of `(drug, target, label)` from the `rdg` as our input data. In order to test the generalizability of the ML models, we split the input data ensuring that a drug (or compound) is only present is one of the `train`, `validation`, and `test` sets. This would ensure that the trivial predictions (for eg, two very similar drugs will have very similar interactions with the same target) are not counted towards model performance. In this section, we show how to split the data into train-val-test splits, how to save the split information as properties on the compound nodes, and finally how to use cypher queries to extarct ML-ready distributed data.\n",
    "\n",
    "### Partitioing Compounds into Disjoint Sets:\n",
    "To achieve this split, we create a `split_label` property on the drug (compound) nodes. This property can have three values: `0`, `1`, and `2` indicating the drug to be part of the train, validation, and test set respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7fef28-6518-442a-8dc4-1704b74e0bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# pylint: disable=C0412 (ungrouped-imports), W0404 (reimport)\n",
    "import dataclasses\n",
    "from enum import IntEnum\n",
    "\n",
    "from katana_enterprise.ai.preprocessing import RandomSplitter\n",
    "from katana_enterprise.ai.preprocessing.preprocessing_graph import PreprocessingGraph\n",
    "from katana_enterprise.distributed import Graph\n",
    "\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class SplitConfig:\n",
    "    train_frac: float = 0.7\n",
    "    val_frac: float = 0.1\n",
    "    test_frac: float = 0.2\n",
    "\n",
    "\n",
    "class SplitType(IntEnum):\n",
    "    \"\"\"SplitType encodes training, validation, and test data separation\"\"\"\n",
    "\n",
    "    TRAIN = 0\n",
    "    VAL = 1\n",
    "    TEST = 2\n",
    "\n",
    "\n",
    "def split_generator(graph: Graph, split_config: SplitConfig):\n",
    "\n",
    "    feat_obj = PreprocessingGraph(graph)\n",
    "\n",
    "    # Generate training, validation, and test splits\n",
    "    split_arr = feat_obj.generate_split_property(\n",
    "        target_property_name=\"random_2\",\n",
    "        split_encoder=RandomSplitter(\n",
    "            split_ratio=[split_config.train_frac, split_config.val_frac, split_config.test_frac], random_state=42\n",
    "        ),\n",
    "    )\n",
    "    # Commit the splits to the graph\n",
    "    feat_obj.upsert_node_feature(feature_name=\"split_label\", feature_data=split_arr)\n",
    "\n",
    "\n",
    "# Set up the split configuration\n",
    "split_config = SplitConfig(train_frac=0.7, val_frac=0.1, test_frac=0.2)\n",
    "\n",
    "\n",
    "# Updating the split through a remote function\n",
    "g.run(lambda g: split_generator(g, split_config))\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98628150-8580-441e-b23f-202e5446c05f",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Testing the Split Generation Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c5d6bb-c72d-45bf-a298-984c0014d50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import numpy\n",
    "\n",
    "\n",
    "def query_str(split_type: SplitType):\n",
    "\n",
    "    query_str = f\"\"\"MATCH (a:compound)\n",
    "    WHERE a.split_label = {split_type}\n",
    "    RETURN count(a) as count\n",
    "    \"\"\"\n",
    "    return query_str\n",
    "\n",
    "\n",
    "num_compounds = g.query(\"\"\"MATCH (a:compound) Return count(a) as count\"\"\")[\"count\"][0]\n",
    "train_count = g.query(query_str(SplitType.TRAIN))[\"count\"][0]\n",
    "val_count = g.query(query_str(SplitType.VAL))[\"count\"][0]\n",
    "test_count = g.query(query_str(SplitType.TEST))[\"count\"][0]\n",
    "\n",
    "\n",
    "assert train_count + val_count + test_count == num_compounds\n",
    "assert numpy.isclose(split_config.train_frac, train_count / num_compounds)\n",
    "assert numpy.isclose(split_config.val_frac, val_count / num_compounds)\n",
    "assert numpy.isclose(split_config.test_frac, test_count / num_compounds)\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c4158c-70a8-4e07-9573-e8b5394e8634",
   "metadata": {},
   "source": [
    "<a id='step4'></a> \n",
    "## Step 4: Setting up Components of AI Training Pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12367be1-7cba-4162-a090-1fd08ade2add",
   "metadata": {},
   "source": [
    "<a id='query'></a>\n",
    "### Step 4.1 Query to Extract ML-ready Data with Train-Val-Test Split\n",
    "\n",
    "Once we have saved the split labels on the compound data, we can extract a ML-ready dataset consisting of `(compound, target, label)` using cypher query. We will execute this query from the distributed graph which would return a distributed table consisting of triplets. We use a `threshold` on the `pchembl_value` to create a binary classification label on the egdes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1c3e22-e07f-481c-8c25-794e77ae3886",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def build_query_str(split_type: SplitType, num_samples, threshold=5):\n",
    "    query_str = f\"\"\"MATCH (a:compound)-[r]->(b:target)\n",
    "    WHERE r.pchembl_value is not NULL and a.split_label = {split_type}\n",
    "    WITH a, b, r.pchembl_value > {threshold} as label\n",
    "    RETURN a as src, b as dst, label\n",
    "    LIMIT {num_samples}\n",
    "    \"\"\"\n",
    "\n",
    "    return query_str\n",
    "\n",
    "\n",
    "g.query(build_query_str(SplitType.TRAIN, 20000))\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e94a84f-f8d0-475c-a4b0-5246e83d182d",
   "metadata": {},
   "source": [
    "<a id='trainer'></a>\n",
    "### Step 4.2: Training Abstractions and Helper Functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1f8a0f-6ed2-4f2c-81c8-dad04c28f693",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### An Utility Function to Share the Validation Data Across All Hosts\n",
    "\n",
    "This method is only required we are interested in `classification` tasks and our validation metric function is `non-decomposable` such as `roc-auc` or `pr-auc`. In this scenario, the validation metric computed in each host can not be combined together in order to reduce to a single score. There are two possible approaches to handle this situation. \n",
    "1. All hosts communicate the predictions on the validation data aftert each epoch to a single source host and the reduced validation metric is computed on the source host. This would increase communication after each epoch.\n",
    "2. All hosts share the same validation data. This increases the communication one-time before the training starts, however during the training there is no additional communication overhead.\n",
    "\n",
    "In this notebook, we adopt the second approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fde3841-1f79-499f-a23a-2934306459a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# pylint: disable=C0412 (ungrouped-imports), W0404 (reimport)\n",
    "import pandas\n",
    "import torch\n",
    "from katana_enterprise.distributed import Graph\n",
    "from katana_enterprise.distributed.pytorch import init_workers\n",
    "\n",
    "\n",
    "def broadcast_val_data(val_df):\n",
    "    \"\"\"A function to gather validation data from all workers to the rank 0 process\"\"\"\n",
    "    if not torch.distributed.is_initialized():\n",
    "        init_workers()\n",
    "\n",
    "    num_hosts = torch.distributed.get_world_size()\n",
    "    gathered_val_data = [None for _ in range(num_hosts)]\n",
    "    torch.distributed.all_gather_object(gathered_val_data, val_df)\n",
    "    combined_val_df = pandas.concat(gathered_val_data)\n",
    "    return combined_val_df\n",
    "\n",
    "\n",
    "def test_broadcast_val_data(graph: Graph):\n",
    "    query_str = build_query_str(SplitType.TEST, num_samples=100)\n",
    "    df = graph.query(query_str, balance_output=True).to_pandas()\n",
    "    assert len(df) == 25  # balance_output must ensure this\n",
    "\n",
    "    df = broadcast_val_data(df)\n",
    "    assert len(df) == 100\n",
    "\n",
    "\n",
    "g.run(test_broadcast_val_data)\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637e41e4-ecca-4e24-9291-5ff7717c427b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Error from above\n",
    "#\n",
    "#  \n",
    "Host 0 errors:\n",
    "Traceback (most recent call last):\n",
    "  File \"/opt/miniconda/lib/python3.8/site-packages/katana_enterprise/worker/worker.py\", line 86, in execute\n",
    "    value = function(graph)\n",
    "  File \"<timed exec>\", line 23, in test_broadcast_val_data\n",
    "AssertionError\n",
    "\n",
    "Host 1 errors:\n",
    "Traceback (most recent call last):\n",
    "  File \"/opt/miniconda/lib/python3.8/site-packages/katana_enterprise/worker/worker.py\", line 86, in execute\n",
    "    value = function(graph)\n",
    "  File \"<timed exec>\", line 23, in test_broadcast_val_data\n",
    "AssertionError\n",
    "\n",
    "Host 2 errors:\n",
    "Traceback (most recent call last):\n",
    "  File \"/opt/miniconda/lib/python3.8/site-packages/katana_enterprise/worker/worker.py\", line 86, in execute\n",
    "    value = function(graph)\n",
    "  File \"<timed exec>\", line 23, in test_broadcast_val_data\n",
    "AssertionError\n",
    "\n",
    "---------------------------------------------------------------------------\n",
    "AssertionError                            Traceback (most recent call last)\n",
    "File <timed exec>:29\n",
    "\n",
    "File /opt/conda/lib/python3.8/site-packages/katana_enterprise/async_to_sync.py:249, in AsyncToSync.<locals>.do_wrap.<locals>.wrapper(self, *args, **kwargs)\n",
    "    246 @wraps(underlying_func)\n",
    "    247 def wrapper(self, *args, **kwargs):\n",
    "    248     return registry.async_to_sync(\n",
    "--> 249         underlying_func(\n",
    "    250             get_self_func(self),\n",
    "    251             *(registry.sync_to_async(a) for a in args),\n",
    "    252             **{k: registry.sync_to_async(v) for k, v in kwargs.items()},\n",
    "    253         )\n",
    "    254     )\n",
    "\n",
    "File /opt/conda/lib/python3.8/site-packages/katana_enterprise/async_to_sync.py:176, in async_to_sync.<locals>.wrapper(timeout, *args, **kwargs)\n",
    "    166     registry = AsyncToSyncClassRegistry.get()\n",
    "    167     return registry.async_to_sync(\n",
    "    168         wait_for(\n",
    "    169             async_func(\n",
    "   (...)\n",
    "    174         )\n",
    "    175     )\n",
    "--> 176 return wait_for(async_func(*args, **kwargs), timeout=timeout)\n",
    "\n",
    "File /opt/conda/lib/python3.8/site-packages/katana_enterprise/async_to_sync.py:147, in wait_for(coro, timeout)\n",
    "    145 try:\n",
    "    146     future = asyncio.run_coroutine_threadsafe(timeout_coro, loop=AsyncRunnerThread.get().loop)\n",
    "--> 147     return future.result()\n",
    "    148 except KeyboardInterrupt:\n",
    "    149     inner_future.cancel()\n",
    "\n",
    "File /opt/conda/lib/python3.8/concurrent/futures/_base.py:444, in Future.result(self, timeout)\n",
    "    442     raise CancelledError()\n",
    "    443 elif self._state == FINISHED:\n",
    "--> 444     return self.__get_result()\n",
    "    445 else:\n",
    "    446     raise TimeoutError()\n",
    "\n",
    "File /opt/conda/lib/python3.8/concurrent/futures/_base.py:389, in Future.__get_result(self)\n",
    "    387 if self._exception:\n",
    "    388     try:\n",
    "--> 389         raise self._exception\n",
    "    390     finally:\n",
    "    391         # Break a reference cycle with the exception in self._exception\n",
    "    392         self = None\n",
    "\n",
    "File /opt/conda/lib/python3.8/asyncio/tasks.py:455, in wait_for(fut, timeout, loop)\n",
    "    450     warnings.warn(\"The loop argument is deprecated since Python 3.8, \"\n",
    "    451                   \"and scheduled for removal in Python 3.10.\",\n",
    "    452                   DeprecationWarning, stacklevel=2)\n",
    "    454 if timeout is None:\n",
    "--> 455     return await fut\n",
    "    457 if timeout <= 0:\n",
    "    458     fut = ensure_future(fut, loop=loop)\n",
    "\n",
    "File /opt/conda/lib/python3.8/site-packages/katana_enterprise/remote/aio/graph.py:509, in Graph.run(self, function)\n",
    "    507 print(result.stdout, file=sys.stdout, end=\"\")\n",
    "    508 print(result.stderr, file=sys.stderr, end=\"\")\n",
    "--> 509 return result.value\n",
    "\n",
    "File /opt/conda/lib/python3.8/site-packages/katana_enterprise/remote/run_result.py:21, in RunResult.value(self)\n",
    "     19 @property\n",
    "     20 def value(self):\n",
    "---> 21     self.reraise_if_error()\n",
    "     22     assert self.success\n",
    "     23     return self._value\n",
    "\n",
    "File /opt/conda/lib/python3.8/site-packages/katana_enterprise/remote/run_result.py:17, in RunResult.reraise_if_error(self)\n",
    "     15 def reraise_if_error(self):\n",
    "     16     if not self.success:\n",
    "---> 17         raise self._value\n",
    "\n",
    "AssertionError: \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36113fc-8d41-4d33-b939-e6b7c80c2288",
   "metadata": {},
   "source": [
    "<a id='step5'></a>\n",
    "## Step 5: Putting the Pipeline Together\n",
    "\n",
    "We are now ready to put together a end-to-end pipeline for DTI task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a60363e-ca12-4963-8fc0-c995fee490fd",
   "metadata": {},
   "source": [
    "### Setting up Model Hyperparameters and Training Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095e0d9c-8e18-4d07-8a8c-b3119f3fc33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pylint: disable=W0404 (reimport), E0602(undefined-variable)\n",
    "# pylint: disable=too-many-instance-attributes\n",
    "import dataclasses\n",
    "from typing import Callable\n",
    "\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class TrainingHyperParams:\n",
    "    num_train_samples: int = 10000\n",
    "    num_val_samples: int = 1000\n",
    "    epochs: int = 5\n",
    "    patience: int = 50\n",
    "    optimizer: Callable = torch.optim.Adam\n",
    "    lr: float = 0.01\n",
    "    weight_decay: float = 0.001\n",
    "    scheduler: Callable = torch.optim.lr_scheduler.StepLR\n",
    "    step_size: int = 5\n",
    "\n",
    "\n",
    "# Setting up training hyper parameters\n",
    "training_hp = TrainingHyperParams()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76373d2e-056b-4229-b76b-7c5b4cd4d34b",
   "metadata": {},
   "source": [
    "### The Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5440ae92-6a27-407a-98cc-e4029895a6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# pylint: disable=C0412 (ungrouped-imports), W0404 (reimport)\n",
    "import numpy\n",
    "import torch\n",
    "from katana_enterprise.ai import data, loss, model, train\n",
    "from katana_enterprise.ai.torch import ReduceMethod\n",
    "from katana_enterprise.ai.train import DistTrainer\n",
    "from katana_enterprise.distributed import Graph\n",
    "from katana_enterprise.distributed.pytorch import init_workers\n",
    "from sklearn.metrics import average_precision_score, mean_squared_error\n",
    "\n",
    "\n",
    "def remote_pipeline(graph, training_params):\n",
    "    if not torch.distributed.is_initialized():\n",
    "        init_workers()\n",
    "\n",
    "    # Initialize random edge samples from the graph using queries\n",
    "    # -----------------------------------------------------------\n",
    "\n",
    "    train_data = graph.query(build_query_str(SplitType.TRAIN, training_params.num_train_samples), balance_output=True)\n",
    "    train_src_nodes, train_dst_nodes, train_labels = (\n",
    "        list(train_data[\"src\"]),\n",
    "        list(train_data[\"dst\"]),\n",
    "        list(train_data[\"label\"]),\n",
    "    )\n",
    "    train_seeds = [\n",
    "        (u, v, label.as_py()) for _, (u, v, label) in enumerate(zip(train_src_nodes, train_dst_nodes, train_labels))\n",
    "    ]\n",
    "\n",
    "    val_data = graph.query(build_query_str(SplitType.VAL, training_params.num_val_samples), balance_output=True)\n",
    "    # val_df = broadcast_val_data(val_data)\n",
    "\n",
    "    val_src_nodes, val_dst_nodes, val_labels = list(val_data[\"src\"]), list(train_data[\"dst\"]), list(val_data[\"label\"])\n",
    "    val_seeds = [(u, v, label.as_py()) for _, (u, v, label) in enumerate(zip(val_src_nodes, val_dst_nodes, val_labels))]\n",
    "\n",
    "    # train_samples = list(train_df.itertuples(index=False))\n",
    "    # val_samples = list(val_df.itertuples(index=False))\n",
    "\n",
    "    # Initialize the Katana multiminibatch sampler.\n",
    "    # --------------------------------------------\n",
    "\n",
    "    fan_in = [5, 10]\n",
    "    batches_at_once = 10\n",
    "    sampler_config = data.SampledSubgraphConfig(\n",
    "        layer_fan=fan_in,\n",
    "        max_minibatches=batches_at_once,\n",
    "        property_batch_size=batches_at_once,\n",
    "        feat_prop_name=\"random_100\",\n",
    "        multilayer_export=True,  # multi-layer export needs to be true to have a subgraph for each layer.\n",
    "        sample_with_replacement=True,\n",
    "        label_prop_name=\"random_100\",\n",
    "    )\n",
    "\n",
    "    # Initialize the Katana Dataloader.\n",
    "    # ---------------------------------\n",
    "\n",
    "    train_sampler = data.LpSubgraphSampler(graph, sampler_config)\n",
    "    train_dataloader = data.LpDataLoader(\n",
    "        train_seeds, train_sampler, local_batch_size=50, shuffle=True, balance_seeds=True\n",
    "    )\n",
    "\n",
    "    val_sampler = data.LpSubgraphSampler(graph, sampler_config)\n",
    "    val_dataloader = data.LpDataLoader(val_seeds, val_sampler, local_batch_size=50, shuffle=False, balance_seeds=True)\n",
    "\n",
    "    # Define a DGL model wrapped in Katana\n",
    "    # ------------------------------------\n",
    "\n",
    "    nr_features = 100\n",
    "    dgl_model = model.LpGcn(in_dim=nr_features, out_dim=2, gnn_hidden_dims=[256, 128], mlp_hidden_dims=[128],)\n",
    "\n",
    "    # Define the loss function, the optimizers, the validaiton metric, and the dsitributed tracker\n",
    "    # --------------------------------------------------------------------------------------------\n",
    "\n",
    "    loss_fn = loss.CrossEntropyLoss([2])\n",
    "    optimizer = training_params.optimizer(\n",
    "        dgl_model.parameters(), lr=training_params.lr, weight_decay=training_params.weight_decay\n",
    "    )\n",
    "    scheduler = training_params.scheduler(optimizer, step_size=training_params.step_size)\n",
    "\n",
    "    # Define the validation metric\n",
    "    # y is a vector of length n and pred is a matrix of dimensions n by 2.\n",
    "    # So to convert it to edges labels, we need to find the distance between the endpoints in pred.\n",
    "    def pr_auc(y, pred):\n",
    "        assert len(pred[0]) == 2\n",
    "        edge_pred = torch.exp(pred[:, 1])\n",
    "        return average_precision_score(y, edge_pred)\n",
    "\n",
    "    # Define the tracker\n",
    "    tracker = train.DistTracker(callback_fn=print, src_rank=0)\n",
    "\n",
    "    # Initialize the distributed trainer\n",
    "    # ---------------------------------\n",
    "\n",
    "    trainer = DistTrainer(\n",
    "        model=dgl_model,\n",
    "        train_loss_fn=loss_fn,\n",
    "        validation_metric_fn=pr_auc,\n",
    "        validation_reduce_method=ReduceMethod.MEAN,\n",
    "        train_loader=train_dataloader,\n",
    "        validation_loader=val_dataloader,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        epochs=training_params.epochs,\n",
    "        patience=training_params.patience,\n",
    "        maximization=True,\n",
    "        tracker=tracker,\n",
    "    )\n",
    "\n",
    "    # Start the Model training\n",
    "    # ------------------------\n",
    "\n",
    "    trained_model, loss_val = trainer.train()\n",
    "    print(f\"Number of trained parameters: {sum(p.numel() for p in trained_model.parameters() if p.requires_grad)}\")\n",
    "    print(f\"Trainer loss:{loss_val}\")\n",
    "\n",
    "\n",
    "#     # Time for inference\n",
    "#     inference_sampler = data.PyGNodeSubgraphSampler(graph, sampler_config)\n",
    "#     nodes_to_infer = graph.master_nodes()\n",
    "#     inference_loader = data.NodeDataLoader(\n",
    "#         inference_sampler, 1000, node_ids=nodes_to_infer, drop_last=False, balance_seeds=False\n",
    "#     )\n",
    "#     print(\"Starting the eval phase...\")\n",
    "#     trained_model.eval()\n",
    "#     embedding = (\n",
    "#         torch.vstack([trained_model.encode(data) for data in inference_loader if len(data.x) > 0]).detach().numpy()\n",
    "#     )\n",
    "#     print(\"Generated the node embeddings...\")\n",
    "#     print(embedding)\n",
    "\n",
    "\n",
    "g.run(lambda g: remote_pipeline(g, training_hp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1913e3e0-a076-4cf2-983e-1bcdc450d75a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "katana": {
   "component": "AI",
   "max_num_hosts": 4
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
