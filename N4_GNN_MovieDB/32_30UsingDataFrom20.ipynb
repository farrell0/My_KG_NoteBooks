{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a947ffc8-26ff-4ee7-bd4a-7a2cbc4d096d",
   "metadata": {},
   "source": [
    "# Part 00: Notebook overview .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072792a8-3286-43e6-a8c6-e20383d23232",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  This Notebook is part of a set that demonstrate GNN using a movie dataset.\n",
    "#  About this Notebook,\n",
    "#\n",
    "#  .  In NoteBook 20*, we built a Movie graph; movies, keywords\n",
    "#     In NoteBook 30* we delivered a primer on CountVectorizers, and Classifiers using non-graph\n",
    "#\n",
    "#  .  Here we'll apply the non-graph Classifier to our Movie nodes from our graph.\n",
    "#\n",
    "#     We'll apply it to just Movie.title, then Movie.tagline, then Movie.overview.\n",
    "#     And then we'll apply it to all 3 at one time.\n",
    "#\n",
    "#     And we'll compare the results to known Movie.genres_primary.\n",
    "#\n",
    "#        Recall Movies actually had an array of genres, and we derived genres_primary\n",
    "#        as the genres in the first position inside the array.\n",
    "#\n",
    "#  .  Our graph, our source of data, was prepared/loaded in NoteBook 20*.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487303ea-82e4-4c07-ae54-047a62595a13",
   "metadata": {},
   "source": [
    "#  Part 01: Graph setup, and initial read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd57fe7-db01-49e8-8bff-9719a2d00f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  The KatanaGraph remote API is expected to run from a node external to\n",
    "#  the Katana Graph cluster itself.\n",
    "#\n",
    "#  This differs from the distributed API, which is meant to run primitives\n",
    "#  on the Katana Graph worker nodes.\n",
    "#\n",
    "\n",
    "from katana import remote\n",
    "from katana.remote import import_data\n",
    "\n",
    "my_client = remote.Client()\n",
    "\n",
    "print(my_client)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0493b77a-8b25-405f-95f3-f003a5a2ab78",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "NUM_PARTITIONS  = 3\n",
    "   #\n",
    "DB_NAME         = \"my_db\"\n",
    "GRAPH_NAME      = \"my_graph1\" \n",
    "\n",
    "MY_DEBUG        = False\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bc42ab-d539-4374-bb04-fef7de5d4325",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "my_graph, *_ = my_client.get_database(name=DB_NAME).find_graphs_by_name(GRAPH_NAME)\n",
    "\n",
    "print(my_graph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1346d7-40cb-46cf-a4a9-e39b9b693fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Sanity check \n",
    "#\n",
    "display(\"Number of Graph Nodes: %d\" % (my_graph.num_nodes()))\n",
    "display(\"Number of Graph Edges: %s\" % (my_graph.num_edges()))\n",
    "\n",
    "\n",
    "#  Sample output,\n",
    "#\n",
    "#     'Number of Graph Nodes: 64857'\n",
    "#     'Number of Graph Edges: 330988'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4519f9d-53dc-4ac0-908b-a317fb77aff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Looking at the graph\n",
    "#\n",
    "l_result = my_graph.query(\"\"\"\n",
    "\n",
    "   MATCH (n) -[r]-> (m)\n",
    "   RETURN n, r, m\n",
    "   LIMIT 1000                        //  Limit is 25,000 for visualization, smaller is better\n",
    "   \n",
    "   \"\"\",\n",
    "   contextualize=True)\n",
    "\n",
    "l_result.view()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8888e38-f1e3-4ef0-b96e-6a4b6480990a",
   "metadata": {},
   "source": [
    "\n",
    "<div> \n",
    "<img src=\"./01_Images/10-Movie-Query-1.png\" alt=\"Drawing\" style=\"width: 1600px;\"/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd60626-5ee8-416a-ab7b-94e4cea7cbea",
   "metadata": {},
   "source": [
    "#  Part 02: Getting data for use by our non-graph classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7145573-17f8-4773-9fcb-403c4f7c043e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  We're using a non-graph set of libraries from sklearn. As such, we need to pull\n",
    "#  the data out of the graph into DataFrames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0416c10d-a1cb-433d-ad5e-85498aaa1744",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  We'll consider these packages to be common knowledge. Else, return to the Compulsaries\n",
    "#  set of NoteBooks for sample use, introduction ..\n",
    "#\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "   #\n",
    "import dask.array as da\n",
    "import dask.dataframe as dd\n",
    "   #\n",
    "from dask.dataframe import from_pandas\n",
    "\n",
    "from tabulate import tabulate\n",
    "\n",
    "\n",
    "   ###\n",
    "    \n",
    "    \n",
    "from sklearn.feature_extraction.text import CountVectorizer            #  Build a sparse vector (matrix) of keywords\n",
    "from sklearn.feature_extraction.text import TfidfTransformer           #  Tool to norrmalize a condition; words that occur frequently, versus giving more weight to infrequent words\n",
    "   #                                                                   #        Ie., Having 6 toes is way more rare than having 5. Should I give more weight that a person is 5'10\", or has 6 toes ?\n",
    "from sklearn.naive_bayes import MultinomialNB                          #  A non-graph ML routine, a Classifier  (similar perhaps to node property prediction, but on non-graph)\n",
    "                                                                       #     Several variants of NaiveBayes, one being 'multi-nomial'.\n",
    "                                                                       #     'multi-nomial', used commonly for word count style problems\n",
    "        \n",
    "from sklearn.pipeline import Pipeline                                  #  Allow us to simplify exuection of many sequential steps\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931b5da0-70ed-4105-bb14-525ac3e458c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  These will be our test movies\n",
    "#     (Previously we cast movie.id as a string, not an integer; no reason for not doing that.)\n",
    "#\n",
    "\n",
    "l_test_movie_ids = [\n",
    "   \"8469\"    ,      #  Animal House\n",
    "   \"11848\"   ,      #  Animal Farm\n",
    "   \"600\"     ,      #  Full Metal Jacket\n",
    "   \"13342\"   ,      #  Fast Times at Ridgemont High\n",
    "   \"10373\"   ,      #  Quadrophenia\n",
    "   \"62\"      ,      #  2001: A Space Odyssey\n",
    "   \"14328\"   ,      #  The Paper Chase\n",
    "   \"11589\"   ,      #  Kelly's Heroes\n",
    "   \"694\"     ,      #  The Shining\n",
    "   \"424\"     ,      #  Schindler's List\n",
    "   ]\n",
    "\n",
    "l_query  = \"\"\"\n",
    "   MATCH (n: Movies) \n",
    "   WHERE n.id IN {0}\n",
    "   RETURN n.id AS id, n.title AS title, n.genres_primary AS genres, n.genres_primary_id AS genres_id, n.tagline AS tagline, n.overview AS overview\n",
    "   ORDER BY n.title\n",
    "   \"\"\".format(l_test_movie_ids)\n",
    "\n",
    "l_test_movies = my_graph.query(l_query)\n",
    "   #\n",
    "#  print(tabulate(l_test_movies, headers='keys', tablefmt='psql'))\n",
    "display(l_test_movies.table())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9f1ed8-7b10-4da7-8fc7-9169d85fb7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Go get our train movies\n",
    "#\n",
    "#     We run both (all movies) and \"NOT () IN\" movies just for a check-\n",
    "#\n",
    "\n",
    "\n",
    "#  This is not one we want; just counting to be sure-\n",
    "#\n",
    "l_query  = \"\"\"\n",
    "   MATCH (n: Movies) \n",
    "   RETURN n.id AS id, n.title AS title, n.genres_primary AS genres, n.genres_primary_id AS genres_id, n.tagline AS tagline, n.overview AS overview\n",
    "   \"\"\".format(l_test_movie_ids)\n",
    "      #\n",
    "l_train_movies = my_graph.query(l_query)\n",
    "   #\n",
    "print(len(l_train_movies))\n",
    "\n",
    "\n",
    "#  This is the one we want moving forward\n",
    "#\n",
    "l_query  = \"\"\"\n",
    "   MATCH (n: Movies) \n",
    "   WHERE NOT n.id IN {0}\n",
    "   RETURN n.id AS id, n.title AS title, n.genres_primary AS genres, n.genres_primary_id AS genres_id, n.tagline AS tagline, n.overview AS overview\n",
    "   \"\"\".format(l_test_movie_ids)\n",
    "      #\n",
    "l_train_movies = my_graph.query(l_query)\n",
    "   #\n",
    "print(len(l_train_movies))\n",
    "\n",
    "if (MY_DEBUG):\n",
    "   print(tabulate(l_train_movies.head(5), headers='keys', tablefmt='psql'))\n",
    "   #  display(l_train_movies.table())\n",
    "\n",
    "\n",
    "#  Sample output,\n",
    "#\n",
    "#     45433\n",
    "#     45423\n",
    "#     +----+----------+-------------+--------+--------------------------------------------------------------------    -------+----------------------------------------------------------------------+---------------------+\n",
    "#     |    | genres   |   genres_id |     id | overview                                                                      | tagline                                                              | title               |\n",
    "#     |----+----------+-------------+--------+---------------------------------------------------------------------    ------+----------------------------------------------------------------------+---------------------|\n",
    "#     |  0 | Family   |       10751 | 291854 | Meet the Mormons examines the stereotypes that surround the Mormon faith. ... | Six ordinary individuals. Six extraordinary stories.                 | Meet the Mormons    |\n",
    "#     |  1 | Comedy   |          35 |  24632 | A horror-thriller in which ...                                                |                                                                      | The Mad             |\n",
    "#     |  2 | Comedy   |          35 |  11860 | An ugly duckling having und ...                                               | You are cordially invited to the most surprising merger of the year. | Sabrina             |\n",
    "#     |  3 | Comedy   |          35 |  40774 | This documentary follows 10 ...                                               | Let's root for the little guys                                       | Midgets Vs. Mascots |\n",
    "#     |  4 | Action   |          28 |  45325 | A mischievous young boy, To ...                                               | The Original Bad Boys.                                               | Tom and Huck        |\n",
    "#     +----+----------+-------------+--------+--------------------------------------------------------------------    -------+----------------------------------------------------------------------+---------------------+\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368aa398-0c31-40a7-8953-c90a67d4afce",
   "metadata": {},
   "source": [
    "# Part 03:  Do the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6a05f9f6-6c7c-4c85-af0c-9b9aaafdbdbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'list'>\n",
      "Shalako\n",
      "<class 'str'>\n",
      "Spy Chasers\n",
      "<class 'str'>\n",
      "Western\n",
      "<class 'str'>\n",
      "Comedy\n",
      "<class 'str'>\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#  As before, build a single structure with all of the data we need.\n",
    "#  More,\n",
    "#\n",
    "#     .  The result of the traversals above were  <class 'katana.remote.ResultSet'>\n",
    "#        This largely functions as a DataFrame, but generally the ML functions\n",
    "#        want a list().\n",
    "#        Hence, to_list()\n",
    "#\n",
    "#     .  We'll start with movies.title, since that should yield hideous results.\n",
    "#        (Compare/contrast.)\n",
    "#\n",
    "\n",
    "\n",
    "\n",
    "l_train_movies2 = l_train_movies.assign(\n",
    "   title2  = lambda x: x.title.fillna(0.0).astype(int),\n",
    "   class2  = lambda x: x.title.fillna(0.0).astype(int),\n",
    "   )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "my_train = {}\n",
    "   #\n",
    "my_train[\"data\" ] = l_train_movies[\"title\" ].to_list()\n",
    "my_train[\"class\"] = l_train_movies[\"genres\"].to_list()\n",
    "\n",
    "\n",
    "MY_DEBUG=True\n",
    "\n",
    "if (MY_DEBUG):\n",
    "   print(type(my_train[\"data\" ]))\n",
    "   print(type(my_train[\"class\"]))\n",
    "      #\n",
    "   l_cntr = 0\n",
    "      #\n",
    "   for l_each in my_train[\"data\" ]:\n",
    "      l_cntr += 1\n",
    "         #\n",
    "      if (l_cntr < 3):\n",
    "         print(l_each)\n",
    "         print(type(l_each))\n",
    "            #\n",
    "   l_cntr = 0\n",
    "      #\n",
    "   for l_each in my_train[\"class\"]:\n",
    "      l_cntr += 1\n",
    "         #\n",
    "      if (l_cntr < 3):\n",
    "         print(l_each)\n",
    "         print(type(l_each))\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b84c2095-3b00-4cd5-a3f8-ccade725d563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7 µs, sys: 0 ns, total: 7 µs\n",
      "Wall time: 11.9 µs\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [44], line 12\u001b[0m\n\u001b[1;32m      4\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m my_pipeline \u001b[38;5;241m=\u001b[39m Pipeline([\n\u001b[1;32m      7\u001b[0m    (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcv\u001b[39m\u001b[38;5;124m\"\u001b[39m,     CountVectorizer()  ),\n\u001b[1;32m      8\u001b[0m    (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtfidf\u001b[39m\u001b[38;5;124m\"\u001b[39m,  TfidfTransformer() ),\n\u001b[1;32m      9\u001b[0m    (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclf\u001b[39m\u001b[38;5;124m\"\u001b[39m,    MultinomialNB()    ),\n\u001b[1;32m     10\u001b[0m    ])\n\u001b[0;32m---> 12\u001b[0m my_classifier \u001b[38;5;241m=\u001b[39m \u001b[43mmy_pipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmy_train\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmy_train\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/sklearn/pipeline.py:378\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;124;03m\"\"\"Fit the model.\u001b[39;00m\n\u001b[1;32m    353\u001b[0m \n\u001b[1;32m    354\u001b[0m \u001b[38;5;124;03mFit all the transformers one after the other and transform the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;124;03m    Pipeline with fitted steps.\u001b[39;00m\n\u001b[1;32m    376\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    377\u001b[0m fit_params_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_fit_params(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m--> 378\u001b[0m Xt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_message(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)):\n\u001b[1;32m    380\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/sklearn/pipeline.py:336\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[0;34m(self, X, y, **fit_params_steps)\u001b[0m\n\u001b[1;32m    334\u001b[0m     cloned_transformer \u001b[38;5;241m=\u001b[39m clone(transformer)\n\u001b[1;32m    335\u001b[0m \u001b[38;5;66;03m# Fit or load from cache the current transformer\u001b[39;00m\n\u001b[0;32m--> 336\u001b[0m X, fitted_transformer \u001b[38;5;241m=\u001b[39m \u001b[43mfit_transform_one_cached\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcloned_transformer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessage_clsname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPipeline\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_log_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_idx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params_steps\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# Replace the transformer of the step with the fitted\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# transformer. This is necessary when loading the transformer\u001b[39;00m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;66;03m# from the cache.\u001b[39;00m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[step_idx] \u001b[38;5;241m=\u001b[39m (name, fitted_transformer)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/joblib/memory.py:349\u001b[0m, in \u001b[0;36mNotMemorizedFunc.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 349\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/sklearn/pipeline.py:870\u001b[0m, in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(transformer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 870\u001b[0m         res \u001b[38;5;241m=\u001b[39m \u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    872\u001b[0m         res \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:1338\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1330\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1331\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1332\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1333\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1334\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1335\u001b[0m             )\n\u001b[1;32m   1336\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m-> 1338\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[1;32m   1341\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:1209\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1207\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m raw_documents:\n\u001b[1;32m   1208\u001b[0m     feature_counter \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m-> 1209\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m \u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1210\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1211\u001b[0m             feature_idx \u001b[38;5;241m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:111\u001b[0m, in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m preprocessor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 111\u001b[0m         doc \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    113\u001b[0m         doc \u001b[38;5;241m=\u001b[39m tokenizer(doc)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:69\u001b[0m, in \u001b[0;36m_preprocess\u001b[0;34m(doc, accent_function, lower)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;124;03m\"\"\"Chain together an optional series of text preprocessing steps to\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;124;03mapply to a document.\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;124;03m    preprocessed string\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lower:\n\u001b[0;32m---> 69\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mdoc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m()\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m accent_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     71\u001b[0m     doc \u001b[38;5;241m=\u001b[39m accent_function(doc)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "\n",
    "#  Reminder:  the steps below were detailed at length in NoteBook 30*\n",
    "#\n",
    "\n",
    "%time\n",
    "\n",
    "my_pipeline = Pipeline([\n",
    "   (\"cv\",     CountVectorizer()  ),\n",
    "   (\"tfidf\",  TfidfTransformer() ),\n",
    "   (\"clf\",    MultinomialNB()    ),\n",
    "   ])\n",
    "\n",
    "my_classifier = my_pipeline.fit(my_train[\"data\"], my_train[\"class\"])\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add916f7-0349-4a30-9308-e694f4eead59",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "l_cntr = 0\n",
    "   #\n",
    "for l_each in l_test_movies.itertuples():\n",
    "   l_cntr += 1\n",
    "      #\n",
    "   if (l_cntr < 4):\n",
    "      print(l_each.title)\n",
    "      print(type(l_each.title))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0699e75-d4a5-4170-8e04-d59fbf317d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "xxx = l_test_movies[\"title\"].to_list()\n",
    "print(type(xxx))\n",
    "print(len(xxx))\n",
    "\n",
    "yyy = l_test_movies[\"genres_id\"].to_list()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "my_classifier = my_pipeline.fit(xxx, yyy)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dafa9ef-f3f6-4668-9e5a-a10368adb821",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223a6cbe-245b-43ac-8e97-b523e5bce053",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68db5ef-0b68-48fc-aa82-0d85953c3e03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551ffb96-b489-43c4-8980-47a3e3e82259",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5c5918-2fb0-47ec-bf50-0c1f2660ab52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e1f1eb-de59-4c6b-bb44-07ef3e21eefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "my_test = {}\n",
    "   #\n",
    "my_test[\"data\"] = [ \n",
    "   \"I hate baseball\" ,\n",
    "   \"I like wine\"     ,\n",
    "   \"eggs with cheese\",\n",
    "   \"eggs with cheese football\",\n",
    "   \"football with eggs and cheese\",\n",
    "   \"Wisconsin, America's Dairyland\",\n",
    "   ]\n",
    "\n",
    "\n",
    "#  Applying/scoring the model\n",
    "#\n",
    "my_result = my_classifier.predict(my_test[\"data\"])\n",
    "\n",
    "\n",
    "\n",
    "#  Recall that the sorted class array is,\n",
    "#\n",
    "#     my_train[\"class\"] = [ \"food\" , \"sport\", ]\n",
    "\n",
    "\n",
    "my_test[\"target\"] = [1, 0, 0, 0, 0, 0]                                     #  We only need this to gauge accuracy below, and the last one is wrong\n",
    "\n",
    "\n",
    "\n",
    "display(\"Average accuracy: %f\" % (np.mean(my_result == my_test[\"target\"])) )\n",
    "   #\n",
    "for l_index, l_zip in enumerate(zip(my_test[\"data\"], my_test[\"target\"])):\n",
    "   print(\"Data: %-36s   Correct Class: %-10s   Predict Class: %-10s\" % (l_zip[0], my_train[\"class\"][l_zip[1]], my_train[\"class\"][my_result[l_index]]) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3c50ab-0e99-4c8a-a5c2-b97a27d90dbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "47641439-ac2b-4515-8f47-b59c9cf5994a",
   "metadata": {},
   "source": [
    "#  Final piece, scoring (applying)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d87255-4a99-4c74-8a9a-8ed80cd19406",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Now we are ready to score (apply) the model.\n",
    "#\n",
    "\n",
    "\n",
    "#  Here is the test data\n",
    "#\n",
    "my_test = {}\n",
    "   #\n",
    "my_test[\"data\"] = [ \n",
    "   \"I hate baseball\" ,\n",
    "   \"I like wine\"     ,\n",
    "   \"eggs with cheese\",\n",
    "   \"eggs with cheese football\",\n",
    "   \"football with eggs and cheese\",\n",
    "   \"Wisconsin, America's Dairyland\",\n",
    "   ]\n",
    "\n",
    "\n",
    "#  Applying/scoring the model\n",
    "#\n",
    "my_result = my_classifier.predict(my_test[\"data\"])\n",
    "\n",
    "\n",
    "\n",
    "#  Recall that the sorted class array is,\n",
    "#\n",
    "#     my_train[\"class\"] = [ \"food\" , \"sport\", ]\n",
    "\n",
    "\n",
    "my_test[\"target\"] = [1, 0, 0, 0, 0, 0]                                     #  We only need this to gauge accuracy below, and the last one is wrong\n",
    "\n",
    "\n",
    "\n",
    "display(\"Average accuracy: %f\" % (np.mean(my_result == my_test[\"target\"])) )\n",
    "   #\n",
    "for l_index, l_zip in enumerate(zip(my_test[\"data\"], my_test[\"target\"])):\n",
    "   print(\"Data: %-36s   Correct Class: %-10s   Predict Class: %-10s\" % (l_zip[0], my_train[\"class\"][l_zip[1]], my_train[\"class\"][my_result[l_index]]) )\n",
    "\n",
    "#  Sample output,\n",
    "#\n",
    "#     'Average accuracy: 0.833333'\n",
    "#\n",
    "#     Data: I hate baseball                        Correct Class: sport        Predict Class: sport     \n",
    "#     Data: I like wine                            Correct Class: food         Predict Class: food      \n",
    "#     Data: eggs with cheese                       Correct Class: food         Predict Class: food      \n",
    "#     Data: eggs with cheese football              Correct Class: food         Predict Class: food      \n",
    "#     Data: football with eggs and cheese          Correct Class: food         Predict Class: food      \n",
    "#     Data: Wisconsin, America's Dairyland         Correct Class: food         Predict Class: sport     \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f49097-92db-4c90-b76c-199b5a7ca20b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
